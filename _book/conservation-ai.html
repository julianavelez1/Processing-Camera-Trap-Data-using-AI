<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Conservation AI | Guide for using artificial intelligence systems for camera trap data processing</title>
  <meta name="description" content="We aim to provide an overview of the steps required to use platforms that implement artificial intelligence into workflows for processing camera trap images." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Conservation AI | Guide for using artificial intelligence systems for camera trap data processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="We aim to provide an overview of the steps required to use platforms that implement artificial intelligence into workflows for processing camera trap images." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Conservation AI | Guide for using artificial intelligence systems for camera trap data processing" />
  
  <meta name="twitter:description" content="We aim to provide an overview of the steps required to use platforms that implement artificial intelligence into workflows for processing camera trap images." />
  

<meta name="author" content="Juliana Velez and John Fieberg" />


<meta name="date" content="2022-01-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mlwic2-machine-learning-for-wildlife-image-classification.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.19/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Guide for using artificial intelligence systems for camera trap data processing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="camera-trap-data.html"><a href="camera-trap-data.html"><i class="fa fa-check"></i><b>2</b> Camera-trap data</a></li>
<li class="chapter" data-level="3" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html"><i class="fa fa-check"></i><b>3</b> Wildlife Insights (WI)</a><ul>
<li class="chapter" data-level="3.1" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#set-up"><i class="fa fa-check"></i><b>3.1</b> Set-up</a></li>
<li class="chapter" data-level="3.2" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#uploadformat-data"><i class="fa fa-check"></i><b>3.2</b> Upload/format data</a></li>
<li class="chapter" data-level="3.3" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#uploadenter-metadata"><i class="fa fa-check"></i><b>3.3</b> Upload/enter metadata</a></li>
<li class="chapter" data-level="3.4" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#processing-images---ai-module"><i class="fa fa-check"></i><b>3.4</b> Processing images - AI module</a></li>
<li class="chapter" data-level="3.5" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#post-ai-image-processing"><i class="fa fa-check"></i><b>3.5</b> Post-AI image processing</a></li>
<li class="chapter" data-level="3.6" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#wi-output"><i class="fa fa-check"></i><b>3.6</b> Using AI output</a></li>
<li class="chapter" data-level="3.7" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#wi-performance"><i class="fa fa-check"></i><b>3.7</b> Assessing AI performance</a><ul>
<li class="chapter" data-level="3.7.1" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#reading-in-data-introduction-to-the-purrr-package"><i class="fa fa-check"></i><b>3.7.1</b> Reading in data, introduction to the Purrr package</a></li>
<li class="chapter" data-level="3.7.2" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#removing-duplicate-images"><i class="fa fa-check"></i><b>3.7.2</b> Removing duplicate images</a></li>
<li class="chapter" data-level="3.7.3" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#images-with-multiple-observations-of-the-same-species"><i class="fa fa-check"></i><b>3.7.3</b> Images with multiple observations of the same species</a></li>
<li class="chapter" data-level="3.7.4" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#images-with-multiple-species"><i class="fa fa-check"></i><b>3.7.4</b> Images with multiple species</a></li>
<li class="chapter" data-level="3.7.5" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#summarizing-human-and-computer-vision-records-by-species"><i class="fa fa-check"></i><b>3.7.5</b> Summarizing human and computer vision records by species</a></li>
<li class="chapter" data-level="3.7.6" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#confusion-matrix-and-performance-measures"><i class="fa fa-check"></i><b>3.7.6</b> Confusion matrix and performance measures</a></li>
<li class="chapter" data-level="3.7.7" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#confidence-thresholds"><i class="fa fa-check"></i><b>3.7.7</b> Confidence thresholds</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#conclusions"><i class="fa fa-check"></i><b>3.8</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html"><i class="fa fa-check"></i><b>4</b> MegaDetector - Microsoft AI</a><ul>
<li class="chapter" data-level="4.1" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-upload"><i class="fa fa-check"></i><b>4.1</b> Upload/format data</a></li>
<li class="chapter" data-level="4.2" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-metadata"><i class="fa fa-check"></i><b>4.2</b> Upload/enter metadata</a></li>
<li class="chapter" data-level="4.3" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-process"><i class="fa fa-check"></i><b>4.3</b> Process images - AI module</a></li>
<li class="chapter" data-level="4.4" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-timelapse"><i class="fa fa-check"></i><b>4.4</b> Image processing with Timelapse 2</a></li>
<li class="chapter" data-level="4.5" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-output"><i class="fa fa-check"></i><b>4.5</b> Using AI output</a></li>
<li class="chapter" data-level="4.6" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-performance"><i class="fa fa-check"></i><b>4.6</b> Assessing AI performance</a><ul>
<li class="chapter" data-level="4.6.1" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#reading-in-data-introduction-to-the-purrr-package-1"><i class="fa fa-check"></i><b>4.6.1</b> Reading in data, introduction to the Purrr package</a></li>
<li class="chapter" data-level="4.6.2" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#format-computer-vision-data-set"><i class="fa fa-check"></i><b>4.6.2</b> Format computer vision data set</a></li>
<li class="chapter" data-level="4.6.3" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-format-hv"><i class="fa fa-check"></i><b>4.6.3</b> Format human vision data set</a></li>
<li class="chapter" data-level="4.6.4" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#merging-computer-and-human-vision-data-sets"><i class="fa fa-check"></i><b>4.6.4</b> Merging computer and human vision data sets</a></li>
<li class="chapter" data-level="4.6.5" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#confusion-matrix-and-performance-measures-1"><i class="fa fa-check"></i><b>4.6.5</b> Confusion matrix and performance measures</a></li>
<li class="chapter" data-level="4.6.6" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-thresholds"><i class="fa fa-check"></i><b>4.6.6</b> Confidence thresholds</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#conclusions-1"><i class="fa fa-check"></i><b>4.7</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html"><i class="fa fa-check"></i><b>5</b> MLWIC2: Machine Learning for Wildlife Image Classification</a><ul>
<li class="chapter" data-level="5.1" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#set-up-1"><i class="fa fa-check"></i><b>5.1</b> Set-up</a></li>
<li class="chapter" data-level="5.2" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#format-mlwic2"><i class="fa fa-check"></i><b>5.2</b> Upload/format data</a></li>
<li class="chapter" data-level="5.3" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#mlwic2-ai-module"><i class="fa fa-check"></i><b>5.3</b> Process images - AI module</a></li>
<li class="chapter" data-level="5.4" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#ai-mlwic2"><i class="fa fa-check"></i><b>5.4</b> Assessing AI performance</a><ul>
<li class="chapter" data-level="5.4.1" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#format-human-vision-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Format human vision data set</a></li>
<li class="chapter" data-level="5.4.2" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#format-computer-vision-data-set-1"><i class="fa fa-check"></i><b>5.4.2</b> Format computer vision data set</a></li>
<li class="chapter" data-level="5.4.3" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#merging-computer-and-human-vision-data-sets-1"><i class="fa fa-check"></i><b>5.4.3</b> Merging computer and human vision data sets</a></li>
<li class="chapter" data-level="5.4.4" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#summarizing-human-and-computer-vision-records-by-class"><i class="fa fa-check"></i><b>5.4.4</b> Summarizing human and computer vision records by class</a></li>
<li class="chapter" data-level="5.4.5" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#confusion-matrix-and-performance-measures-2"><i class="fa fa-check"></i><b>5.4.5</b> Confusion matrix and performance measures</a></li>
<li class="chapter" data-level="5.4.6" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#confidence-thresholds-1"><i class="fa fa-check"></i><b>5.4.6</b> Confidence thresholds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#model-training"><i class="fa fa-check"></i><b>5.5</b> Model training</a></li>
<li class="chapter" data-level="5.6" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#classify-trained"><i class="fa fa-check"></i><b>5.6</b> Classify using a trained model</a></li>
<li class="chapter" data-level="5.7" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#conclusion"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conservation-ai.html"><a href="conservation-ai.html"><i class="fa fa-check"></i><b>6</b> Conservation AI</a><ul>
<li class="chapter" data-level="6.1" data-path="conservation-ai.html"><a href="conservation-ai.html#set-up-2"><i class="fa fa-check"></i><b>6.1</b> Set-up</a></li>
<li class="chapter" data-level="6.2" data-path="conservation-ai.html"><a href="conservation-ai.html#uploadformat-data-1"><i class="fa fa-check"></i><b>6.2</b> Upload/format data</a></li>
<li class="chapter" data-level="6.3" data-path="conservation-ai.html"><a href="conservation-ai.html#image-tagging"><i class="fa fa-check"></i><b>6.3</b> Image tagging</a></li>
<li class="chapter" data-level="6.4" data-path="conservation-ai.html"><a href="conservation-ai.html#process-images---ai-module"><i class="fa fa-check"></i><b>6.4</b> Process images - AI module</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Guide for using artificial intelligence systems for camera trap data processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conservation-ai" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Conservation AI</h1>
<p>Conservation AI is a platform aimed at facilitating the use of AI to solve conservation problems. It is developed by research experts in Machine Learning, Computer Science and Conservation Biology from the Liverpool John Moores University and NVIDIA (a technology company that developed the NVIDIA CUDA®, a collection of libraries and tools for using AI; more information can be found here <a href="https://developer.nvidia.com/gpu-accelerated-libraries" class="uri">https://developer.nvidia.com/gpu-accelerated-libraries</a>). Currently, Conservation AI can detect species from the UK, North America and from Sub-Saharan Africa. However, it also provides a user-friendly interface for creating a data set that can be used to train an AI model to detect species from your particular region. To create the training data set you will need to tag a subset of your images. This step requires creating a bounding box around animals in the images and assigning a species label. We illustrate the tagging process using the Conservation AI infrastructure.</p>
<div id="set-up-2" class="section level2">
<h2><span class="header-section-number">6.1</span> Set-up</h2>
<ul>
<li><p>Create an account here
<a href="https://www.conservationai.co.uk/register/" class="uri">https://www.conservationai.co.uk/register/</a></p></li>
<li><p>Once you create an account, it needs to be activated by the Conservation AI team. To do that you can contact them using their message center <a href="https://www.conservationai.co.uk/contact/" class="uri">https://www.conservationai.co.uk/contact/</a></p></li>
<li><p>After being approved, you will have a dashboard were you have sections for uploading your files, tagging images with the corresponding classification, and checking AI results (Figure <a href="conservation-ai.html#fig:dashboard-fig">6.1</a>).</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dashboard-fig"></span>
<img src="input_figures/conservation_ai/dashboard.png" alt="Conservation AI dashboard with sections for uploading images, tagging images and checking AI results." width="85%" />
<p class="caption">
Figure 6.1: Conservation AI dashboard with sections for uploading images, tagging images and checking AI results.
</p>
</div>
</div>
<div id="uploadformat-data-1" class="section level2">
<h2><span class="header-section-number">6.2</span> Upload/format data</h2>
<p>The Conservation AI team will help you to set up a project, after which you will be able to upload images and begin the tagging process. You will need to submit a list of species contained in your images to the Conservation AI team, and they will create the tagging project for you. For categories that are not of interest or species that are difficult to identify, you can include in your list higher taxonomic levels (e.g., Class Aves, Order Rodentia). Additionally, you will have to share a subset of images that will be used for training; you can do that either using the “Upload Files” section (Figure <a href="conservation-ai.html#fig:dashboard-fig">6.1</a>) or using other transfer methods like Google Drive to share your images with the Conservation AI team. The first option is recommended as it allows users to maintain a more independent workflow for image upload; otherwise, you will have to request the Conservation AI team to upload the images for you, which might slow down your tagging process. Uploads are currently limited to 500 images per batch but should increase to 10,000 images per batch in early 2022. Once uploaded, you will find the images in your tagging interface (Figure <a href="conservation-ai.html#fig:tagging-interface">6.4</a>).</p>
</div>
<div id="image-tagging" class="section level2">
<h2><span class="header-section-number">6.3</span> Image tagging</h2>
<p>In the tagging section, you can find a report of species that have been tagged within the Conservation AI platform and the number of tags for each species (Figure <a href="conservation-ai.html#fig:tagging-report">6.2</a>). Here you can check if your species of interest already contains tags from other users. Additionally, after sharing your species list with the Conservation AI team, you will find your species included in this list and see updates of the number of tags for each species as you advance in the tagging process.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tagging-report"></span>
<img src="input_figures/conservation_ai/tagged_sp.png" alt="Report of species tagged within Conservation AI and number of tags per species." width="85%" />
<p class="caption">
Figure 6.2: Report of species tagged within Conservation AI and number of tags per species.
</p>
</div>
<p>To begin image tagging, you can access the tagging interface by clicking on the “Start Tagging” green tab (Figure <a href="conservation-ai.html#fig:tagging-report">6.2</a>), which will direct you to a list of projects (Figure <a href="conservation-ai.html#fig:tagging-projects">6.3</a>) where you should search for the project created for you.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tagging-projects"></span>
<img src="input_figures/conservation_ai/tagging_projects.png" alt="List of projects that can be accessed to perform image tagging." width="85%" />
<p class="caption">
Figure 6.3: List of projects that can be accessed to perform image tagging.
</p>
</div>
<p>Accessing your project will direct you to your tagging interface (Figure <a href="conservation-ai.html#fig:tagging-interface">6.4</a>), where you will see images to be tagged at the left, an image viewer at the center, and a tag list at the right (this is the same list that you shared with the Conservation AI team). As you detect and tag species in an image, make sure you select the “Detection” tab in the top right.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tagging-interface"></span>
<img src="input_figures/conservation_ai/tagging_interface.png" alt="Tagging interface for drawing bounding boxes around animals detected in the images and for assigning species labels." width="85%" />
<p class="caption">
Figure 6.4: Tagging interface for drawing bounding boxes around animals detected in the images and for assigning species labels.
</p>
</div>
<p>You will inspect your images using the image viewer, where you can zoom in and out and enter the full-screen mode using the controls below the image. (Figure <a href="conservation-ai.html#fig:tagging-interface">6.4</a>). To tag the image, you must draw a bounding box by clicking and dragging a rectangle on top of the animal detected (Figure <a href="conservation-ai.html#fig:rectangle">6.5</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rectangle"></span>
<img src="input_figures/conservation_ai/rectangle.png" alt="A bounding box is drawn around an animal detected in the image." width="85%" />
<p class="caption">
Figure 6.5: A bounding box is drawn around an animal detected in the image.
</p>
</div>
<p>Once you draw the bounding box, you must assign a species label by selecting it from the tagging list (Figure <a href="conservation-ai.html#fig:leopardus">6.6</a>) and save your tag using the blue “Save” tab. Once you save your tag, the image will be moved to the tagged images list at the bottom of the interface (Figure <a href="conservation-ai.html#fig:saved-tag">6.7</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:leopardus"></span>
<img src="input_figures/conservation_ai/leopardus_tag.png" alt="Species labels can be assigned by selecting the species name from the Tag List and saved using the blue &quot;Save&quot; tab." width="85%" />
<p class="caption">
Figure 6.6: Species labels can be assigned by selecting the species name from the Tag List and saved using the blue “Save” tab.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:saved-tag"></span>
<img src="input_figures/conservation_ai/saved_tag.png" alt="Images tagged will be collected at the bottom of the tagging interface." width="85%" />
<p class="caption">
Figure 6.7: Images tagged will be collected at the bottom of the tagging interface.
</p>
</div>
<p>The species might also contain higher taxonomic levels or other categories of interest (e.g., vehicles, people, etc.). It will also contain a “No Good” category that must be used for tagging empty or blurred images (Figure <a href="conservation-ai.html#fig:no-good">6.8</a>), or images containing small fragments of animal’s body parts. The “No Good” images will not be used for model training.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:no-good"></span>
<img src="input_figures/conservation_ai/no_good.png" alt="Use of the &quot;No Good&quot; tag for empty images." width="85%" />
<p class="caption">
Figure 6.8: Use of the “No Good” tag for empty images.
</p>
</div>
<p>When more than one animal is present, multiple tags can be assigned to the image (Figure <a href="conservation-ai.html#fig:multiple-objs">6.9</a>). Remember to save all your tags.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multiple-objs"></span>
<img src="input_figures/conservation_ai/multiple_objs.png" alt="Tags used for multiple animals in an image." width="85%" />
<p class="caption">
Figure 6.9: Tags used for multiple animals in an image.
</p>
</div>
<p>Instead of selecting species labels from the species list at the right, after sketching the bounding box, you can type the shortcuts represented by the letters, numbers or symbols at the right of each species name in the Tag list. You can edit these shortcuts by clicking the <code>...</code> icon, as we show with the Class Aves, to which we assign the <code>+</code> sign as shortcut (Figure <a href="conservation-ai.html#fig:tag-shortcuts">6.10</a>). You can also use shortcuts to hide/show bounding boxes, enter full-screen mode and for saving your tags (Figure <a href="conservation-ai.html#fig:other-shortcuts">6.11</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tag-shortcuts"></span>
<img src="input_figures/conservation_ai/shortcut.png" alt="Shortcuts can be assigned or editted for every species or group in the Tag List. You can use these shortcuts for quickly assigning species labels." width="30%" />
<p class="caption">
Figure 6.10: Shortcuts can be assigned or editted for every species or group in the Tag List. You can use these shortcuts for quickly assigning species labels.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:other-shortcuts"></span>
<img src="input_figures/conservation_ai/other_shortcuts.png" alt="Shortcuts for bounding boxes display, entering full-screen mode and saving your tags are also available below the image viewer." width="30%" />
<p class="caption">
Figure 6.11: Shortcuts for bounding boxes display, entering full-screen mode and saving your tags are also available below the image viewer.
</p>
</div>
<p>Once you finish tagging your batch of 500 images, you need to upload another batch to your tagging site. If you shared your images via Google Drive with the Conservation AI team, you can contact them to upload the images for you. The upload of each of these batches by the Conservation AI team can take up a few weeks depending on developers’ availability, so it’s recommended that you upload your images directly into the platform.</p>
</div>
<div id="process-images---ai-module" class="section level2">
<h2><span class="header-section-number">6.4</span> Process images - AI module</h2>
<p>Once the tagging stage is complete, you can contact the Conservation AI team (at <a href="mailto:admin@conservationai.co.uk" class="email">admin@conservationai.co.uk</a>) to have them train models using a transfer learning approach using the training data set that you provided in the image-tagging stage. Metadata, such as the time/date of the image and filename, are automatically read once images are uploaded for classification. Thus, you do not need to enter any metadata other than a sensor id if applicable. Once the classification stage is complete, the results can be accessed in the platform’s analytical dashboard. Completion of model training will depend on the Conservation AI team’s availability, so it’s important to schedule model training with developers that will re-train models with your data.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mlwic2-machine-learning-for-wildlife-image-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-conservationai.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["CTWorkflows.pdf", "CTWorkflows.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
