<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 MegaDetector - Microsoft AI | Guide for using artificial intelligence systems for camera trap data processing</title>
  <meta name="description" content="We aim to provide an overview of the steps required to use platforms that implement artificial intelligence into workflows for processing camera trap images." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 MegaDetector - Microsoft AI | Guide for using artificial intelligence systems for camera trap data processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="We aim to provide an overview of the steps required to use platforms that implement artificial intelligence into workflows for processing camera trap images." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 MegaDetector - Microsoft AI | Guide for using artificial intelligence systems for camera trap data processing" />
  
  <meta name="twitter:description" content="We aim to provide an overview of the steps required to use platforms that implement artificial intelligence into workflows for processing camera trap images." />
  

<meta name="author" content="Juliana Velez and John Fieberg" />


<meta name="date" content="2022-01-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="wildlife-insights-wi.html"/>
<link rel="next" href="mlwic2-machine-learning-for-wildlife-image-classification.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.19/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Guide for using artificial intelligence systems for camera trap data processing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="camera-trap-data.html"><a href="camera-trap-data.html"><i class="fa fa-check"></i><b>2</b> Camera-trap data</a></li>
<li class="chapter" data-level="3" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html"><i class="fa fa-check"></i><b>3</b> Wildlife Insights (WI)</a><ul>
<li class="chapter" data-level="3.1" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#set-up"><i class="fa fa-check"></i><b>3.1</b> Set-up</a></li>
<li class="chapter" data-level="3.2" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#uploadformat-data"><i class="fa fa-check"></i><b>3.2</b> Upload/format data</a></li>
<li class="chapter" data-level="3.3" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#uploadenter-metadata"><i class="fa fa-check"></i><b>3.3</b> Upload/enter metadata</a></li>
<li class="chapter" data-level="3.4" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#processing-images---ai-module"><i class="fa fa-check"></i><b>3.4</b> Processing images - AI module</a></li>
<li class="chapter" data-level="3.5" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#post-ai-image-processing"><i class="fa fa-check"></i><b>3.5</b> Post-AI image processing</a></li>
<li class="chapter" data-level="3.6" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#wi-output"><i class="fa fa-check"></i><b>3.6</b> Using AI output</a></li>
<li class="chapter" data-level="3.7" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#wi-performance"><i class="fa fa-check"></i><b>3.7</b> Assessing AI performance</a><ul>
<li class="chapter" data-level="3.7.1" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#reading-in-data-introduction-to-the-purrr-package"><i class="fa fa-check"></i><b>3.7.1</b> Reading in data, introduction to the Purrr package</a></li>
<li class="chapter" data-level="3.7.2" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#removing-duplicate-images"><i class="fa fa-check"></i><b>3.7.2</b> Removing duplicate images</a></li>
<li class="chapter" data-level="3.7.3" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#images-with-multiple-observations-of-the-same-species"><i class="fa fa-check"></i><b>3.7.3</b> Images with multiple observations of the same species</a></li>
<li class="chapter" data-level="3.7.4" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#images-with-multiple-species"><i class="fa fa-check"></i><b>3.7.4</b> Images with multiple species</a></li>
<li class="chapter" data-level="3.7.5" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#summarizing-human-and-computer-vision-records-by-species"><i class="fa fa-check"></i><b>3.7.5</b> Summarizing human and computer vision records by species</a></li>
<li class="chapter" data-level="3.7.6" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#confusion-matrix-and-performance-measures"><i class="fa fa-check"></i><b>3.7.6</b> Confusion matrix and performance measures</a></li>
<li class="chapter" data-level="3.7.7" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#confidence-thresholds"><i class="fa fa-check"></i><b>3.7.7</b> Confidence thresholds</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="wildlife-insights-wi.html"><a href="wildlife-insights-wi.html#conclusions"><i class="fa fa-check"></i><b>3.8</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html"><i class="fa fa-check"></i><b>4</b> MegaDetector - Microsoft AI</a><ul>
<li class="chapter" data-level="4.1" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-upload"><i class="fa fa-check"></i><b>4.1</b> Upload/format data</a></li>
<li class="chapter" data-level="4.2" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-metadata"><i class="fa fa-check"></i><b>4.2</b> Upload/enter metadata</a></li>
<li class="chapter" data-level="4.3" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-process"><i class="fa fa-check"></i><b>4.3</b> Process images - AI module</a></li>
<li class="chapter" data-level="4.4" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-timelapse"><i class="fa fa-check"></i><b>4.4</b> Image processing with Timelapse 2</a></li>
<li class="chapter" data-level="4.5" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-output"><i class="fa fa-check"></i><b>4.5</b> Using AI output</a></li>
<li class="chapter" data-level="4.6" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-performance"><i class="fa fa-check"></i><b>4.6</b> Assessing AI performance</a><ul>
<li class="chapter" data-level="4.6.1" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#reading-in-data-introduction-to-the-purrr-package-1"><i class="fa fa-check"></i><b>4.6.1</b> Reading in data, introduction to the Purrr package</a></li>
<li class="chapter" data-level="4.6.2" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#format-computer-vision-data-set"><i class="fa fa-check"></i><b>4.6.2</b> Format computer vision data set</a></li>
<li class="chapter" data-level="4.6.3" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-format-hv"><i class="fa fa-check"></i><b>4.6.3</b> Format human vision data set</a></li>
<li class="chapter" data-level="4.6.4" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#merging-computer-and-human-vision-data-sets"><i class="fa fa-check"></i><b>4.6.4</b> Merging computer and human vision data sets</a></li>
<li class="chapter" data-level="4.6.5" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#confusion-matrix-and-performance-measures-1"><i class="fa fa-check"></i><b>4.6.5</b> Confusion matrix and performance measures</a></li>
<li class="chapter" data-level="4.6.6" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#md-thresholds"><i class="fa fa-check"></i><b>4.6.6</b> Confidence thresholds</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="megadetector---microsoft-ai.html"><a href="megadetector---microsoft-ai.html#conclusions-1"><i class="fa fa-check"></i><b>4.7</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html"><i class="fa fa-check"></i><b>5</b> MLWIC2: Machine Learning for Wildlife Image Classification</a><ul>
<li class="chapter" data-level="5.1" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#set-up-1"><i class="fa fa-check"></i><b>5.1</b> Set-up</a></li>
<li class="chapter" data-level="5.2" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#format-mlwic2"><i class="fa fa-check"></i><b>5.2</b> Upload/format data</a></li>
<li class="chapter" data-level="5.3" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#mlwic2-ai-module"><i class="fa fa-check"></i><b>5.3</b> Process images - AI module</a></li>
<li class="chapter" data-level="5.4" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#ai-mlwic2"><i class="fa fa-check"></i><b>5.4</b> Assessing AI performance</a><ul>
<li class="chapter" data-level="5.4.1" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#format-human-vision-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Format human vision data set</a></li>
<li class="chapter" data-level="5.4.2" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#format-computer-vision-data-set-1"><i class="fa fa-check"></i><b>5.4.2</b> Format computer vision data set</a></li>
<li class="chapter" data-level="5.4.3" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#merging-computer-and-human-vision-data-sets-1"><i class="fa fa-check"></i><b>5.4.3</b> Merging computer and human vision data sets</a></li>
<li class="chapter" data-level="5.4.4" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#summarizing-human-and-computer-vision-records-by-class"><i class="fa fa-check"></i><b>5.4.4</b> Summarizing human and computer vision records by class</a></li>
<li class="chapter" data-level="5.4.5" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#confusion-matrix-and-performance-measures-2"><i class="fa fa-check"></i><b>5.4.5</b> Confusion matrix and performance measures</a></li>
<li class="chapter" data-level="5.4.6" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#confidence-thresholds-1"><i class="fa fa-check"></i><b>5.4.6</b> Confidence thresholds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#model-training"><i class="fa fa-check"></i><b>5.5</b> Model training</a></li>
<li class="chapter" data-level="5.6" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#classify-trained"><i class="fa fa-check"></i><b>5.6</b> Classify using a trained model</a></li>
<li class="chapter" data-level="5.7" data-path="mlwic2-machine-learning-for-wildlife-image-classification.html"><a href="mlwic2-machine-learning-for-wildlife-image-classification.html#conclusion"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conservation-ai.html"><a href="conservation-ai.html"><i class="fa fa-check"></i><b>6</b> Conservation AI</a><ul>
<li class="chapter" data-level="6.1" data-path="conservation-ai.html"><a href="conservation-ai.html#set-up-2"><i class="fa fa-check"></i><b>6.1</b> Set-up</a></li>
<li class="chapter" data-level="6.2" data-path="conservation-ai.html"><a href="conservation-ai.html#uploadformat-data-1"><i class="fa fa-check"></i><b>6.2</b> Upload/format data</a></li>
<li class="chapter" data-level="6.3" data-path="conservation-ai.html"><a href="conservation-ai.html#image-tagging"><i class="fa fa-check"></i><b>6.3</b> Image tagging</a></li>
<li class="chapter" data-level="6.4" data-path="conservation-ai.html"><a href="conservation-ai.html#process-images---ai-module"><i class="fa fa-check"></i><b>6.4</b> Process images - AI module</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Guide for using artificial intelligence systems for camera trap data processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="megadetector---microsoft-ai" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> MegaDetector - Microsoft AI</h1>
<p>The <a href="https://github.com/microsoft/CameraTraps/blob/master/megadetector.md">MegaDetector (MD)</a> model detects objects (animals, people, and vehicles) in camera trap images, and can be used to separate images into categories of “Empty”, “Animal”, “Human” and “Vehicles” <span class="citation">(Beery, Morris, and Yang <a href="#ref-beery2019efficient">2019</a>)</span>. You can run MD on your own or request assistance from the Microsoft AI for Earth team. To choose one of these options, you can contact a member of the Microsoft AI team at <a href="mailto:cameratraps@lila.science" class="email">cameratraps@lila.science</a> to see which approach is right for you; this decision will depend on the number of images that you have, whether data can be shared with third parties, and how comfortable you are running Python code <span class="citation">(Beery, Morris, and Yang <a href="#ref-beery2019efficient">2019</a>)</span>. We will describe a workflow for a user that will submit images to the Microsoft AI for Earth team. A member of the Microsoft AI for Earth team will then run the models and return an output file that can be loaded into the image review software <a href="http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.Download2">Timelapse 2</a> <span class="citation">(Greenberg, Godin, and Whittington <a href="#ref-greenberg-design">2019</a>)</span>. Sections <a href="megadetector---microsoft-ai.html#md-upload">4.1</a>-<a href="megadetector---microsoft-ai.html#md-process">4.3</a> go through the steps required to run MD and interpret its output. Sections <a href="megadetector---microsoft-ai.html#md-timelapse">4.4</a> and <a href="megadetector---microsoft-ai.html#md-output">4.5</a> focus on how MD output can be processed in Timelapse 2 and how AI output can be used to accelerate image review and identification by humans. Specifically, AI identifications with high confidence values (i.e., likely to be correct) can be filtered, subset, and subsequently identified by humans using batch operations (see Section <a href="megadetector---microsoft-ai.html#md-output">4.5</a>). Section <a href="megadetector---microsoft-ai.html#md-performance">4.6</a> focuses on assessing the performance of MD by comparing human labels with the ones provided by MD for a set of images. This last section is particularly useful as it provides tools to evaluate how MD performs with specific data sets and guidance to understand differences in MD output when you use different confidence thresholds for assigning predictions.</p>
<div id="md-upload" class="section level2">
<h2><span class="header-section-number">4.1</span> Upload/format data</h2>
<p>To use MD you can choose between running the model on your own or contacting a member of the Microsoft AI for Earth team at <a href="mailto:cameratraps@mlila.science" class="email">cameratraps@mlila.science</a> to help you run the model. For the first option you won’t need to write any code, but you will need to be comfortable running commands at the command line. If you choose the second option, the Microsoft AI for Earth team will create a <a href="https://azure.microsoft.com/en-us/services/storage/blobs/">Microsoft Azure Blob Storage</a> container for you, to which you will upload your images. You will typically upload your images using <a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10">AzCopy</a>, a command-line utility for batch data transfers, or <a href="https://azure.microsoft.com/en-us/features/storage-explorer">Azure Storage Explorer</a>, a GUI-based utility for working with Azure Storage. The team will provide more specific instructions when you contact them.</p>
</div>
<div id="md-metadata" class="section level2">
<h2><span class="header-section-number">4.2</span> Upload/enter metadata</h2>
<p>When using MD, there is no need to enter metadata before processing your data using AI. You will enter metadata during the post-AI image processing stage, when MD output is integrated with other image processing tools such as <a href="http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.Download2">Timelapse 2</a> or <a href="https://camelot-project.readthedocs.io/en/latest/introduction.html">Camelot</a>.</p>
</div>
<div id="md-process" class="section level2">
<h2><span class="header-section-number">4.3</span> Process images - AI module</h2>
<p>The Microsoft AI for Earth team will run models for you and return a JSON file or a CSV file if requested. This file will contain image filenames (e.g., “A01/01020108.JPG”, where A01 represents the subfolder of a camera trap location and 01020108.JPG is the picture filename), maximum confidence values associated with all detections within an image, and the maximum confidence value for each category (animal, person, or vehicle) (Figure <a href="megadetector---microsoft-ai.html#fig:md-compvision-fig">4.1</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:md-compvision-fig"></span>
<img src="input_figures/md/compvision.png" alt="Identifications provided by Megadetector." width="90%" />
<p class="caption">
Figure 4.1: Identifications provided by Megadetector.
</p>
</div>
</div>
<div id="md-timelapse" class="section level2">
<h2><span class="header-section-number">4.4</span> Image processing with Timelapse 2</h2>
<p>This section synthesize general features of Timelapse 2 that facilitate species identification and annotation of other features of interest from the images (e.g., animals’ sex, behavior, condition, etc.). Section <a href="megadetector---microsoft-ai.html#md-output">4.5</a> demonstrates how you can incorporate AI output (i.e., the JSON file) in Timelapse 2, to accelerate image review and identification.</p>
<p>To run Timelapse 2, you will need a computer running Microsoft Windows, and your images must be organized in subfolders (e.g., by study area and camera trap location; Figure <a href="megadetector---microsoft-ai.html#fig:md-subfolders">4.2</a>). Timelapse 2 software, video tutorials and a detailed manual with all the software features explained can be downloaded here: <a href="http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.Download2" class="uri">http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.Download2</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:md-subfolders"></span>
<img src="input_figures/md/subfolders.png" alt="Organization of images folders and subfolders appropriate for Timelapse 2 [@greenberg-timelapse]." width="40%" />
<p class="caption">
Figure 4.2: Organization of images folders and subfolders appropriate for Timelapse 2 <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>.
</p>
</div>
<p>Before using Timelapse 2, you will need to create a data schema (i.e., a template with a .tdb file extension) that identifies the data fields that will be recorded when processing your camera trap data (e.g., species name, sex, behavior, etc.). This step can be accomplished by using the Timelapse Template Editor that is downloaded along with Timelapse 2. The Template Editor allows you to create the .tdb file that will be read in Timelapse 2 and will adjust the software image processing interface to capture the different data fields you specify in the .tdb file <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>.</p>
<p>Once photos are imported to Timelapse 2, you can review them using different processing tools provided by the software. The Timelapse 2 manual <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span> and documentation about the software design <span class="citation">(Greenberg, Godin, and Whittington <a href="#ref-greenberg-design">2019</a>)</span> describe in detail the processing tools available in the software, which include features to:</p>
<ul>
<li>Magnify images and explore image difference extraction tools (e.g., to identify small animals that are otherwise difficult to spot).</li>
<li>Select multiple images quickly from small thumbnails and classify them all at once.</li>
<li>Automate data entry via metadata extraction (time/date, filename, temperature sensed by the camera) and copy annotated information to other pictures.</li>
<li>Sort images by date, species or any other annotated feature, to easily review and verify this information.</li>
</ul>
</div>
<div id="md-output" class="section level2">
<h2><span class="header-section-number">4.5</span> Using AI output</h2>
<p>To integrate MD output with Timelapse 2, you will need to have the same folder and subfolder structure (Figure <a href="megadetector---microsoft-ai.html#fig:md-subfolders">4.2</a>) that you used to upload pictures to the Azure Blob Storage. Using the same folder structure allows Timelapse 2 to match each image with MD output using relative paths associated with each filename (see notes in Figure <a href="megadetector---microsoft-ai.html#fig:md-matchingpaths">4.3</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:md-matchingpaths"></span>
<img src="input_figures/md/matching_paths.png" alt="Notes from the Timelapse 2 manual on how to match MD output with photos when using the Timelapse 2 software [@greenberg-timelapse]" width="95%" />
<p class="caption">
Figure 4.3: Notes from the Timelapse 2 manual on how to match MD output with photos when using the Timelapse 2 software <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>
</p>
</div>
<p>In Timelapse 2, you must activate the option for working with image recognition data (Figure <a href="megadetector---microsoft-ai.html#fig:md-imagerecognition">4.4</a> and import computer vision results stored in the JSON file provided by Microsoft AI team <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>. You will see bounding boxes that can facilitate image review once the automatic image recognition is activated.</p>
<p>To use AI results to accelerate image review, you can filter MD output categories detected with high confidence levels that are likely correct. To do that, you must provide a confidence range to accept predictions made by computer vision (Figure <a href="megadetector---microsoft-ai.html#fig:md-imagerecognition">4.4</a> and Figure <a href="megadetector---microsoft-ai.html#fig:md-confidence-value">4.5</a>). For example, if you choose “Empty” as the detected entity and a high confidence range (e.g, from 0.65 to 1.00), your data set will be filtered to display images identified as “Empty” by AI and whose predictions are likely to be correct.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:md-imagerecognition"></span>
<img src="input_figures/md/image_recognition.png" alt="Section of &quot;Custom selection&quot; from the Timelapse 2 software where the option to work with AI detections can be activated [@greenberg-timelapse]." width="80%" />
<p class="caption">
Figure 4.4: Section of “Custom selection” from the Timelapse 2 software where the option to work with AI detections can be activated <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:md-confidence-value"></span>
<img src="input_figures/md/confidence_value.png" alt="When activating the use of AI detections, users can filter which images are displayed depending on the range of confidence values provided [@greenberg-timelapse]." width="80%" />
<p class="caption">
Figure 4.5: When activating the use of AI detections, users can filter which images are displayed depending on the range of confidence values provided <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>.
</p>
</div>
<p>After subsetting images to be displayed depending on their confidence values, you can easily inspect images in the overview mode in Timelapse 2 and select multiple images at a time and assign an “Empty” category to them if the AI output is correct (Figure <a href="megadetector---microsoft-ai.html#fig:md-overview-timelapse">4.6</a>). If AI output is not correct, you can edit those classifications.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:md-overview-timelapse"></span>
<img src="input_figures/md/overview.png" alt="Timelapse 2 overview mode where users can perform bulk actions (e.g., selection of multiple images) to accept or reject AI predictions [@greenberg-timelapse]." width="80%" />
<p class="caption">
Figure 4.6: Timelapse 2 overview mode where users can perform bulk actions (e.g., selection of multiple images) to accept or reject AI predictions <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>.
</p>
</div>
<p>After, processing pictures with Timelapse 2, you can export the data as a CSV File. This file (<code>TimelapseData.csv</code>) will contain all the data entries that you specified in the template (Figure <a href="megadetector---microsoft-ai.html#fig:output-timelapse">4.7</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:output-timelapse"></span>
<img src="input_figures/md/output.png" alt="Output contained in a CSV file exported from Timelapse 2 software [@greenberg-timelapse]." width="80%" />
<p class="caption">
Figure 4.7: Output contained in a CSV file exported from Timelapse 2 software <span class="citation">(Greenberg <a href="#ref-greenberg-timelapse">2020</a>)</span>.
</p>
</div>
</div>
<div id="md-performance" class="section level2">
<h2><span class="header-section-number">4.6</span> Assessing AI performance</h2>
<p>Before exploring model performance with your data, let us recapitulate. MD runs AI models and outputs broad categories of predictions for images (in a JSON file). These AI predictions can be integrated with Timelapse 2 to further process camera trap images (i.e., identifying pictures with the help of AI output). However, if you have previously classified images (e.g., identified using your software or platform of preference), you can explore MD performance before integrating AI results using Timelapse 2.</p>
<p>To evaluate MD performance, you will need to 1) classify a subset of your pictures and export the classification results to a CSV file (containing at minimum, the image filename, camera location and species name), and 2) request the Microsoft AI team to run MD (if you decide not to run the models by yourself) and send the results as a CSV file. You can then follow the steps below to evaluate the performance of MD’s AI model. We also discuss how one can select an appropriate confidence threshold for filtering images to accelerate the image review process (e.g., by focusing only on images that likely contain an animal).</p>
<div id="reading-in-data-introduction-to-the-purrr-package-1" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Reading in data, introduction to the Purrr package</h3>
<p>Below, we demonstrate a step-by-step workflow for how to get MD output into R, join computer and human vision identifications, and estimate model performance metrics for each class. Throughout, we will use the <code>purrr</code> package in R <span class="citation">(Henry and Wickham <a href="#ref-purrr">2020</a>; R Core Team <a href="#ref-R-base">2021</a>)</span> to repeatedly apply the same function to objects in a list or column in a nested data frame efficiently and without the need for writing loops. Readers unfamiliar with <code>purrr</code> syntax, may want to view one or more of the tutorials, below, or make use of the <a href="https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf">purrr cheat sheet</a>.</p>
<ul>
<li><a href="http://www.rebeccabarter.com/blog/2019-08-19_purrr/" class="uri">http://www.rebeccabarter.com/blog/2019-08-19_purrr/</a></li>
<li><a href="https://www.r-bloggers.com/2020/05/one-stop-tutorial-on-purrr-package-in-r/" class="uri">https://www.r-bloggers.com/2020/05/one-stop-tutorial-on-purrr-package-in-r/</a></li>
<li><a href="https://jennybc.github.io/purrr-tutorial/index.html" class="uri">https://jennybc.github.io/purrr-tutorial/index.html</a></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Once you finish the identification of a subset of your images using your platform of preference, export your classifications as a CSV file and name it as <code>images_hv.csv</code>. The other CSV file containing the MD results can be named as <code>images_cv.csv</code>.</p></li>
<li><p>Create a data folder to store your two CSV files <code>images_cv.csv</code> and <code>images_hv.csv</code> that refer to classifications of computer and human vision, respectively (note, we provide an example of both files with the repository associated with this guide named <code>images_cv_jan2020.csv</code> and <code>images_hv_jan2020.csv</code>).</p></li>
<li><p>Process the two data files using the R code provided below.</p></li>
</ol>
<p>First, we load required libraries and open files.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1"><span class="kw">library</span>(tidyverse) <span class="co"># for data wrangling and visualization, includes dplyr and purrr</span></a>
<a class="sourceLine" id="cb33-2" data-line-number="2"><span class="kw">library</span>(here) <span class="co"># to allow use of relative paths</span></a></code></pre></div>
<p>Next, we tell R the path (i.e., directory name) that holds our files. We will use the <code>here</code> package <span class="citation">(Müller <a href="#ref-here">2017</a>)</span> to tell R that our files live in the “./data/md” directory. You may, alternatively, type in the full path to the file folder or a relative path from the root directory if you are using a project in Rstudio.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="co"># Create filefolder&#39;s path. This should point to the folder name</span></a>
<a class="sourceLine" id="cb34-2" data-line-number="2"><span class="co"># where you stored your CSV files, one with classifications of some of your pictures</span></a>
<a class="sourceLine" id="cb34-3" data-line-number="3"><span class="co"># and the other with MD output</span></a>
<a class="sourceLine" id="cb34-4" data-line-number="4">filesfolder &lt;-<span class="st"> </span><span class="kw">here</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;md&quot;</span>) </a>
<a class="sourceLine" id="cb34-5" data-line-number="5">filesfolder</a></code></pre></div>
<pre><code>## [1] &quot;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/md&quot;</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1"><span class="co"># List your files contained in the filesfolder directory. This code will</span></a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="co"># list all your CSV files (i.e., images_cv.csv and images_hv.csv)</span></a>
<a class="sourceLine" id="cb36-3" data-line-number="3">files &lt;-<span class="st"> </span><span class="kw">dir</span>(filesfolder, <span class="dt">pattern =</span> <span class="st">&quot;*.csv&quot;</span>) </a>
<a class="sourceLine" id="cb36-4" data-line-number="4">files</a></code></pre></div>
<pre><code>## [1] &quot;images_cv_jan2020.csv&quot; &quot;images_hv_jan2020.csv&quot;</code></pre>
<p>We then use the <code>map</code> function in the <code>purrr</code> library to read in all of the files and store them in a list object named <code>mycsv</code>. The first argument to <code>map</code> is a list (here, <code>files</code>) which is “piped in” using <code>%&gt;%</code> from the <code>magrittr</code> package <span class="citation">(Bache and Wickham <a href="#ref-magrittr">2020</a>)</span>. Pipes (<code>%&gt;%</code>) provide a way to execute a sequence of data operations, organized so that the operations can be read from left to right (e.g., “Take the set of files and then read them in using <code>read_csv</code>”). The second argument to <code>map</code> is a function, in this case <code>read_csv</code>, to be applied to the list. The <code>map</code> function iterates over the two files stored in the <code>files</code> object, reads in the data files and then stores them in a new list named <code>mycsv.</code> We use <code>~</code> to refer to our function and use <code>.x</code> to refer to the list object that is passed to the function as an additional argument.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="co"># Read both CSV files</span></a>
<a class="sourceLine" id="cb38-2" data-line-number="2">mycsv &lt;-<span class="st"> </span>files <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-3" data-line-number="3"><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="kw">read_csv</span>(<span class="kw">file.path</span>(filesfolder, .x)))</a>
<a class="sourceLine" id="cb38-4" data-line-number="4"></a>
<a class="sourceLine" id="cb38-5" data-line-number="5"><span class="co"># Inspect how the data sets look like</span></a>
<a class="sourceLine" id="cb38-6" data-line-number="6">mycsv </a></code></pre></div>
<pre><code>## [[1]]
## # A tibble: 112,247 × 7
##    image_path               max_confidence detections max_conf_animal max_conf_person
##    &lt;chr&gt;                             &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;           &lt;dbl&gt;
##  1 jan2020/A01/01080001.JPG          0.998 NA                  NA               0.998
##  2 jan2020/A01/01080002.JPG          0.97  NA                  NA               0.97 
##  3 jan2020/A01/01080003.JPG          0.996 NA                  NA               0.996
##  4 jan2020/A01/01080004.JPG          0.985 NA                   0.202           0.985
##  5 jan2020/A01/01080005.JPG          0.939 NA                   0.209           0.939
##  6 jan2020/A01/01080006.JPG          0.996 NA                  NA               0.996
##  7 jan2020/A01/01080007.JPG          0.999 NA                  NA               0.999
##  8 jan2020/A01/01080008.JPG          0.997 NA                  NA               0.997
##  9 jan2020/A01/01080009.JPG          0.914 NA                   0.428           0.914
## 10 jan2020/A01/01080010.JPG          0.992 NA                  NA               0.992
## # … with 112,237 more rows, and 2 more variables: max_conf_group &lt;dbl&gt;,
## #   max_conf_vehicle &lt;lgl&gt;
## 
## [[2]]
## # A tibble: 104,826 × 4
##    deployment_id       filename     timestamp           common_name 
##    &lt;chr&gt;               &lt;chr&gt;        &lt;dttm&gt;              &lt;chr&gt;       
##  1 A01-Jan2020-Jul2020 01090079.JPG 2020-01-09 10:54:39 Blank       
##  2 A01-Jan2020-Jul2020 01090080.JPG 2020-01-09 10:54:40 Blank       
##  3 A01-Jan2020-Jul2020 01090081.JPG 2020-01-09 10:54:41 Blank       
##  4 A01-Jan2020-Jul2020 01090082.JPG 2020-01-09 10:54:49 Blank       
##  5 A01-Jan2020-Jul2020 01090083.JPG 2020-01-09 10:54:50 Blank       
##  6 A01-Jan2020-Jul2020 01090084.JPG 2020-01-09 10:54:51 Blank       
##  7 A01-Jan2020-Jul2020 01100085.JPG 2020-01-10 16:14:15 Black Agouti
##  8 A01-Jan2020-Jul2020 01100086.JPG 2020-01-10 16:14:16 Blank       
##  9 A01-Jan2020-Jul2020 01100087.JPG 2020-01-10 16:14:17 Black Agouti
## 10 A01-Jan2020-Jul2020 01130088.JPG 2020-01-13 08:53:39 Blank       
## # … with 104,816 more rows</code></pre>
</div>
<div id="format-computer-vision-data-set" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Format computer vision data set</h3>
<p>Columns of interest in the the MD data set include:</p>
<ul>
<li><code>filename</code>: contains camera location and filename (e.g., A01/01010461.JPG).</li>
<li><code>max_confidence</code>: contains the maximum confidence value found for a detection in a picture.</li>
<li><code>max_conf_animal</code>, <code>max_conf_person</code>, <code>max_conf_group</code>, <code>max_conf_vehicle</code>: each of these columns contain a confidence value associated with a prediction by computer vision for the different classification classes.</li>
</ul>
<p>We begin by creating a <code>max_conf_blank</code> variable, which we will use to identify blank images. We assign a value of 1 to this variable whenever the observation has missing values for all of the other <code>max_conf</code> variables.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">cv_wide &lt;-<span class="st"> </span>mycsv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb40-2" data-line-number="2"><span class="st">  </span><span class="kw">pluck</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#extract the computer vision images_cv.csv file</span></a>
<a class="sourceLine" id="cb40-3" data-line-number="3"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">filename =</span> image_path) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb40-4" data-line-number="4"><span class="st">  </span><span class="kw">group_by</span>(filename) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># group by filename</span></a>
<a class="sourceLine" id="cb40-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">max_conf_blank =</span> <span class="kw">case_when</span>(<span class="kw">sum</span>(<span class="kw">is.na</span>(max_conf_animal),</a>
<a class="sourceLine" id="cb40-6" data-line-number="6">                               <span class="kw">is.na</span>(max_conf_person), </a>
<a class="sourceLine" id="cb40-7" data-line-number="7">                               <span class="kw">is.na</span>(max_conf_group), </a>
<a class="sourceLine" id="cb40-8" data-line-number="8">                               <span class="kw">is.na</span>(max_conf_vehicle)) <span class="op">==</span><span class="st"> </span><span class="dv">4</span> <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb40-9" data-line-number="9"><span class="st">  </span><span class="kw">ungroup</span>()</a></code></pre></div>
<p>MD can detect and classify more than one object in an picture, and each object will be assigned its own confidence value. We want to keep each of these classifications in a separate row. To do that, we first change the data set from “wide” to “long” format using the <code>pivot_longer</code> function <span class="citation">(Wickham and Henry <a href="#ref-R-tidyr">2018</a>)</span>. This function will create two new variables, <code>name</code> and <code>value</code>, that will hold the classification (“Human”, “Animal”, “Vehicle”, “Blank”) and associated confidence values, respectively.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1">cv_long &lt;-<span class="st"> </span>cv_wide <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>detections) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># remove the &quot;detections&quot; column</span></a>
<a class="sourceLine" id="cb41-3" data-line-number="3"><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="kw">c</span>(<span class="st">&quot;max_conf_animal&quot;</span>,</a>
<a class="sourceLine" id="cb41-4" data-line-number="4">                 <span class="st">&quot;max_conf_person&quot;</span>, </a>
<a class="sourceLine" id="cb41-5" data-line-number="5">                 <span class="st">&quot;max_conf_group&quot;</span>, </a>
<a class="sourceLine" id="cb41-6" data-line-number="6">                 <span class="st">&quot;max_conf_vehicle&quot;</span>, </a>
<a class="sourceLine" id="cb41-7" data-line-number="7">                 <span class="st">&quot;max_conf_blank&quot;</span>)) </a>
<a class="sourceLine" id="cb41-8" data-line-number="8"></a>
<a class="sourceLine" id="cb41-9" data-line-number="9">cv_long</a></code></pre></div>
<pre><code>## # A tibble: 561,235 × 4
##    filename                 max_confidence name              value
##    &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;
##  1 jan2020/A01/01080001.JPG          0.998 max_conf_animal  NA    
##  2 jan2020/A01/01080001.JPG          0.998 max_conf_person   0.998
##  3 jan2020/A01/01080001.JPG          0.998 max_conf_group   NA    
##  4 jan2020/A01/01080001.JPG          0.998 max_conf_vehicle NA    
##  5 jan2020/A01/01080001.JPG          0.998 max_conf_blank   NA    
##  6 jan2020/A01/01080002.JPG          0.97  max_conf_animal  NA    
##  7 jan2020/A01/01080002.JPG          0.97  max_conf_person   0.97 
##  8 jan2020/A01/01080002.JPG          0.97  max_conf_group   NA    
##  9 jan2020/A01/01080002.JPG          0.97  max_conf_vehicle NA    
## 10 jan2020/A01/01080002.JPG          0.97  max_conf_blank   NA    
## # … with 561,225 more rows</code></pre>
<p>Next, we drop all rows where the confidence value is equal to NA and rename our classification variable to <code>class</code>. Additionally, we remove images with the “Human” class; human images in this data set predominately correspond to when the cameras were initially being set up.</p>
<p>To simplify things, we also change the “Group” label to “Animal”. We do this for 3 reasons: 1) we found that there were very few “Group” classifications in our data set; 2) these predictions were not very accurate; and 3) MD will most often be used to separate blank images from those that have at least one animal present, and thus, the “Group” label is not all that informative.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">cv &lt;-<span class="st"> </span>cv_long <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb43-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(<span class="kw">is.na</span>(value) <span class="op">!=</span><span class="st"> </span><span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb43-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">name =</span> <span class="kw">replace</span>(name, name <span class="op">==</span><span class="st"> &quot;max_conf_blank&quot;</span> , <span class="st">&quot;Blank&quot;</span>), </a>
<a class="sourceLine" id="cb43-4" data-line-number="4">         <span class="dt">name =</span> <span class="kw">replace</span>(name, name <span class="op">==</span><span class="st"> &quot;max_conf_animal&quot;</span>, <span class="st">&quot;Animal&quot;</span>), </a>
<a class="sourceLine" id="cb43-5" data-line-number="5">         <span class="dt">name =</span> <span class="kw">replace</span>(name, name <span class="op">==</span><span class="st"> &quot;max_conf_person&quot;</span>, <span class="st">&quot;Human&quot;</span>), </a>
<a class="sourceLine" id="cb43-6" data-line-number="6">         <span class="dt">name =</span> <span class="kw">replace</span>(name, name <span class="op">==</span><span class="st"> &quot;max_conf_group&quot;</span>, <span class="st">&quot;Animal&quot;</span>), </a>
<a class="sourceLine" id="cb43-7" data-line-number="7">         <span class="dt">name =</span> <span class="kw">replace</span>(name, name <span class="op">==</span><span class="st"> &quot;max_conf_vehicle&quot;</span>, <span class="st">&quot;Vehicle&quot;</span>)) <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb43-8" data-line-number="8"><span class="st">  </span><span class="co"># rename predictions</span></a>
<a class="sourceLine" id="cb43-9" data-line-number="9"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">class =</span> name) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># rename column with predicted classes</span></a>
<a class="sourceLine" id="cb43-10" data-line-number="10"><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>class <span class="op">==</span><span class="st"> &quot;Human&quot;</span>)</a></code></pre></div>
<p>Changing the “Group” label to “Animal” results in some images having multiple predictions of “Animal” for the same image. In these cases, we use the <code>slice</code> function to select the first record <span class="citation">(Wickham et al. <a href="#ref-R-dplyr">2019</a>)</span>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1">cv &lt;-<span class="st"> </span>cv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb44-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(filename, class) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb44-3" data-line-number="3"><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>)</a></code></pre></div>
<p>Comparing output from human and computer vision also requires that the classification variables have the same levels attribute. We first create a vector, <code>all_levels</code>, containing the names of the classes. Then, we use the <code>factor</code> function <span class="citation">(R Core Team <a href="#ref-R-base">2021</a>)</span> to convert common names into factor classes and assign the levels to the <code>class</code> column.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="co"># Create a vector with levels of predicted categories</span></a>
<a class="sourceLine" id="cb45-2" data-line-number="2">all_levels &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Animal&quot;</span>, <span class="st">&quot;Blank&quot;</span>)</a>
<a class="sourceLine" id="cb45-3" data-line-number="3"></a>
<a class="sourceLine" id="cb45-4" data-line-number="4"><span class="co"># Assign the levels attribute to the class column</span></a>
<a class="sourceLine" id="cb45-5" data-line-number="5">cv<span class="op">$</span>class &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(cv<span class="op">$</span>class, <span class="dt">levels =</span> all_levels))</a></code></pre></div>
<p>Lastly, we add a variable that will indicate if there are multiple detections within the same image.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1">cv &lt;-<span class="st"> </span>cv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(filename) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">multiple_det =</span> <span class="kw">n</span>() <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb46-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(filename,  class, value, multiple_det) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># select columns of interest</span></a>
<a class="sourceLine" id="cb46-5" data-line-number="5"><span class="st">  </span><span class="kw">ungroup</span>()</a></code></pre></div>
</div>
<div id="md-format-hv" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Format human vision data set</h3>
<p>The human vision data set (<code>ìmages_hv_jan2020.csv</code>) was previously cleaned to remove duplicated records and to summarise multiple rows that reference animals of the same species identified in the same image (see Chapter <a href="wildlife-insights-wi.html#wildlife-insights-wi">3</a> for details about these steps).</p>
<p>We begin by inspecting all the species names contained in the human vision data set using the <code>unique</code> function <span class="citation">(R Core Team <a href="#ref-R-base">2021</a>)</span>. We will eventually need to create a variable that can be compared to MD output (i.e., broad classes identifying “Blank” and “Animal”).</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1">hv_sp &lt;-<span class="st"> </span>mycsv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb47-2" data-line-number="2"><span class="st">  </span><span class="kw">pluck</span>(<span class="dv">2</span>) <span class="co"># select human vision data frame</span></a>
<a class="sourceLine" id="cb47-3" data-line-number="3">  </a>
<a class="sourceLine" id="cb47-4" data-line-number="4"><span class="co"># check all species present in the data set</span></a>
<a class="sourceLine" id="cb47-5" data-line-number="5"><span class="kw">unique</span>(hv_sp<span class="op">$</span>common_name) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sort</span>()</a></code></pre></div>
<pre><code>##  [1] &quot;Alouatta Species&quot;             &quot;Amazonian Motmot&quot;            
##  [3] &quot;Ants&quot;                         &quot;Bird&quot;                        
##  [5] &quot;Black Agouti&quot;                 &quot;Blank&quot;                       
##  [7] &quot;Bos Species&quot;                  &quot;Bush Dog&quot;                    
##  [9] &quot;Caprimulgidae Family&quot;         &quot;Capybara&quot;                    
## [11] &quot;Cervidae Family&quot;              &quot;Collared Peccary&quot;            
## [13] &quot;Common Green Iguana&quot;          &quot;Crab-eating Fox&quot;             
## [15] &quot;Crestless Curassow&quot;           &quot;Dasypus Species&quot;             
## [17] &quot;Domestic Dog&quot;                 &quot;Domestic Horse&quot;              
## [19] &quot;Fasciated Tiger-heron&quot;        &quot;Giant Anteater&quot;              
## [21] &quot;Giant Armadillo&quot;              &quot;Giant Otter&quot;                 
## [23] &quot;Insect&quot;                       &quot;Jaguar&quot;                      
## [25] &quot;Jaguarundi&quot;                   &quot;Lizards and Snakes&quot;          
## [27] &quot;Lowland Tapir&quot;                &quot;Mammal&quot;                      
## [29] &quot;Margarita Island Capuchin&quot;    &quot;Margay&quot;                      
## [31] &quot;Neotropical Otter&quot;            &quot;Northern Amazon Red Squirrel&quot;
## [33] &quot;Ocelot&quot;                       &quot;Ornate Tití Monkey&quot;          
## [35] &quot;Pecari Species&quot;               &quot;Possum Family&quot;               
## [37] &quot;Puma&quot;                         &quot;Red Brocket&quot;                 
## [39] &quot;Rodent&quot;                       &quot;Saimiri Species&quot;             
## [41] &quot;South American Coati&quot;         &quot;Southern Tamandua&quot;           
## [43] &quot;Spix&#39;s Guan&quot;                  &quot;Spotted Paca&quot;                
## [45] &quot;Tayra&quot;                        &quot;Turkey Vulture&quot;              
## [47] &quot;Turtle Order&quot;                 &quot;Unknown species&quot;             
## [49] &quot;Weasel Family&quot;                &quot;White-lipped Peccary&quot;        
## [51] &quot;White-tailed Deer&quot;</code></pre>
<p>MD output represents filenames as “camera_location/image_filename”. If camera location and filename are contained in different columns of your human vision data set, we can combine them using the <code>unite</code> function <span class="citation">(Wickham et al. <a href="#ref-R-dplyr">2019</a>)</span>. Before doing so, we use the <code>str_replace</code> function to reformat the <code>deployment_id</code> variable so that it matches the format of the MD data set. These steps will allow us to later join our human and computer vision data sets using the common variable, <code>filename</code>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1">hv_sp<span class="op">$</span>deployment_id &lt;-<span class="st"> </span>hv_sp<span class="op">$</span>deployment_id <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb49-2" data-line-number="2"><span class="st">  </span><span class="kw">str_replace</span>(<span class="dt">pattern =</span> <span class="st">&quot;-.*&quot;</span>, <span class="st">&quot;/&quot;</span>) <span class="co"># assign the same patterns found in MD filename column.</span></a>
<a class="sourceLine" id="cb49-3" data-line-number="3">  </a>
<a class="sourceLine" id="cb49-4" data-line-number="4"></a>
<a class="sourceLine" id="cb49-5" data-line-number="5">hv_sp &lt;-<span class="st"> </span>hv_sp <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb49-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">deployment_id =</span> <span class="kw">paste0</span>(<span class="st">&quot;jan2020/&quot;</span>, deployment_id)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb49-7" data-line-number="7"><span class="st">  </span><span class="kw">unite</span>(filename, <span class="kw">c</span>(<span class="st">&quot;deployment_id&quot;</span>, <span class="st">&quot;filename&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)</a></code></pre></div>
<p>We use the <code>case_when</code> function <span class="citation">(Wickham et al. <a href="#ref-R-dplyr">2019</a>)</span> to implement multiple conditional statements to create a variable, <code>class_hv</code>, containing the classes found in MD output. We select the columns of interest, including a newly created variable (<code>multiple_det</code>) that will keep track of images that have two or more species present in the same image, each recorded in a different row. For the multiple detections, we keep a single row using the <code>slice</code> function <span class="citation">(Wickham et al. <a href="#ref-R-dplyr">2019</a>)</span>.</p>
<p>We also assign a levels attribute to the <code>class</code> variable in human vision so that it is comparable to the <code>class</code> variable in the computer vision data set.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1">hv &lt;-<span class="st"> </span>hv_sp <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb50-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(filename, timestamp) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb50-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">multiple_det =</span> <span class="kw">n</span>() <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># create multiple detections column (TRUE/FALSE).</span></a>
<a class="sourceLine" id="cb50-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">class =</span> <span class="kw">case_when</span>(common_name <span class="op">==</span><span class="st"> &quot;Blank&quot;</span> <span class="op">~</span><span class="st"> </span>common_name,</a>
<a class="sourceLine" id="cb50-5" data-line-number="5">                           <span class="ot">TRUE</span> <span class="op">~</span><span class="st"> &quot;Animal&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb50-6" data-line-number="6"><span class="st">  </span><span class="kw">select</span>(filename, timestamp, class, multiple_det)  <span class="co"># select columns of interest</span></a>
<a class="sourceLine" id="cb50-7" data-line-number="7"></a>
<a class="sourceLine" id="cb50-8" data-line-number="8">hv &lt;-<span class="st"> </span>hv <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb50-9" data-line-number="9"><span class="st">  </span><span class="kw">group_by</span>(filename, timestamp,  class) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb50-10" data-line-number="10"><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb50-11" data-line-number="11"><span class="st">  </span><span class="kw">ungroup</span>()</a>
<a class="sourceLine" id="cb50-12" data-line-number="12"></a>
<a class="sourceLine" id="cb50-13" data-line-number="13"><span class="co"># Assign the levels attribute to the class column</span></a>
<a class="sourceLine" id="cb50-14" data-line-number="14">hv<span class="op">$</span>class &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(hv<span class="op">$</span>class), <span class="dt">levels =</span> all_levels)</a></code></pre></div>
</div>
<div id="merging-computer-and-human-vision-data-sets" class="section level3">
<h3><span class="header-section-number">4.6.4</span> Merging computer and human vision data sets</h3>
<p>Now that we have the same format for both human and computer vision data sets, we can use various “joins” <span class="citation">(Wickham et al. <a href="#ref-R-dplyr">2019</a>)</span> to merge the two data sets together so that we can evaluate the accuracy of MD. First, however, we will eliminate any pictures that were not processed by both humans and AI.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="co"># Determine which images have been viewed by both methods</span></a>
<a class="sourceLine" id="cb51-2" data-line-number="2">ind1 &lt;-<span class="st"> </span>cv<span class="op">$</span>filename <span class="op">%in%</span><span class="st"> </span>hv<span class="op">$</span>filename <span class="co"># in both</span></a>
<a class="sourceLine" id="cb51-3" data-line-number="3">ind2 &lt;-<span class="st"> </span>hv<span class="op">$</span>filename <span class="op">%in%</span><span class="st"> </span>cv<span class="op">$</span>filename <span class="co"># in both</span></a>
<a class="sourceLine" id="cb51-4" data-line-number="4"></a>
<a class="sourceLine" id="cb51-5" data-line-number="5">cv &lt;-<span class="st"> </span>cv[ind1,] <span class="co"># eliminate images not processed by human vision</span></a>
<a class="sourceLine" id="cb51-6" data-line-number="6">hv &lt;-<span class="st"> </span>hv[ind2,] <span class="co"># eliminate images not processed by computer vision</span></a>
<a class="sourceLine" id="cb51-7" data-line-number="7"></a>
<a class="sourceLine" id="cb51-8" data-line-number="8"><span class="co"># Number of photos eliminated</span></a>
<a class="sourceLine" id="cb51-9" data-line-number="9"><span class="kw">sum</span>(ind1 <span class="op">!=</span><span class="st"> </span><span class="ot">TRUE</span>) <span class="co"># in computer vision but not in hv</span></a></code></pre></div>
<pre><code>## [1] 5397</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="kw">sum</span>(ind2 <span class="op">!=</span><span class="st"> </span><span class="ot">TRUE</span>) <span class="co"># in human vision but not in cv</span></a></code></pre></div>
<pre><code>## [1] 2940</code></pre>
<p>Now, we can use:</p>
<ul>
<li>an <code>inner_join</code> with <code>filename</code> and <code>class</code> to determine images that have correct predictions (i.e., images with the same class assigned by computer and human vision)</li>
<li>an <code>anti_join</code> with <code>filename</code> and <code>class</code> to determine which records in the human vision data set have incorrect predictions from computer vision.</li>
<li>an <code>anti_join</code> with <code>filename</code> and <code>class</code> to determine which records in the computer vision data set have incorrect predictions.</li>
</ul>
<p>We assume the classifications from human vision to be correct and distinguish them from MD predictions. The MD predictions will be correct if they match a class assigned by human vision for a particular record and incorrect if the classes assigned by the two visions differ.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1"><span class="co"># correct predictions</span></a>
<a class="sourceLine" id="cb55-2" data-line-number="2">matched &lt;-<span class="st"> </span>cv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb55-3" data-line-number="3"><span class="st">  </span><span class="kw">inner_join</span>(<span class="dt">y =</span> hv, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;filename&quot;</span>, <span class="st">&quot;class&quot;</span>), <span class="dt">suffix =</span> <span class="kw">c</span>(<span class="st">&quot;_cv&quot;</span>, <span class="st">&quot;_hv&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">class_hv =</span> class) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-5" data-line-number="5"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">class_cv =</span> class) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-6" data-line-number="6"><span class="st">  </span><span class="kw">select</span>(filename, value, class_cv, class_hv, multiple_det_cv, multiple_det_hv)</a>
<a class="sourceLine" id="cb55-7" data-line-number="7"></a>
<a class="sourceLine" id="cb55-8" data-line-number="8"><span class="co"># records in the human vision data set whose predictions are incorrect</span></a>
<a class="sourceLine" id="cb55-9" data-line-number="9">hv_only &lt;-<span class="st"> </span>hv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb55-10" data-line-number="10"><span class="st">  </span><span class="kw">anti_join</span>(<span class="dt">y =</span> cv, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;filename&quot;</span>, <span class="st">&quot;class&quot;</span>), <span class="dt">suffix =</span> <span class="kw">c</span>(<span class="st">&quot;_hv&quot;</span>, <span class="st">&quot;_cv&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-11" data-line-number="11"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">class_hv =</span> class) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-12" data-line-number="12"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">multiple_det_hv =</span> multiple_det)</a>
<a class="sourceLine" id="cb55-13" data-line-number="13">  </a>
<a class="sourceLine" id="cb55-14" data-line-number="14"><span class="co"># records in the computer vision data set whose predictions are incorrect</span></a>
<a class="sourceLine" id="cb55-15" data-line-number="15">cv_only&lt;-<span class="st"> </span>cv <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb55-16" data-line-number="16"><span class="st">  </span><span class="kw">anti_join</span>(<span class="dt">y =</span> hv, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;filename&quot;</span>, <span class="st">&quot;class&quot;</span>), <span class="dt">suffix =</span> <span class="kw">c</span>(<span class="st">&quot;_cv&quot;</span>, <span class="st">&quot;_hv&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-17" data-line-number="17"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">class_cv =</span> class)</a></code></pre></div>
<p>We then use <code>left_join</code> to merge the predictions from the <code>cv_only</code> (computer vision) data set onto the records from the <code>hv_only</code> (human vision) data set.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1">hv_mismatch &lt;-<span class="st"> </span>hv_only <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb56-2" data-line-number="2"><span class="st">  </span><span class="kw">left_join</span>(cv_only, <span class="dt">by =</span> <span class="st">&quot;filename&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb56-3" data-line-number="3"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">multiple_det_cv =</span> multiple_det) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb56-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(filename, value, class_cv, class_hv, multiple_det_cv, multiple_det_hv)</a></code></pre></div>
<p>We then check for any computer vision records that are not yet accounted for in our data sets containing records with correct or incorrect predictions, i.e., <code>matched</code> and <code>hv_mismatch</code>, respectively.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">cv_others &lt;-<span class="st"> </span>cv_only[cv_only<span class="op">$</span>filename <span class="op">%in%</span><span class="st"> </span>hv_mismatch<span class="op">$</span>filename <span class="op">!=</span><span class="st"> </span><span class="ot">TRUE</span>,]</a>
<a class="sourceLine" id="cb57-2" data-line-number="2"><span class="kw">table</span>(cv_others<span class="op">$</span>multiple_det)</a></code></pre></div>
<pre><code>## &lt; table of extent 0 &gt;</code></pre>
<p>Then, we select only the variables we need, combine the matched and mismatched data sets, and again make sure that the classifications have the same factor levels attribute.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1">matched &lt;-<span class="st"> </span>matched <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb59-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(filename, value, class_cv, class_hv)</a>
<a class="sourceLine" id="cb59-3" data-line-number="3">hv_mismatch &lt;-<span class="st"> </span>hv_mismatch <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb59-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(filename, value, class_cv, class_hv)</a>
<a class="sourceLine" id="cb59-5" data-line-number="5"></a>
<a class="sourceLine" id="cb59-6" data-line-number="6">both_visions &lt;-<span class="st"> </span><span class="kw">rbind</span>(matched, hv_mismatch)</a>
<a class="sourceLine" id="cb59-7" data-line-number="7">both_visions<span class="op">$</span>class_cv &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(both_visions<span class="op">$</span>class_cv), <span class="dt">levels =</span> all_levels)</a>
<a class="sourceLine" id="cb59-8" data-line-number="8">both_visions<span class="op">$</span>class_hv &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(both_visions<span class="op">$</span>class_hv), <span class="dt">levels =</span> all_levels)</a></code></pre></div>
</div>
<div id="confusion-matrix-and-performance-measures-1" class="section level3">
<h3><span class="header-section-number">4.6.5</span> Confusion matrix and performance measures</h3>
<p>Finally, we can proceed with estimating a confusion matrix and various AI performance measures using the <code>confusionMatrix</code> function from the <code>caret</code> package <span class="citation">(Kuhn <a href="#ref-R-caret">2021</a>)</span> and specifying a 0.65 confidence threshold to accept MD predictions. We can then plot the confusion matrix using <code>ggplot2</code> <span class="citation">(Wickham et al. <a href="#ref-R-ggplot2">2018</a>)</span>. The <code>confusionMatrix</code> function requires a data argument for predicted classes and a reference for true classifications, both as factor classes and with the same factor levels. We specify <code>mode = &quot;prec_recall&quot;</code> when calling the <code>confusionMatrix</code> function <span class="citation">(Kuhn <a href="#ref-R-caret">2021</a>)</span> to generate estimates of precision and recall.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1"><span class="kw">library</span>(caret) <span class="co"># to inspect model performance</span></a>
<a class="sourceLine" id="cb60-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb60-3" data-line-number="3"></a>
<a class="sourceLine" id="cb60-4" data-line-number="4"><span class="co"># Estimate confusion matrix</span></a>
<a class="sourceLine" id="cb60-5" data-line-number="5">both_visions_<span class="fl">0.65</span> &lt;-<span class="st"> </span>both_visions</a>
<a class="sourceLine" id="cb60-6" data-line-number="6">both_visions_<span class="fl">0.65</span><span class="op">$</span>class_cv[both_visions_<span class="fl">0.65</span><span class="op">$</span>value <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.65</span>] &lt;-<span class="st"> &quot;Blank&quot;</span>  </a>
<a class="sourceLine" id="cb60-7" data-line-number="7">cm_md_<span class="fl">0.65</span> &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> both_visions_<span class="fl">0.65</span><span class="op">$</span>class_cv, </a>
<a class="sourceLine" id="cb60-8" data-line-number="8">                      <span class="dt">reference =</span> both_visions_<span class="fl">0.65</span><span class="op">$</span>class_hv, </a>
<a class="sourceLine" id="cb60-9" data-line-number="9">                      <span class="dt">mode =</span> <span class="st">&quot;prec_recall&quot;</span>)</a>
<a class="sourceLine" id="cb60-10" data-line-number="10"></a>
<a class="sourceLine" id="cb60-11" data-line-number="11">cm_md_<span class="fl">0.65</span>_blank &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> both_visions_<span class="fl">0.65</span><span class="op">$</span>class_cv, </a>
<a class="sourceLine" id="cb60-12" data-line-number="12">                      <span class="dt">reference =</span> both_visions_<span class="fl">0.65</span><span class="op">$</span>class_hv, </a>
<a class="sourceLine" id="cb60-13" data-line-number="13">                      <span class="dt">mode =</span> <span class="st">&quot;prec_recall&quot;</span>,</a>
<a class="sourceLine" id="cb60-14" data-line-number="14">                      <span class="dt">positive =</span> <span class="st">&quot;Blank&quot;</span>)</a>
<a class="sourceLine" id="cb60-15" data-line-number="15"></a>
<a class="sourceLine" id="cb60-16" data-line-number="16"><span class="co"># Plot confusion matrix</span></a>
<a class="sourceLine" id="cb60-17" data-line-number="17">plot_cm_md_<span class="fl">0.65</span> &lt;-<span class="st"> </span>cm_md_<span class="fl">0.65</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb60-18" data-line-number="18"><span class="st">  </span><span class="kw">pluck</span>(<span class="st">&quot;table&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb60-19" data-line-number="19"><span class="st">  </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb60-20" data-line-number="20"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">Frequency =</span> Freq) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb60-21" data-line-number="21"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y=</span>Prediction, <span class="dt">x=</span>Reference, <span class="dt">fill=</span>Frequency)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb60-22" data-line-number="22"><span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb60-23" data-line-number="23"><span class="st">  </span><span class="kw">scale_fill_gradient</span>(<span class="dt">low =</span> <span class="st">&quot;#D6EAF8&quot;</span>,<span class="dt">high =</span> <span class="st">&quot;#2E86C1&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb60-24" data-line-number="24"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb60-25" data-line-number="25"><span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> Frequency), <span class="dt">size =</span> <span class="dv">3</span>) <span class="co">#set size to 3</span></a>
<a class="sourceLine" id="cb60-26" data-line-number="26"></a>
<a class="sourceLine" id="cb60-27" data-line-number="27">plot_cm_md_<span class="fl">0.65</span></a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:confmatrix-md"></span>
<img src="03-md_files/figure-html/confmatrix-md-1.png" alt="Confusion matrix applied to classifications from MegaDetector using a confidence threshold of 0.65." width="672" />
<p class="caption">
Figure 4.8: Confusion matrix applied to classifications from MegaDetector using a confidence threshold of 0.65.
</p>
</div>
<p>Now we can use the confusion matrix to estimate metrics of model performance including accuracy, precision, recall and F-1 score (See Chapter <a href="introduction.html#introduction">1</a> for metrics description).</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1">(overall_accuracy &lt;-<span class="st"> </span>cm_md_<span class="fl">0.65</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2"><span class="st">  </span><span class="kw">pluck</span>(<span class="st">&quot;overall&quot;</span>, <span class="st">&quot;Accuracy&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-3" data-line-number="3"><span class="st">  </span><span class="kw">round</span>(., <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 0.93</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1">classes_metrics_md_<span class="fl">0.65</span> &lt;-<span class="st"> </span><span class="kw">list</span>(cm_md_<span class="fl">0.65</span>, cm_md_<span class="fl">0.65</span>_blank)</a>
<a class="sourceLine" id="cb63-2" data-line-number="2"></a>
<a class="sourceLine" id="cb63-3" data-line-number="3">classes_metrics_md_<span class="fl">0.65</span>  &lt;-<span class="st"> </span>classes_metrics_md_<span class="fl">0.65</span> <span class="op">%&gt;%</span><span class="st">   </span></a>
<a class="sourceLine" id="cb63-4" data-line-number="4"><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="kw">pluck</span>(.x, <span class="st">&quot;byClass&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-5" data-line-number="5"><span class="st">  </span><span class="kw">t</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-6" data-line-number="6"><span class="st">  </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-7" data-line-number="7"><span class="st">  </span><span class="kw">select</span>(Precision, Recall, F1) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-8" data-line-number="8"><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">where</span>(is.numeric), <span class="op">~</span><span class="kw">round</span>(., <span class="dv">2</span>)))) </a>
<a class="sourceLine" id="cb63-9" data-line-number="9"></a>
<a class="sourceLine" id="cb63-10" data-line-number="10">classes_metrics_md_<span class="fl">0.65</span> &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">pluck</span>(classes_metrics_md_<span class="fl">0.65</span>, <span class="dv">1</span>), <span class="kw">pluck</span>(classes_metrics_md_<span class="fl">0.65</span>,<span class="dv">2</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb63-11" data-line-number="11"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Class =</span> <span class="kw">c</span>(<span class="st">&quot;Animal&quot;</span>, <span class="st">&quot;Blank&quot;</span>), <span class="dt">.before =</span> Precision)</a></code></pre></div>
<div id="htmlwidget-8f6a82d6dafe8ba51677" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-8f6a82d6dafe8ba51677">{"x":{"filter":"none","vertical":false,"data":[["1","2"],["Animal","Blank"],[0.98,0.77],[0.93,0.93],[0.96,0.84]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Class<\/th>\n      <th>Precision<\/th>\n      <th>Recall<\/th>\n      <th>F1<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<table>
<caption>
<span id="tab:myDThtmltools2">Table 4.1: </span>Model performance metrics for the classes detected by MegaDetector using a 0.65 confidence threshold.
</caption>
</table>
<p>We see that the model is really good at picking up animals, with a precision of 98% at a 93% recall. Thus, we expect that 93% of the animals present in our images will be picked up by MD. Further, only 2% of the images flagged as having an animal will not. We can also inspect model performance for a range of confidence thresholds as we demonstrate in the next section.</p>
</div>
<div id="md-thresholds" class="section level3">
<h3><span class="header-section-number">4.6.6</span> Confidence thresholds</h3>
<p>Lets begin by looking at the confidence values associated with each MD classification using the <code>geom_bar</code> function <span class="citation">(Wickham et al. <a href="#ref-R-ggplot2">2018</a>)</span>. Before plotting, we filter the <code>both_visions</code> data frame to remove blanks as they do not have associated confidence values.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">both_visions_nb &lt;-<span class="st"> </span>both_visions <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb64-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(class_cv <span class="op">!=</span><span class="st"> &quot;Blank&quot;</span>)</a>
<a class="sourceLine" id="cb64-3" data-line-number="3"></a>
<a class="sourceLine" id="cb64-4" data-line-number="4"><span class="co"># Plot confidence values </span></a>
<a class="sourceLine" id="cb64-5" data-line-number="5">both_visions_nb <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb64-6" data-line-number="6"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(value, <span class="dt">group =</span> class_cv, <span class="dt">colour =</span> class_cv)) <span class="op">+</span></a>
<a class="sourceLine" id="cb64-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_bar</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb64-8" data-line-number="8"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>class_cv, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb64-9" data-line-number="9"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Empirical distribution&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;Confidence values&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb64-10" data-line-number="10"><span class="st">  </span><span class="kw">scale_color_viridis_d</span>()</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="03-md_files/figure-html/unnamed-chunk-12-1.png" alt="Distribution of confidence values for animals predicted by MegaDetector." width="864" />
<p class="caption">
Figure 4.9: Distribution of confidence values for animals predicted by MegaDetector.
</p>
</div>
<p>We see that the distribution of confidence values for “Animal” classifications is left skewed with most records having high confidence values suggesting that the AI prediction is presumed to be correct most of the time.</p>
<p>To inspect how precision and recall change when different confidence thresholds are established for assigning a class predicted by computer vision, we define a function that will calculate these metrics for a user-defined confidence threshold. This function will assign a “Blank” label whenever the confidence for a computer vision prediction is below a particular confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall change with the confidence threshold and choose a threshold that balances these two performance metrics.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1">threshold_for_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">conf_threshold =</span> <span class="fl">0.7</span>) {</a>
<a class="sourceLine" id="cb65-2" data-line-number="2">  tmp &lt;-<span class="st"> </span>both_visions</a>
<a class="sourceLine" id="cb65-3" data-line-number="3">  tmp<span class="op">$</span>class_cv[tmp<span class="op">$</span>value <span class="op">&lt;</span><span class="st"> </span>conf_threshold] &lt;-<span class="st"> &quot;Blank&quot;</span> </a>
<a class="sourceLine" id="cb65-4" data-line-number="4">  <span class="co"># assign a &quot;No Detection&quot; (i.e., Blank) whenever the confidence value of</span></a>
<a class="sourceLine" id="cb65-5" data-line-number="5">  <span class="co"># a prediction (max_confidence) is lower than the threshold provided as </span></a>
<a class="sourceLine" id="cb65-6" data-line-number="6">  <span class="co"># an argument in the function</span></a>
<a class="sourceLine" id="cb65-7" data-line-number="7">  cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> tmp<span class="op">$</span>class_cv, </a>
<a class="sourceLine" id="cb65-8" data-line-number="8">                        <span class="dt">reference =</span> tmp<span class="op">$</span>class_hv, </a>
<a class="sourceLine" id="cb65-9" data-line-number="9">                        <span class="dt">mode =</span> <span class="st">&quot;prec_recall&quot;</span>) </a>
<a class="sourceLine" id="cb65-10" data-line-number="10">  <span class="co"># use the confusionMatrix function from the caret package using the</span></a>
<a class="sourceLine" id="cb65-11" data-line-number="11">  <span class="co"># class_cv containing the new labels according to a particular</span></a>
<a class="sourceLine" id="cb65-12" data-line-number="12">  <span class="co"># confidence threshold</span></a>
<a class="sourceLine" id="cb65-13" data-line-number="13">  classes_metrics &lt;-<span class="st"> </span>cm <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># get confusion matrix</span></a>
<a class="sourceLine" id="cb65-14" data-line-number="14"><span class="st">    </span><span class="kw">pluck</span>(<span class="st">&quot;byClass&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># get metrics by class</span></a>
<a class="sourceLine" id="cb65-15" data-line-number="15"><span class="st">    </span><span class="kw">t</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb65-16" data-line-number="16"><span class="st">    </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb65-17" data-line-number="17"><span class="st">    </span><span class="kw">select</span>(Precision, Recall, F1) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb65-18" data-line-number="18"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">conf_threshold =</span> conf_threshold)</a>
<a class="sourceLine" id="cb65-19" data-line-number="19">  </a>
<a class="sourceLine" id="cb65-20" data-line-number="20">  <span class="kw">return</span>(classes_metrics) <span class="co"># return a data frame with metrics for every class</span></a>
<a class="sourceLine" id="cb65-21" data-line-number="21">}</a></code></pre></div>
<p>Let’s estimate model performance metrics for confidence values ranging from 0.1 to 0.99 using the map_df function <span class="citation">(Henry and Wickham <a href="#ref-purrr">2020</a>)</span> . The map_df function <span class="citation">(Henry and Wickham <a href="#ref-purrr">2020</a>)</span> returns a dataframe object. Once we get a dataframe of model performance metrics for a range of confidence values, we can plot the results using the <code>ggplot2</code> package <span class="citation">(Wickham et al. <a href="#ref-R-ggplot2">2018</a>)</span>.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1">conf_vector =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="fl">0.99</span>, <span class="dt">length=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb66-2" data-line-number="2"></a>
<a class="sourceLine" id="cb66-3" data-line-number="3">metrics_all_confs &lt;-<span class="st"> </span><span class="kw">map_df</span>(conf_vector, threshold_for_metrics)</a></code></pre></div>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1">prec_rec_md &lt;-<span class="st"> </span>metrics_all_confs <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb67-2" data-line-number="2"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">Confidence_threshold =</span> conf_threshold) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb67-3" data-line-number="3"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Recall, <span class="dt">y =</span> Precision)) <span class="op">+</span></a>
<a class="sourceLine" id="cb67-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">size =</span> Confidence_threshold, <span class="dt">color =</span> <span class="st">&quot;#8ecae6&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb67-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_size</span>(<span class="dt">range =</span> <span class="kw">c</span>(<span class="fl">0.1</span>,<span class="dv">3</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb67-6" data-line-number="6"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Recall&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Precision&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb67-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color =</span> <span class="st">&quot;#8ecae6&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb67-8" data-line-number="8"><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="st">&quot;#8ecae6&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;Class&quot;</span>, <span class="dt">labels =</span> <span class="st">&quot;Animal&quot;</span>)</a>
<a class="sourceLine" id="cb67-9" data-line-number="9"></a>
<a class="sourceLine" id="cb67-10" data-line-number="10">prec_rec_md</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:md-thresholds"></span>
<img src="03-md_files/figure-html/md-thresholds-1.png" alt="Precision and recall for different confidence thresholds for the animal class predicted by MegaDetector." width="864" />
<p class="caption">
Figure 4.10: Precision and recall for different confidence thresholds for the animal class predicted by MegaDetector.
</p>
</div>
<p>We see that as we increase the confidence threshold, precision increases and recall decreases for the “Animal” class (Figure <a href="megadetector---microsoft-ai.html#fig:md-thresholds">4.10</a>). Ideally, we would like to choose a confidence threshold that maximizes both precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the class is truly present when AI identifies the class as being present in an image (Chapter <a href="introduction.html#introduction">1</a>). If AI suffers from low precision, then we may have to manually review photos that AI tags as having a class present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a class in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing a class that is truly present. To remedy this problem, we would need to review images where AI says the class is absent in order to reduce false negatives.</p>
<p>Let’s say that we were willing to miss only 3% of the animals present. In this case, we could pick the confidence threshold that maximizes precision under the constraint that recall does not fall below 97%.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="co"># Thresholds that meet our criterion that Recall &gt;= 0.97</span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2">conf_meets &lt;-<span class="st"> </span>metrics_all_confs <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb68-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="kw">across</span>(<span class="kw">where</span>(is.numeric), <span class="op">~</span><span class="kw">round</span>(., <span class="dv">2</span>))) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb68-4" data-line-number="4"><span class="st">  </span><span class="kw">filter</span>(Recall <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.97</span>)</a></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-16">Table 4.2: </span>Performance metrics using a confidence threshold that
maximizes precision for a 97% recall.</caption>
<thead>
<tr class="header">
<th align="right">Precision</th>
<th align="right">Recall</th>
<th align="right">F1</th>
<th align="right">conf_threshold</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.95</td>
<td align="right">0.97</td>
<td align="right">0.96</td>
<td align="right">0.1</td>
</tr>
</tbody>
</table>
<p>We see that we can increase Precision to 95% (while keeping Recall = 97%) by using a confidence threshold of 0.1.
Thus, if we integrate MD output with Timelapse 2, filtering using a confidence threshold of 0.1, we expect to capture 97% of Animals that are truly present and 95% of the flagged images should actually include one or more animals.</p>
<p>Let’s examine the confusion matrix using this threshold.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">threshold &lt;-<span class="st"> </span>conf_meets[<span class="kw">which.max</span>(conf_meets<span class="op">$</span>Precision), <span class="dv">4</span>]</a>
<a class="sourceLine" id="cb69-2" data-line-number="2">  </a>
<a class="sourceLine" id="cb69-3" data-line-number="3">conf_red &lt;-<span class="st"> </span>both_visions</a>
<a class="sourceLine" id="cb69-4" data-line-number="4">conf_red<span class="op">$</span>class_cv[conf_red<span class="op">$</span>value <span class="op">&lt;</span><span class="st"> </span>threshold] &lt;-<span class="st"> &quot;Blank&quot;</span>  </a>
<a class="sourceLine" id="cb69-5" data-line-number="5"></a>
<a class="sourceLine" id="cb69-6" data-line-number="6">  </a>
<a class="sourceLine" id="cb69-7" data-line-number="7">cm_md_th &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> conf_red<span class="op">$</span>class_cv, </a>
<a class="sourceLine" id="cb69-8" data-line-number="8">                        <span class="dt">reference =</span> conf_red<span class="op">$</span>class_hv, </a>
<a class="sourceLine" id="cb69-9" data-line-number="9">                        <span class="dt">mode =</span> <span class="st">&quot;prec_recall&quot;</span>)</a>
<a class="sourceLine" id="cb69-10" data-line-number="10"></a>
<a class="sourceLine" id="cb69-11" data-line-number="11">plot_cm_md_th &lt;-<span class="st"> </span>cm_md_th <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb69-12" data-line-number="12"><span class="st">        </span><span class="kw">pluck</span>(<span class="st">&quot;table&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb69-13" data-line-number="13"><span class="st">        </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb69-14" data-line-number="14"><span class="st">        </span><span class="kw">rename</span>(<span class="dt">Frequency =</span> Freq) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb69-15" data-line-number="15"><span class="st">        </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y=</span>Prediction, <span class="dt">x=</span>Reference, <span class="dt">fill=</span>Frequency)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb69-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb69-17" data-line-number="17"><span class="st">  </span><span class="kw">scale_fill_gradient</span>(<span class="dt">low =</span> <span class="st">&quot;#D6EAF8&quot;</span>,<span class="dt">high =</span> <span class="st">&quot;#2E86C1&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb69-18" data-line-number="18"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb69-19" data-line-number="19"><span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> Frequency), <span class="dt">size =</span> <span class="dv">3</span>)</a></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="kw">library</span>(patchwork)</a>
<a class="sourceLine" id="cb70-2" data-line-number="2"></a>
<a class="sourceLine" id="cb70-3" data-line-number="3">plot_cm_md_<span class="fl">0.65</span> &lt;-<span class="st"> </span>plot_cm_md_<span class="fl">0.65</span> <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;None&quot;</span>)</a>
<a class="sourceLine" id="cb70-4" data-line-number="4">plot_cm_md_th &lt;-<span class="st"> </span>plot_cm_md_th <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;None&quot;</span>)</a>
<a class="sourceLine" id="cb70-5" data-line-number="5"></a>
<a class="sourceLine" id="cb70-6" data-line-number="6">(plots_cms_md &lt;-<span class="st"> </span>plot_cm_md_<span class="fl">0.65</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb70-7" data-line-number="7"><span class="st">  </span>plot_cm_md_th <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb70-8" data-line-number="8"><span class="st">  </span><span class="kw">plot_annotation</span>(<span class="dt">tag_levels =</span> <span class="st">&#39;A&#39;</span>))</a></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:compare"></span>
<img src="03-md_files/figure-html/compare-1.png" alt="Confusion matrices processed using 0.65 (A) and 0.1 (B) as confidence thresholds." width="100%" />
<p class="caption">
Figure 4.11: Confusion matrices processed using 0.65 (A) and 0.1 (B) as confidence thresholds.
</p>
</div>
<p>Comparing the confusion matrix with our original (using a 0.65 confidence threshold; Figure <a href="megadetector---microsoft-ai.html#fig:compare">4.11</a>), we see that we have decreased the false negatives using a confidence threshold of 0.1 (i.e., cases where MD suggests a blank image but an animal is present; last row, first column) but increased the number of false positives (i.e., cases where MD suggests an animal is present, but the image is blank; first row, second column).</p>
</div>
</div>
<div id="conclusions-1" class="section level2">
<h2><span class="header-section-number">4.7</span> Conclusions</h2>
<p>We have seen how to use MD and how to integrate its output with Timelapse 2 for using AI while processing camera trap data. Additionally, we illustrated how to evaluate MD performance by comparing true classifications with computer predictions. We found that MD has a high performance to detect animals in images. Thus, the human labor required to review photos can be reliably focused on images with the “Animal” class predictions.</p>
<p>We also showed how users can explore MD performance by comparing confusion matrices estimated using different confidence thresholds. This comparison will be useful to understand the trade-off between precision and recall, and help users choose a confidence threshold that maximizes these metrics using their particular data sets.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-magrittr">
<p>Bache, Stefan Milton, and Hadley Wickham. 2020. <em>Magrittr: A Forward-Pipe Operator for R</em>. <a href="https://CRAN.R-project.org/package=magrittr">https://CRAN.R-project.org/package=magrittr</a>.</p>
</div>
<div id="ref-beery2019efficient">
<p>Beery, Sara, Dan Morris, and Siyu Yang. 2019. “Efficient Pipeline for Camera Trap Image Review.” <em>arXiv Preprint arXiv:1907.06772</em>.</p>
</div>
<div id="ref-greenberg-timelapse">
<p>Greenberg, Saul. 2020. “Timelapse 2.0 User Guide.” Computer Program. Greenberg Consulting Inc. / University of Calgary.</p>
</div>
<div id="ref-greenberg-design">
<p>Greenberg, Saul, Theresa Godin, and Jesse Whittington. 2019. “Design Patterns for Wildlife-Related Camera Trap Image Analysis.” Journal Article. <em>Ecology and Evolution</em> 9 (24): 13706–30. <a href="https://doi.org/10.1002/ece3.5767">https://doi.org/10.1002/ece3.5767</a>.</p>
</div>
<div id="ref-purrr">
<p>Henry, Lionel, and Hadley Wickham. 2020. <em>Purrr: Functional Programming Tools</em>. <a href="https://CRAN.R-project.org/package=purrr">https://CRAN.R-project.org/package=purrr</a>.</p>
</div>
<div id="ref-R-caret">
<p>Kuhn, Max. 2021. <em>Caret: Classification and Regression Training</em>. <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.</p>
</div>
<div id="ref-here">
<p>Müller, Kirill. 2017. <em>Here: A Simpler Way to Find Your Files</em>. <a href="https://CRAN.R-project.org/package=here">https://CRAN.R-project.org/package=here</a>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2021. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-R-ggplot2">
<p>Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, and Kara Woo. 2018. <em>Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics</em>. <a href="https://CRAN.R-project.org/package=ggplot2">https://CRAN.R-project.org/package=ggplot2</a>.</p>
</div>
<div id="ref-R-dplyr">
<p>Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2019. <em>Dplyr: A Grammar of Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr">https://CRAN.R-project.org/package=dplyr</a>.</p>
</div>
<div id="ref-R-tidyr">
<p>Wickham, Hadley, and Lionel Henry. 2018. <em>Tidyr: Easily Tidy Data with ’Spread()’ and ’Gather()’ Functions</em>. <a href="https://CRAN.R-project.org/package=tidyr">https://CRAN.R-project.org/package=tidyr</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="wildlife-insights-wi.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mlwic2-machine-learning-for-wildlife-image-classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-md.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["CTWorkflows.pdf", "CTWorkflows.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
