[["index.html", "Guide for using artificial intelligence systems for camera trap data processing", " Guide for using artificial intelligence systems for camera trap data processing Juliana Velez and John Fieberg 2022-02-01 Cover photograph: Lowland tapir in El Rey Zamuro Reserve, Central Colombia, July 2019. Photo taken by Joares A. May. Suggested Citation: Velez, J. and J. Fieberg. 2022. Guide for using artificial intelligence systems for camera trap data processing. https://ai-camtraps.netlify.app/. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Our objectives in writing this guideline are to: Describe the steps needed to set up and process camera trap data using popular artificial intelligence (AI) platforms, including Wildlife Insights, MegaDetector, MLWIC2, and Conservation AI. Demonstrate common workflows for analyzing camera trap data using these platforms via a case study in which we process data collected by the lead author. The aim of the case study project is to develop a joint species distribution model integrating data from camera traps and acoustic sensors to understand interactions between wildlife species in multi-functional landscapes in Colombia that support both biological diversity and economic activities such as cattle ranching. Each chapter covers a different AI system, and we provide appropriate links to instruction manuals and other resources for researchers looking for additional documentation. We describe the steps required to set up the platforms, upload pictures (e.g., required folder structure), and include and format metadata (e.g., geographical coordinates of locations, deployment dates, and other deployment information such as camera height, use of bait, etc.). We then provide guidance on how to use the artificial intelligence platforms for object detection (e.g., to separate blanks from non-blanks) and species classification. Importantly, we also demonstrate methods for evaluating the performance of AI platforms. Before AI platforms can be evaluated, users will need to manually label a subset of images which can then be compared with AI output. This labeling can be done using a variety of available software (e.g., Scotson et al. 2017), but the resulting data should include, at minimum, the 1) image filename, 2) camera location and 3) species name. The first two variables (e.g., filename and location) are needed to match records from the human-labeled and AI-labeled data sets (hereafter human and computer vision, respectively), and the third variable will allow one to compare human and AI-generated labels. Having a subset of labeled images will allow you to assess how a particular AI model is performing with your data set and determine appropriate use given its performance. We provide annotated R code and examples demonstrating how to compute model performance metrics estimated using categories described in Table 1.1 that classify correct and incorrect predictions. Table 1.1: Notation and categories of classifications used to estimate model performance metrics. NotationDescription TP - True PositivesNumber of observations where the species was correctly identified as being present in the photo. TN - True NegativesNumber of observations where the species was correctly identified as being absent in the photo. FP - False PositivesNumber of observations where the species was absent, but the AI classified the species as being present. FN - False NegativesNumber of observations where the species was present, but the AI classified the species as being absent. Performance metrics include model accuracy, precision, recall and F1 score (Table 1.2; Sokolova and Lapalme 2009). To describe these metrics, we will refer to AI classifications as “predictions” and human vision classifications as “true classifications”. Accuracy is the proportion of correct AI predictions in the data set (Kuhn and Vaughan 2021), precision is the probability that the species is present given it is predicted to be present, and recall is the probability a species is predicted to be present given it is truly present; F1 score is a weighted average of precision and recall (Table 1.2). When inspecting model performance, it can be useful to calculate these metrics separately for each species. ` Table 1.2: Metrics used to assess model performance MetricsEquationInterpretation Accuracy(TP+TN)/(TP+FP+TN+FN)Proportion of correct predictions in a data set. PrecisionTP/(TP+FP)Probability the species is correctly classified as present given that the AI system classified it as present. RecallTP/(TP+FN)Probability the species is correctly classified as present given that the species truly is present. F1 Score2*precision*recall / (precision + recall)Weighted average of precision and recall. AI platforms typically assign a confidence level to each classification, with higher values reflective of more certain classifications. These confidence levels can be used to post-process the data in a way that trades off precision and recall. For example, one can choose to only accept classifications that have a high level of confidence. Doing so will typically reduce the number of false positives, leading to high levels of precision (i.e., users can be more confident that the species is truly present when AI returns a species classification). The number of true positives, and thus recall, may also be reduced but hopefully to a lesser extent. References "],["camera-trap-data.html", "Chapter 2 Camera-trap data", " Chapter 2 Camera-trap data We evaluated model performance using data from a camera trap survey performed from January - July 2020 for wildlife detection within the private natural reserves El Rey Zamuro (31 km2) and Las Unamas (40 km2), located in the Meta department in the Orinoquia region in central Colombia. During the survey period, we collected 112,247 images from a 50-camera-trap array, with cameras spaced 1-km apart; 20 percent of the images were blank and 80 percent contained at least one animal. Images were stored and reviewed by experts using the WI platform. WI was chosen because it provides advanced processing capabilities that helped to accelerate image review (e.g., multiple image selection, image editing and infrastructure for collaborative data processing). Expert (i.e., human vision) labels were compared to classifications by computer vision derived from AI models associated with WI (downloaded in February 2021), MD (version 4.1) and MLWIC2 (version 1.0) platforms to determine how well these models would perform when applied to data that were not included in the training data set. Records containing the “Human” class were removed from the data set; these were predominately associated with images during camera setup. Figure 2.1: Camera trap images collected in the reserves El Rey Zamuro and Las Unamas, located in the Meta department in central Colombia. "],["wildlife-insights-wi.html", "Chapter 3 Wildlife Insights (WI) 3.1 Set-up 3.2 Upload/format data 3.3 Upload/enter metadata 3.4 Processing images - AI module 3.5 Post-AI image processing 3.6 Using AI output 3.7 Assessing AI performance 3.8 Conclusions", " Chapter 3 Wildlife Insights (WI) Wildlife Insights (WI) is an initiative developed by Conservation International in partnership with the Wildlife Conservation Society, World Wildlife Fund, Zoological Society of London, The Smithsonian Institution, North Carolina Museum of Natural Sciences, Yale University and Google (Ahumada et al. 2019). WI provides an interface and tools to support workflows for processing, visualizing, and analyzing camera trap data. These tools include infrastructure to store, review and process images, AI to classify species and blanks, and an analysis engine for implementing common statistical methods with camera trap data (e.g., estimation of species’ activity patterns, occupancy, density and diversity indices) (Ahumada et al. 2019). Beyond serving as an interface for image processing and data analysis, WI was also conceived as a data repository for hosting camera trap data collected worldwide; images and associated metadata stored in WI can be downloaded by the public after sensitive content (e.g., images with people or endangered species) has been removed and once an embargo time (maximum 48 months) provided by data providers has passed (Ahumada et al. 2019). WI provides comprehensive guides for navigating the platform and tutorials showing step-by-step usage of the system’s features. This documentation can be found here: https://www.wildlifeinsights.org/get-started. Additionally, WI provides references describing how their AI models work along with a table listing species used for model training and performance metrics for each species, which users can consult here: https://www.wildlifeinsights.org/about-wildlife-insights-ai. We synthesize some of the key navigation steps and illustrate how one can evaluate performance of built-in AI models. Specifically, we provide code for comparing human classifications with model predictions for a subset of your images. This comparison will be useful to better understand how AI models are likely to perform with your particular images and whether AI may be able to provide accurate enough classification for some of your species. Before we get started: if you plan to compare model predictions with human classifications, you should download computer vision identifications right after uploading pictures to the WI platform (see Section 3.7). It is important that you have a record of the WI classifications before you do any processing on your data. 3.1 Set-up Create an account here https://app.wildlifeinsights.org/join In WI, you can structure your data hierarchically. Below, we provide an example structure from our camera trapping project in Colombia: Organization: University of Minnesota Initiative: Wildlife monitoring in South America using camera traps. Project: Large mammals Colombia. Subprojects: Jul2019-Jan2020 Deployment Jan2020-Jul2020 Deployment Location: We use alphanumerical codes to name each camera trap location, with letters representing different areas within our study area (Figure 3.1). Each location has its own geographical coordinates. Deployment: Information for deployments include temporal record (start and end dates) of a camera trap survey within a particular location. In our example, for our location “A08” a camera trap was deployed from 2019-07-10 to 2020-01-08 (Figure 3.2). This data structure allows you to manage multiple collaborators and data sets collected by different organizations and teams but associated with a single purpose (Initiatives 2021). For example, an “Initiative” allows you to share a project between different organizations (Initiatives 2021) which facilitates data management and labor distribution. Hierarchical data storage also might help to organize and filter subsets of images or data. For example “Projects” can contain “Subprojects” that might represent groups of deployments and/or locations. See the Glossary page for more terms found in WI https://www.wildlifeinsights.org/get-started/glossary. Figure 3.1: Locations of camera traps mapped in the Wildlife Insights platform along with location names and geographical coordinates. Figure 3.2: Deployments showing “Start date” and “End date” for each camera loaction. 3.2 Upload/format data You can upload already labeled images (e.g., for storing and managing pictures in the cloud) or unlabeled images to be processed. For labeled images, you will need to (re)format images’ metadata using the WI batch upload templates and transfer images from a public URL (e.g., Google Drive) or directly to the Google Cloud Platform. See https://www.wildlifeinsights.org/get-started/upload/bulk-data-uploads For unlabeled images, you can upload images via the WI platform. Images will then be stored in the Google Cloud Platform and displayed in the user’s project. See https://www.wildlifeinsights.org/get-started/upload/upload-new-data 3.3 Upload/enter metadata Metadata, such as the time and date each picture was taken, the filename, camera trap name and Exif data (camera settings: photo exposure, ISO and aperture) are automatically read once unlabeled pictures are uploaded. Figure 3.3: Picture metadata displayed after uploading an image to Wildlife Insights online platform. You can provide additional metadata, including coordinates for the camera trap or other features associated with camera deployment (e.g., dates, camera height, settings, use of bait, etc.). This information can be entered manually for each deployment or you can use a CSV file formatted using the WI template for a bulk deployment upload. See the deployments guide https://www.wildlifeinsights.org/get-started/manage-metadata/deployments. 3.4 Processing images - AI module Once images are uploaded, they will be processed by WI’s AI model. Computer vision classifications will be available in the WI platform as soon as your pictures are uploaded (which can take ~ 11 minutes per 1000 pictures with a 225 Mbps internet upload speed). Additionally, after pictures are uploaded, you can download the output (see download tab in the upper right corner of Figure 3.4). The output will include a CSV file with the AI classifications (See Section 3.6 for more information on WI output); you will receive an email with a link for downloading the output approximately 5 minutes after requesting it. Again, to facilitate evaluation of AI performance, we recommend downloading this CSV file before you do any other manipulations in the WI platform. Your project in WI will include the uploaded pictures in the “Identify” tab. Species classifications will be shown whenever the confidence values associated with the AI-classifications are above 65% and 95% for species and blanks, respectively; otherwise images will get a “No CV Result” label (Identifications 2021). You will be able to verify if these classifications are correct, after which they will be moved to the “Catalogued” tab (Figure 3.4). Figure 3.4: WI processing module after uploading pictures. 3.5 Post-AI image processing WI provides a platform with multiple tools for reviewing images, allowing users to verify AI output. In addition, users can: Sort images by “Date taken”, “Upload date” or “Last modification” (the latter only for “Catalogued” images) (Figure 3.4). Filter images by categories such as Subprojects, Deployments, Species, Status (e.g., Blank or Not blank) or Photos (e.g., Highlighted pictures for quick access or Not highlighted) (Figure 3.4). Edit under- or over-exposed pictures by adjusting brightness, contrast and saturation. Edit identifications using bulk actions by selecting and entering information for multiple pictures at a time (e.g., for 100 or 200 images). Group images within a Burst defined by a timeframe (from 0-600 seconds) to perform bulk actions. Manage collaborations for data processing by assigning different roles with different levels of data access (e.g., project owner, editor, contributor, tagger, viewer). 3.6 Using AI output All the above mentioned processing tools can be used to review and verify AI output presented in the “Identify” tab. You can approve computer vision identifications or edit them in the processing module. You can also include additional identifications if more than 1 animal (of the same or different species) is present in the picture and add other identifying information (e.g., sex, age, markings for each individual) or other remarks (e.g., comments or observations) that may be useful. When you download the resulting output, you will receive 4 different files that capture data related to your cameras, their deployments, and your projects (Figure 3.5): cameras.csv: contains metadata related to the cameras, including camera_id, make, model, serial_number and year purchased. deployments.csv: contains deployment dates, geographical coordinates, details of camera trap placement and camera settings. images.csv: includes classifications and features recorded for each image. projects.csv: includes project details such as project objectives, licenses for metadata and images, and information about the sampling design used for camera trap deployment. Figure 3.5: WI files created when you download project information and AI processed data. In additional to these 4 CSV files, the folder with downloaded data will contain a tutorial for downloading images from WI and a file containing WI terms of use and privacy policy (Figure 3.5). You can also quickly inspect AI results in the Summary tab (upper left of Figure 3.4). You will see a map with your camera locations and a summary of the species in your data set (Figure 3.6). Figure 3.6: WI summary of images by type and identified species. 3.7 Assessing AI performance AI classification systems have been improving, but their performance is still highly variable, both across study sites and species (Tabak et al. 2018). Thus, it is extremely important to evaluate model performance with your data set. Before reviewing all the pictures and AI classifications, you can classify a subset of your pictures and compare these identifications with AI output. This step will allow you to determine how well the model is working for various species of interest and also to determine if there are particular species or locations where model performance is particularly poor. Below, we demonstrate a step-by-step workflow for how to get WI output into R, join computer and human vision identifications, and estimate model performance metrics for each species. Throughout, we will use the purrr package in R (Henry and Wickham 2020; R Core Team 2021) to repeatedly apply the same function to objects in a list or column in a nested data frame efficiently and without the need for writing loops. Readers unfamiliar with purrr syntax, may want to view one or more of the tutorials, below, or make use of the purrr cheat sheet. http://www.rebeccabarter.com/blog/2019-08-19_purrr/ https://www.r-bloggers.com/2020/05/one-stop-tutorial-on-purrr-package-in-r/ https://jennybc.github.io/purrr-tutorial/index.html Right after uploading pictures to the WI platform and before doing any image processing (i.e., identification), download the WI output with the download tab (see Figure 3.4). WI’s computer vision identifications will be contained in the images.csv file (Figure 3.7). Save that file as images_cv.csv. Figure 3.7: Identifications provided by computer vision Use the WI processing module to verify a subset of your pictures (e.g., ~100,000) and either accept the computer vision identification as correct or edit the identification with the correct species label. The identified_by column presented in Figure 2.5 will change according to the new identifier (Figure 3.8). Figure 3.8: Identifications verified by a human Once you finish identifying a subset of your images, download the data from WI and change the name of the images.csv file to images_hv.csv. Create a data folder to store your two CSV files images_cv.csv and images_hv.csv that refer to classifications of computer and human vision, respectively. We provide an example of both files with the repository associated with this guide, named images_cv_jan2020_raw.csv and images_hv_jan2020_raw.csv. Process the two data files using the R code provided in the sections below. 3.7.1 Reading in data, introduction to the Purrr package Before comparing human and computer vision we need to do some data cleaning. This cleaning includes removing duplicated uploads to the WI platform and making sure to keep a single record for each image, as WI creates multiple rows when more than one animal (or object) is identified in an image. First, we load required libraries. library(tidyverse) # for data wrangling and visualization, includes dplyr and purrr library(here) # to allow use of relative paths library(DT) # for viewing data tables Next, we tell R the path (i.e., directory name) that holds our files. We will use the here package (Müller 2017) to tell R that our files live in the “./data/wi” directory. You may, alternatively, type in the full path to the file folder or a relative path from the root directory if you are using a project in Rstudio. # Create filefolder&#39;s path. This should point to the folder name # where you stored your CSV files downloaded from WI filesfolder &lt;- here(&quot;data&quot;, &quot;wi&quot;) filesfolder ## [1] &quot;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/wi&quot; Next, we use the dir function to list the files contained in the filesfolder directory. # list all your CSV files (i.e., &quot;images_cv_jan2020_raw.csv&quot; and &quot;images_hv_jan2020_raw.csv&quot;) files &lt;- dir(filesfolder, pattern = &quot;*.csv&quot;) files ## [1] &quot;images_cv_jan2020_raw.csv.zip&quot; &quot;images_hv_jan2020_raw.csv.zip&quot; We then use the map function in the purrr package to read in all of the files and store them in a list object named mycsv. The first argument to map is a list (here, files) which is “piped in” using %&gt;% from the magrittr package (Bache and Wickham 2020). Pipes (%&gt;%) provide a way to execute a sequence of data operations, organized so that the operations can be read from left to right (e.g., “Take this set of files and then read them in using read_csv”). The second argument to map is a function, in this case read_csv, to be applied to the list. The map function iterates over the two files stored in the files object, reads in the data files and then stores them in a new list named mycsv. We use ~ to refer to our function and use .x to refer to the list object that is passed to the function as an additional argument. # Read both CSV files mycsv &lt;- files %&gt;% map(~ read_csv(file.path(filesfolder, .x))) # Inspect how the data sets look like mycsv ## [[1]] ## # A tibble: 112,323 × 27 ## ...1 project_id deployment_id image_id filename location is_blank ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2000949 N23-Jan2020-Jul2020 45eab5c4-… 0219056… gs://large… 0 ## 2 2 2000949 N23-Jan2020-Jul2020 03fe2e6d-… 0210041… gs://large… 0 ## 3 3 2000949 N23-Jan2020-Jul2020 a0c07226-… 0216046… gs://large… 0 ## 4 4 2000949 A08-Jan2020-Jul2020 810d0020-… 0115028… gs://large… 0 ## 5 5 2000949 A08-Jan2020-Jul2020 d546edcc-… 0501078… gs://large… 0 ## 6 6 2000949 A08-Jan2020-Jul2020 21ecd9a7-… 0608005… gs://large… 0 ## 7 7 2000949 A08-Jan2020-Jul2020 aa2f7b21-… 0127084… gs://large… 0 ## 8 8 2000949 A08-Jan2020-Jul2020 b4920b3e-… 0607005… gs://large… 0 ## 9 9 2000949 N12-Jan2020-Jul2020 151a52ac-… 0615081… gs://large… 0 ## 10 10 2000949 N12-Jan2020-Jul2020 897bbf5d-… 0529065… gs://large… 0 ## # … with 112,313 more rows, and 20 more variables: identified_by &lt;chr&gt;, ## # wi_taxon_id &lt;chr&gt;, class &lt;chr&gt;, order &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, ## # species &lt;chr&gt;, common_name &lt;chr&gt;, uncertainty &lt;lgl&gt;, timestamp &lt;dttm&gt;, ## # age &lt;lgl&gt;, sex &lt;lgl&gt;, animal_recognizable &lt;lgl&gt;, individual_id &lt;lgl&gt;, ## # number_of_objects &lt;dbl&gt;, individual_animal_notes &lt;lgl&gt;, highlighted &lt;lgl&gt;, ## # markings &lt;lgl&gt;, cv_confidence &lt;dbl&gt;, license &lt;chr&gt; ## ## [[2]] ## # A tibble: 114,237 × 27 ## X1 project_id deployment_id image_id filename location is_blank ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2000949 N25-Jan2020-Jul2020 902b671f-… 0331008… gs://large… 0 ## 2 2 2000949 N23-Jan2020-Jul2020 9a430206-… 0424015… gs://large… 0 ## 3 3 2000949 N29-Jan2020-Jul2020 e727dc42-… 0331028… gs://large… 0 ## 4 4 2000949 A06-Jan2020-Jul2020 db3c3213-… 0602047… gs://large… 0 ## 5 5 2000949 A02-Jan2020-Jul2020 c7e33138-… 0310038… gs://large… 1 ## 6 6 2000949 A04-Jan2020-Jul2020 52f77e0c-… 0418003… gs://large… 0 ## 7 7 2000949 A06-Jan2020-Jul2020 94a5b596-… 0302034… gs://large… 0 ## 8 8 2000949 A06-Jan2020-Jul2020 063c0153-… 0619014… gs://large… 0 ## 9 9 2000949 A06-Jan2020-Jul2020 4b6f9c4e-… 0625031… gs://large… 0 ## 10 10 2000949 A07-Jan2020-Jul2020 15c4e9f4-… 0509024… gs://large… 0 ## # … with 114,227 more rows, and 20 more variables: identified_by &lt;chr&gt;, ## # wi_taxon_id &lt;chr&gt;, class &lt;chr&gt;, order &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, ## # species &lt;chr&gt;, common_name &lt;chr&gt;, uncertainty &lt;lgl&gt;, timestamp &lt;dttm&gt;, ## # age &lt;chr&gt;, sex &lt;chr&gt;, animal_recognizable &lt;lgl&gt;, individual_id &lt;lgl&gt;, ## # number_of_objects &lt;dbl&gt;, individual_animal_notes &lt;chr&gt;, highlighted &lt;lgl&gt;, ## # markings &lt;chr&gt;, cv_confidence &lt;lgl&gt;, license &lt;chr&gt; There are many variables that we will not need when evaluating AI performance. To simplify things, we use the select function to only keep the variables of interest: deployment_id: deployment name including camera location. filename: image filename. timestamp: time of a camera trigger. image_id: WI identifier for images uploaded to the platform. common_name: species’ common name labeled either by computer or human vision. cv_confidence: confidence value associated with the computer vision label. Additionally, we remove images without a classification (i.e., common_name = “NA”) and with the “Human” class, as human images in this data set predominately correspond to camera-set-up images. We inspect the final number of rows using nrow(). mycsv &lt;- mycsv %&gt;% map(~.x %&gt;% select(deployment_id, filename, timestamp, image_id, common_name, cv_confidence) %&gt;% filter(!is.na(common_name)) %&gt;% filter(common_name != &quot;Human&quot; &amp; common_name != &quot;Human-Camera Trapper&quot;)) mycsv %&gt;% map(~.x %&gt;% nrow()) ## [[1]] ## [1] 108515 ## ## [[2]] ## [1] 106787 3.7.2 Removing duplicate images Before discussing how to join the two data sets corresponding to human and computer vision, we need to remove duplicated rows that might result from accidentally uploading the same image more than once to the WI platform. We can identify these duplicated uploads as they contain the same information in all the columns except for image_id. Figure 3.9 shows two different examples of duplicated uploads. Figure 3.9: Identification of duplicated image uploads in the images.csv WI output. We will use the information contained in deployment_id, filename, and timestamp to uniquely identify each image event; hereafter, we will refer to this suite of variables as key columns. To remove duplicates in our data sets, we first use the group_by function to group rows using these key columns and store our grouped dataframe as mycsv_grouped. We use the summarise function (Wickham et al. 2019) to create a new column uploads that contains the count of unique image_id values for each group of rows. Groups that have more than one image_id indicate duplicated uploads (Figure 3.9). We then filter by counts &gt; 1 to inspect duplicated images, remove duplicated rows in our data sets using the unique function (R Core Team 2021) and store the remaining data in the no_duplicates list object. mycsv_grouped &lt;- mycsv %&gt;% map(~.x %&gt;% group_by(deployment_id, filename, timestamp)) # Create data set with duplicated images duplicated_images &lt;- mycsv_grouped %&gt;% map(~.x %&gt;% summarise(uploads = length(unique(image_id))) %&gt;% # count unique ID&#39;s filter(uploads &gt; 1)) # for inspecting duplicated uploads # Create a data set without the duplicated images no_duplicates &lt;- mycsv_grouped %&gt;% map(~.x %&gt;% filter(image_id == unique(image_id)[1])) # This code keeps all the records that have a single image ID and keeps only # one record of duplicated uploads selected by indexing the first &quot;image_id&quot; within a group 3.7.3 Images with multiple observations of the same species WI creates extra rows when you identify more than one animal per picture. Thus, it is likely that your two CSV files will slightly differ in number of rows (as is the case here). After removing duplicated records, we need to drop extra rows in the human vision data frame that result from identifying more than 1 animal of the same species in the same image. As an example, Figure 3.10 displays records for both a juvenile and adult Southern tamandua detected in the same image. Figure 3.10: Highlighted rows represent detections of one juvenile and an adult of the Southern Tamandua in the same image. We regroup the data by the key columns from before, adding common_name; we also add cv_confidence to retain this variable for later use. We then apply the summarise function to reduce multiple records of the same species in the same image to a single record. same_sp_dets &lt;- no_duplicates %&gt;% map(~.x %&gt;% group_by(deployment_id, filename, timestamp, cv_confidence, common_name) %&gt;% summarise() %&gt;% # summarise rows that correspond to different animals of the same species ungroup()) We will use the human vision data set contained in the same_sp_dets list object as the ground truth when evaluating model performance for all platforms reviewed in this gitbook. This list object has been saved as images_hv_jan2020.csv using the following code. write_csv(same_sp_dets %&gt;% pluck(2) %&gt;% select(-cv_confidence), file = &quot;data/wi/processed_data/images_hv_jan2020.csv&quot;) Now that we removed duplicated records and multiple observations of the same species in the same image, we can proceed to join the computer and human vision data sets. We will match rows using a right_join, specifying our key columns as unique image identifiers to pair the cases from the two data sets. This will keep all records in the human vision data set and their corresponding matches in the computer vision data set. Records that are only in the computer vision data set will be dropped from further consideration. We add either a _cv or _hv suffix to the common_name to indicate classification by computer and human, respectively. both_visions &lt;- same_sp_dets %&gt;% reduce(right_join, by = c(&quot;filename&quot;, &quot;deployment_id&quot;, &quot;timestamp&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) 3.7.4 Images with multiple species Similar to multiple detections of the same species, when we identify different species in the same image, these observations will be recorded in separate rows in the human vision data set (Figure 3.11). When using the right_join to match observations from the computer and human vision data sets, some of the computer vision labels will be replicated to match the multiple rows generated for different species detected in the same image by human vision. Figure 3.11: Highlighted rows represent detections of different species in the same image. To address this complication, we create a sp_num column containing the number of species per image, which can then be used to identify records with more than one species in an image sp_num &gt; 1. We then subset the observations that have more than 1 species in an image. both_visions &lt;- both_visions %&gt;% group_by(deployment_id, filename, timestamp) %&gt;% mutate(sp_num = length(unique(common_name_hv))) multiple_sp_dets &lt;- both_visions %&gt;% filter(sp_num &gt; 1) For each of these images, we determine if there is a match between computer and human vision. We find that there are three images where AI was able to correctly identify one of the species present. options(width=180) multiple_sp_dets %&gt;% filter(common_name_cv %in% common_name_hv) %&gt;% ungroup() %&gt;% select(!c(timestamp, deployment_id, cv_confidence_hv, sp_num)) %&gt;% print(width=Inf, n=1000) ## # A tibble: 6 × 4 ## filename cv_confidence_cv common_name_cv common_name_hv ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 02240652.JPG 0.77 Collared Peccary Collared Peccary ## 2 02240652.JPG 0.77 Collared Peccary Margarita Island Capuchin ## 3 02240653.JPG 0.69 Collared Peccary Collared Peccary ## 4 02240653.JPG 0.69 Collared Peccary Margarita Island Capuchin ## 5 02240654.JPG 0.75 Collared Peccary Collared Peccary ## 6 02240654.JPG 0.75 Collared Peccary Margarita Island Capuchin We can also look at some of the records for images that had multiple species in them but AI failed to correctly identify any of them. multiple_sp_dets %&gt;% filter(!common_name_cv %in% common_name_hv) %&gt;% ungroup() %&gt;% select(!c(timestamp, deployment_id, cv_confidence_hv, sp_num)) %&gt;% head(10) ## # A tibble: 10 × 4 ## filename cv_confidence_cv common_name_cv common_name_hv ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 06300682.JPG 0.37 No CV Result Bos Species ## 2 06300682.JPG 0.37 No CV Result Domestic Horse ## 3 06300683.JPG 0.82 No CV Result Bos Species ## 4 06300683.JPG 0.82 No CV Result Domestic Horse ## 5 02240646.JPG 0.42 No CV Result Collared Peccary ## 6 02240646.JPG 0.42 No CV Result Margarita Island Capuchin ## 7 02240650.JPG 0.52 No CV Result Collared Peccary ## 8 02240650.JPG 0.52 No CV Result Margarita Island Capuchin ## 9 02240651.JPG 0.64 No CV Result Collared Peccary ## 10 02240651.JPG 0.64 No CV Result Margarita Island Capuchin We then need to decide how to treat these records. Because WI will only identify 1 species in an image, we want to give it credit when it matches one of multiple species present. We also choose to consider it a single failure when AI fails to match any of the multiple species present. Thus, we chose to create a data set that combines: records for all images that contain only 1 species the matched observation whenever there are multiple species present and one of them is identified using AI the first observation whenever there are multiple species present and AI fails to identify any of them dataA &lt;- both_visions %&gt;% filter(sp_num ==1) %&gt;% ungroup() %&gt;% drop_na(common_name_hv) dataB &lt;- multiple_sp_dets %&gt;% filter(common_name_cv %in% common_name_hv) %&gt;% ungroup() %&gt;% filter(common_name_cv == common_name_hv)%&gt;% drop_na(common_name_hv) dataC &lt;- multiple_sp_dets %&gt;% filter(!common_name_cv %in% common_name_hv) %&gt;% filter(row_number()==1) %&gt;% ungroup()%&gt;% drop_na(common_name_hv) both_visions_clean&lt;- rbind(dataA, dataB, dataC) 3.7.5 Summarizing human and computer vision records by species Next, we count the number of records of each species separately for human and computer vision. We will group the data by common_name and then count the number of observations using n() inside summarise. Then, we use the full_join function to join the species counts in computer and human vision using the common_name to match observations in the two data sets. Lastly, we add a suffix to the common_name variable to distinguish the counts of human and computer vision. sp_counts_cv &lt;- both_visions_clean %&gt;% group_by(common_name_cv) %&gt;% summarise(n = n()) %&gt;% rename(common_name = common_name_cv) sp_counts_hv &lt;- both_visions_clean %&gt;% group_by(common_name_hv) %&gt;% summarise(n = n()) %&gt;% rename(common_name = common_name_hv) sp_counts &lt;- full_join(sp_counts_cv, sp_counts_hv, by = &quot;common_name&quot;, suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% arrange(common_name) %&gt;% mutate(common_name = as.factor(common_name)) You can inspect the resulting data frame with the per-species counts for the two visions (Table 3.1). Table 3.1: Counts of images classified by Wildlife Insights AI (n_cv, using a 0.65 and 0.95 confidence threshold for predicting species and blanks, respectively) and humans (n_hv) for each species in the data set. When looking at the table of species counts, you will likely notice that you have some species that show up in the computer vision (i.e., AI) data set that are not present in the human vision data set and vice versa. Using the sp_counts data frame (Table 3.1) calculated above, we can easily identify the species labels that are not found in both data sets. We can then simplify the table by replacing labels found only in the human vision data set as “Other_hv” and labels only found in the computer vision data set as “Other_cv”. Doing so will allow us to create a simplified confusion matrix (a matrix that shows the combinations of all predicted and true classifications used to inspect model performance). We begin by creating new data sets, NAcv and NAhv, to contain the species labels that are only found in one of the two data sets. NAcv &lt;- sp_counts %&gt;% filter(is.na(n_cv)) NAhv &lt;- sp_counts %&gt;% filter(is.na(n_hv)) We then replace species names not shared by both visions by “Other_cv” and “Other_hv” for computer and human vision, respectively, using the species labels stored in NAcv and NAhv. We use the mutate and if_else functions (Wickham et al. 2019) to overwrite the original species names with these new labels. # Replace categories within the NA&#39;s vector by &quot;Other_cv&quot; or &quot;Other_hv replace_others &lt;- both_visions_clean %&gt;% mutate(common_name_hv = if_else(common_name_hv %in% NAcv$common_name, &quot;Other_hv&quot;, as.character(common_name_hv)), common_name_cv = if_else(common_name_cv %in% NAhv$common_name, &quot;Other_cv&quot;, as.character(common_name_cv))) %&gt;% rename(conf_cv = cv_confidence_cv) # Both inputs in both visions and NA&#39;s vectors are characters. # Both true and false cases for the if_else statements have to be of the same class. 3.7.6 Confusion matrix and performance measures Using the replace_others data frame, we can estimate a confusion matrix using the confusionMatrix function from the caret package (Kuhn 2021) and plot it using ggplot2 (Wickham et al. 2018). The confusionMatrix function requires a data argument for predicted classes and a reference for true classifications, both as factor classes and with the same factor levels. We use the factor and the levels function (R Core Team 2021) to convert common names into factor classes and assign them the same levels, respectively. We specifiy mode = &quot;prec_recall&quot; when calling the confusionMatrix function (Kuhn 2021) to estimate the precision and recall for the WI classifications. library(caret) # to inspect model performance library(ggplot2) # to plot results # Create a vector containing all factor levels for both visions all_levels &lt;- append(&quot;Other_cv&quot;, levels(factor(replace_others$common_name_hv))) %&gt;% sort() # Assign the same factor levels to columns for computer and human vision labels. # Levels are assigned to &quot;character&quot; columns to avoid unwanted label changes # and then transformed with the &quot;factor&quot; function. replace_others$common_name_hv &lt;- factor(as.character(replace_others$common_name_hv), levels = all_levels) replace_others$common_name_cv &lt;- factor(as.character(replace_others$common_name_cv), levels = all_levels) # Estimate confusion matrix cm_wi &lt;- confusionMatrix(data = replace_others$common_name_cv, reference = replace_others$common_name_hv, mode = &quot;prec_recall&quot;) # Plot confusion matrix plot_cm_wi &lt;- cm_wi %&gt;% pluck(&quot;table&quot;) %&gt;% as.data.frame() %&gt;% rename(Frequency = Freq) %&gt;% ggplot(aes(x=Reference, y=Prediction, fill=Frequency)) + # define axes geom_raster() + # specifies a tile plot scale_fill_gradient(low = &quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + # color scales geom_text(aes(label = Frequency), size = 1.5) +# size for matrix counts theme(legend.position = &quot;bottom&quot;, axis.text.x = element_text(angle = 90), # define angle for x axis text) legend.text = element_text(size = 7)) plot_cm_wi Figure 3.12: Confusion matrix applied to classfications from Wildlife Insights using a confidence threshold of 0.65 and 0.95 for species and blanks, respectively. Now we can use the confusion matrix to estimate model performance metrics including accuracy, precision, recall and F-1 score (See Chapter 1 for metrics description). (overall_accuracy &lt;- cm_wi %&gt;% pluck(&quot;overall&quot;, &quot;Accuracy&quot;) %&gt;% round(., 2)) ## [1] 0.16 classes_metrics &lt;- cm_wi %&gt;% pluck(&quot;byClass&quot;) %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% rownames_to_column() %&gt;% rename(species = rowname) %&gt;% mutate(across(is.numeric, ~round(., 2))) classes_metrics$species &lt;- str_remove(string = classes_metrics$species, pattern = &quot;Class: &quot;) Table 3.1: Model performance metrics for each species in the data set, for a confidence threshold of 0.65 and 0.95 for species and blanks, respectively. 3.7.7 Confidence thresholds Finally, we define a function that allows us to inspect how precision and recall change when different confidence thresholds are established for assigning a species label by computer vision (i.e., a prediction). Our function will assign an “Other_cv” label whenever the confidence for a computer vision prediction is below a user-specified confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall for each species change with the confidence threshold and identify a threshold that balances precision and recall for the different species. threshold_for_metrics &lt;- function(conf_threshold = 0.7, tmp = replace_others) { tmp$common_name_cv[tmp$conf_cv &lt; conf_threshold] &lt;- &quot;Other_cv&quot; # assign a &quot;Other_cv whenever the confidence value of a prediction # (conf_cv) is lower than the threshold provided as an argument in the # function cm &lt;- confusionMatrix(data = tmp$common_name_cv, reference = tmp$common_name_hv, mode = &quot;prec_recall&quot;) # use the confusionMatrix function from the caret package using the # common_name_cv containing the new labels according to a particular # confidence threshold classes_metrics &lt;- cm %&gt;% # get confusion matrix pluck(&quot;byClass&quot;) %&gt;% # get metrics by class as.data.frame() %&gt;% # assign a data frame object select(Precision, Recall, F1) %&gt;% # select metrics of interest rownames_to_column() %&gt;% # format data frame rename(species = rowname) %&gt;% # rename species column mutate(conf_threshold = conf_threshold) classes_metrics$species &lt;- str_remove(string = classes_metrics$species, pattern = &quot;Class: &quot;) return(classes_metrics) # return a data frame with metrics for every species } Before demonstrating this approach, we first identify the species that have more than 50 records. # Labels for prevalent species sp_plots &lt;- sp_counts %&gt;% filter(n_cv &gt; 50 &amp; n_hv &gt; 50 &amp; common_name != &quot;Blank&quot;) kable(sp_plots, caption= &quot;Species with at least 50 records in the human and computer vision data sets.&quot;) Table 3.2: Species with at least 50 records in the human and computer vision data sets. common_name n_cv n_hv Black Agouti 575 14210 Collared Peccary 11794 24784 Giant Anteater 103 807 Lowland Tapir 51 1562 Spotted Paca 2017 5650 White-lipped Peccary 77 3813 Let’s look at the distribution of confidence values associated with these species using the geom_bar function (Wickham et al. 2018). # Plot confidence values replace_others %&gt;% filter(common_name_cv %in% sp_plots$common_name &amp; common_name_hv %in% sp_plots$common_name) %&gt;% ggplot(aes(conf_cv, group = common_name_cv, colour = common_name_cv)) + geom_bar() + facet_wrap(~common_name_cv, scales = &quot;free&quot;) + labs(y = &quot;Empirical distribution&quot;, x = &quot;Confidence values&quot;)+ theme(legend.position=&quot;bottom&quot;) + scale_color_viridis_d() Figure 3.13: Distribution of confidence values associated with species that have more than 50 records. We can see that the distribution of confidence values is left skewed for the collared peccary, giant anteater, and spotted paca, with most records having high confidence values suggesting that the AI prediction is presumed to be correct most of the time. By contrast, the black agouti, lowland tapir and the white-lipped peccary have more uniform distributions with a higher number of records that have confidence values below 0.80. Let’s estimate model performance metrics for confidence values ranging from 0.65 to 0.99 using the map_df function (Henry and Wickham 2020) . The map_df function (Henry and Wickham 2020) returns a dataframe object. Once we get a dataframe of model performance metrics for a range of confidence values, we can plot the results using the ggplot2 package (Wickham et al. 2018). conf_vector = seq(0.65, 0.99, length=100) metrics_all_confs &lt;- map_df(conf_vector, threshold_for_metrics, tmp = replace_others) # Plot Precision and Recall metrics_all_confs &lt;- metrics_all_confs %&gt;% mutate_if(is.numeric, round, digits = 2) prec_rec_wi &lt;- metrics_all_confs %&gt;% filter(species %in% sp_plots$common_name) %&gt;% rename(Species = species, Confidence_threshold = conf_threshold) %&gt;% ggplot(aes(x = Recall, y = Precision, group = Species, colour = Species)) + geom_point(aes(size = Confidence_threshold)) + scale_size(range = c(0.6,3)) + labs(x = &quot;Recall&quot;, y = &quot;Precision&quot;, ) + scale_color_viridis_d() + geom_line() prec_rec_wi Figure 3.14: Precision and recall for different confidence thresholds for species with at least 50 records. Point sizes represent the confidence thresholds used to accept AI predictions. We see that as we increase the confidence threshold, precision usually increases and recall decreases (Figure 3.14). Ideally, we would like AI to have high precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the species is truly present when AI identifies the species as being present in an image (Chapter 1). If AI suffers from low precision, then we may have to manually review photos that AI tags as having species present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a species in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing images of species that are truly present. To remedy this problem, we would need to review images where AI says the species is absent in order to reduce false negatives. For this particular data set, AI would be most useful for classifying collared peccaries and spotted pacas. For example, we can be confident that WI is correctly labeling collared peccaries, with a precision of 90% at a 43% recall, using a 0.65 confidence threshold (Figure 3.14). The collared peccary is the most abundant species in the data set (representing 22% of the animal records) and AI could be used to catch 43% of the records of this species. We can also be very confident that WI is correctly labeling spotted pacas (precision of 100%, and it will help us to spot 36% of the actual records for the species, also using a 0.65 confidence threshold). Yet, the low recall for all species suggests that we will still have to view all images to identify other records of animals that are not detected by WI. 3.8 Conclusions We have seen how to set up a project, upload and process camera trap photos using WI’s platform. Additionally we provided R code for evaluating per-species model performance for different confidence thresholds. Although we found that WI was able to classify some species with high levels of precision, recall values were typically low; thus, experts will still need to review images to find the animals missed by computer vision. References "],["megadetector---microsoft-ai.html", "Chapter 4 MegaDetector - Microsoft AI 4.1 Upload/format data 4.2 Upload/enter metadata 4.3 Process images - AI module 4.4 Image processing with Timelapse 2 4.5 Using AI output 4.6 Assessing AI performance 4.7 Conclusions", " Chapter 4 MegaDetector - Microsoft AI The MegaDetector (MD) model detects objects (animals, people, and vehicles) in camera trap images, and can be used to separate images into categories of “Empty”, “Animal”, “Human” and “Vehicles” (Beery, Morris, and Yang 2019). You can run MD on your own or request assistance from the Microsoft AI for Earth team. To choose one of these options, you can contact a member of the Microsoft AI team at cameratraps@lila.science to see which approach is right for you; this decision will depend on the number of images that you have, whether data can be shared with third parties, and how comfortable you are running Python code (Beery, Morris, and Yang 2019). We will describe a workflow for a user that will submit images to the Microsoft AI for Earth team. A member of the Microsoft AI for Earth team will then run the models and return an output file that can be loaded into the image review software Timelapse 2 (Greenberg, Godin, and Whittington 2019). Sections 4.1-4.3 go through the steps required to run MD and interpret its output. Sections 4.4 and 4.5 focus on how MD output can be processed in Timelapse 2 and how AI output can be used to accelerate image review and identification by humans. Specifically, AI identifications with high confidence values (i.e., likely to be correct) can be filtered, subset, and subsequently identified by humans using batch operations (see Section 4.5). Section 4.6 focuses on assessing the performance of MD by comparing human labels with the ones provided by MD for a set of images. This last section is particularly useful as it provides tools to evaluate how MD performs with specific data sets and guidance to understand differences in MD output when you use different confidence thresholds for assigning predictions. 4.1 Upload/format data To use MD you can choose between running the model on your own or contacting a member of the Microsoft AI for Earth team at cameratraps@mlila.science to help you run the model. For the first option you won’t need to write any code, but you will need to be comfortable running commands at the command line. If you choose the second option, the Microsoft AI for Earth team will create a Microsoft Azure Blob Storage container for you, to which you will upload your images. You will typically upload your images using AzCopy, a command-line utility for batch data transfers, or Azure Storage Explorer, a GUI-based utility for working with Azure Storage. The team will provide more specific instructions when you contact them. 4.2 Upload/enter metadata When using MD, there is no need to enter metadata before processing your data using AI. You will enter metadata during the post-AI image processing stage, when MD output is integrated with other image processing tools such as Timelapse 2 or Camelot. 4.3 Process images - AI module The Microsoft AI for Earth team will run models for you and return a JSON file or a CSV file if requested. This file will contain image filenames (e.g., “A01/01020108.JPG”, where A01 represents the subfolder of a camera trap location and 01020108.JPG is the picture filename), maximum confidence values associated with all detections within an image, and the maximum confidence value for each category (animal, person, or vehicle) (Figure 4.1). Figure 4.1: Identifications provided by Megadetector. 4.4 Image processing with Timelapse 2 This section synthesize general features of Timelapse 2 that facilitate species identification and annotation of other features of interest from the images (e.g., animals’ sex, behavior, condition, etc.). Section 4.5 demonstrates how you can incorporate AI output (i.e., the JSON file) in Timelapse 2, to accelerate image review and identification. To run Timelapse 2, you will need a computer running Microsoft Windows, and your images must be organized in subfolders (e.g., by study area and camera trap location; Figure 4.2). Timelapse 2 software, video tutorials and a detailed manual with all the software features explained can be downloaded here: http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.Download2 Figure 4.2: Organization of images folders and subfolders appropriate for Timelapse 2 (Greenberg 2020). Before using Timelapse 2, you will need to create a data schema (i.e., a template with a .tdb file extension) that identifies the data fields that will be recorded when processing your camera trap data (e.g., species name, sex, behavior, etc.). This step can be accomplished by using the Timelapse Template Editor that is downloaded along with Timelapse 2. The Template Editor allows you to create the .tdb file that will be read in Timelapse 2 and will adjust the software image processing interface to capture the different data fields you specify in the .tdb file (Greenberg 2020). Once photos are imported to Timelapse 2, you can review them using different processing tools provided by the software. The Timelapse 2 manual (Greenberg 2020) and documentation about the software design (Greenberg, Godin, and Whittington 2019) describe in detail the processing tools available in the software, which include features to: Magnify images and explore image difference extraction tools (e.g., to identify small animals that are otherwise difficult to spot). Select multiple images quickly from small thumbnails and classify them all at once. Automate data entry via metadata extraction (time/date, filename, temperature sensed by the camera) and copy annotated information to other pictures. Sort images by date, species or any other annotated feature, to easily review and verify this information. 4.5 Using AI output To integrate MD output with Timelapse 2, you will need to have the same folder and subfolder structure (Figure 4.2) that you used to upload pictures to the Azure Blob Storage. Using the same folder structure allows Timelapse 2 to match each image with MD output using relative paths associated with each filename (see notes in Figure 4.3). Figure 4.3: Notes from the Timelapse 2 manual on how to match MD output with photos when using the Timelapse 2 software (Greenberg 2020) In Timelapse 2, you must activate the option for working with image recognition data (Figure 4.4 and import computer vision results stored in the JSON file provided by Microsoft AI team (Greenberg 2020). You will see bounding boxes that can facilitate image review once the automatic image recognition is activated. To use AI results to accelerate image review, you can filter MD output categories detected with high confidence levels that are likely correct. To do that, you must provide a confidence range to accept predictions made by computer vision (Figure 4.4 and Figure 4.5). For example, if you choose “Empty” as the detected entity and a high confidence range (e.g, from 0.65 to 1.00), your data set will be filtered to display images identified as “Empty” by AI and whose predictions are likely to be correct. Figure 4.4: Section of “Custom selection” from the Timelapse 2 software where the option to work with AI detections can be activated (Greenberg 2020). Figure 4.5: When activating the use of AI detections, users can filter which images are displayed depending on the range of confidence values provided (Greenberg 2020). After subsetting images to be displayed depending on their confidence values, you can easily inspect images in the overview mode in Timelapse 2 and select multiple images at a time and assign an “Empty” category to them if the AI output is correct (Figure 4.6). If AI output is not correct, you can edit those classifications. Figure 4.6: Timelapse 2 overview mode where users can perform bulk actions (e.g., selection of multiple images) to accept or reject AI predictions (Greenberg 2020). After, processing pictures with Timelapse 2, you can export the data as a CSV File. This file (TimelapseData.csv) will contain all the data entries that you specified in the template (Figure 4.7). Figure 4.7: Output contained in a CSV file exported from Timelapse 2 software (Greenberg 2020). 4.6 Assessing AI performance Before exploring model performance with your data, let us recapitulate. MD runs AI models and outputs broad categories of predictions for images (in a JSON file). These AI predictions can be integrated with Timelapse 2 to further process camera trap images (i.e., identifying pictures with the help of AI output). However, if you have previously classified images (e.g., identified using your software or platform of preference), you can explore MD performance before integrating AI results using Timelapse 2. To evaluate MD performance, you will need to 1) classify a subset of your pictures and export the classification results to a CSV file (containing at minimum, the image filename, camera location and species name), and 2) request the Microsoft AI team to run MD (if you decide not to run the models by yourself) and send the results as a CSV file. You can then follow the steps below to evaluate the performance of MD’s AI model. We also discuss how one can select an appropriate confidence threshold for filtering images to accelerate the image review process (e.g., by focusing only on images that likely contain an animal). 4.6.1 Reading in data, introduction to the Purrr package Below, we demonstrate a step-by-step workflow for how to get MD output into R, join computer and human vision identifications, and estimate model performance metrics for each class. Throughout, we will use the purrr package in R (Henry and Wickham 2020; R Core Team 2021) to repeatedly apply the same function to objects in a list or column in a nested data frame efficiently and without the need for writing loops. Readers unfamiliar with purrr syntax, may want to view one or more of the tutorials, below, or make use of the purrr cheat sheet. http://www.rebeccabarter.com/blog/2019-08-19_purrr/ https://www.r-bloggers.com/2020/05/one-stop-tutorial-on-purrr-package-in-r/ https://jennybc.github.io/purrr-tutorial/index.html Once you finish the identification of a subset of your images using your platform of preference, export your classifications as a CSV file and name it as images_hv.csv. The other CSV file containing the MD results can be named as images_cv.csv. Create a data folder to store your two CSV files images_cv.csv and images_hv.csv that refer to classifications of computer and human vision, respectively (note, we provide an example of both files with the repository associated with this guide named images_cv_jan2020.csv and images_hv_jan2020.csv). Process the two data files using the R code provided below. First, we load required libraries and open files. library(tidyverse) # for data wrangling and visualization, includes dplyr and purrr library(here) # to allow use of relative paths Next, we tell R the path (i.e., directory name) that holds our files. We will use the here package (Müller 2017) to tell R that our files live in the “./data/md” directory. You may, alternatively, type in the full path to the file folder or a relative path from the root directory if you are using a project in Rstudio. # Create filefolder&#39;s path. This should point to the folder name # where you stored your CSV files, one with classifications of some of your pictures # and the other with MD output filesfolder &lt;- here(&quot;data&quot;, &quot;md&quot;) filesfolder ## [1] &quot;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/md&quot; # List your files contained in the filesfolder directory. This code will # list all your CSV files (i.e., images_cv.csv and images_hv.csv) files &lt;- dir(filesfolder, pattern = &quot;*.csv&quot;) files ## [1] &quot;images_cv_jan2020.csv&quot; &quot;images_hv_jan2020.csv&quot; We then use the map function in the purrr library to read in all of the files and store them in a list object named mycsv. The first argument to map is a list (here, files) which is “piped in” using %&gt;% from the magrittr package (Bache and Wickham 2020). Pipes (%&gt;%) provide a way to execute a sequence of data operations, organized so that the operations can be read from left to right (e.g., “Take the set of files and then read them in using read_csv”). The second argument to map is a function, in this case read_csv, to be applied to the list. The map function iterates over the two files stored in the files object, reads in the data files and then stores them in a new list named mycsv. We use ~ to refer to our function and use .x to refer to the list object that is passed to the function as an additional argument. # Read both CSV files mycsv &lt;- files %&gt;% map(~read_csv(file.path(filesfolder, .x))) # Inspect how the data sets look like mycsv ## [[1]] ## # A tibble: 112,247 × 7 ## image_path max_confidence detections max_conf_animal max_conf_person ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 jan2020/A01/01080001.JPG 0.998 NA NA 0.998 ## 2 jan2020/A01/01080002.JPG 0.97 NA NA 0.97 ## 3 jan2020/A01/01080003.JPG 0.996 NA NA 0.996 ## 4 jan2020/A01/01080004.JPG 0.985 NA 0.202 0.985 ## 5 jan2020/A01/01080005.JPG 0.939 NA 0.209 0.939 ## 6 jan2020/A01/01080006.JPG 0.996 NA NA 0.996 ## 7 jan2020/A01/01080007.JPG 0.999 NA NA 0.999 ## 8 jan2020/A01/01080008.JPG 0.997 NA NA 0.997 ## 9 jan2020/A01/01080009.JPG 0.914 NA 0.428 0.914 ## 10 jan2020/A01/01080010.JPG 0.992 NA NA 0.992 ## # … with 112,237 more rows, and 2 more variables: max_conf_group &lt;dbl&gt;, ## # max_conf_vehicle &lt;lgl&gt; ## ## [[2]] ## # A tibble: 104,826 × 4 ## deployment_id filename timestamp common_name ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 A01-Jan2020-Jul2020 01090079.JPG 2020-01-09 10:54:39 Blank ## 2 A01-Jan2020-Jul2020 01090080.JPG 2020-01-09 10:54:40 Blank ## 3 A01-Jan2020-Jul2020 01090081.JPG 2020-01-09 10:54:41 Blank ## 4 A01-Jan2020-Jul2020 01090082.JPG 2020-01-09 10:54:49 Blank ## 5 A01-Jan2020-Jul2020 01090083.JPG 2020-01-09 10:54:50 Blank ## 6 A01-Jan2020-Jul2020 01090084.JPG 2020-01-09 10:54:51 Blank ## 7 A01-Jan2020-Jul2020 01100085.JPG 2020-01-10 16:14:15 Black Agouti ## 8 A01-Jan2020-Jul2020 01100086.JPG 2020-01-10 16:14:16 Blank ## 9 A01-Jan2020-Jul2020 01100087.JPG 2020-01-10 16:14:17 Black Agouti ## 10 A01-Jan2020-Jul2020 01130088.JPG 2020-01-13 08:53:39 Blank ## # … with 104,816 more rows 4.6.2 Format computer vision data set Columns of interest in the the MD data set include: filename: contains camera location and filename (e.g., A01/01010461.JPG). max_confidence: contains the maximum confidence value found for a detection in a picture. max_conf_animal, max_conf_person, max_conf_group, max_conf_vehicle: each of these columns contain a confidence value associated with a prediction by computer vision for the different classification classes. We begin by creating a max_conf_blank variable, which we will use to identify blank images. We assign a value of 1 to this variable whenever the observation has missing values for all of the other max_conf variables. cv_wide &lt;- mycsv %&gt;% pluck(1) %&gt;% #extract the computer vision images_cv.csv file rename(filename = image_path) %&gt;% group_by(filename) %&gt;% # group by filename mutate(max_conf_blank = case_when(sum(is.na(max_conf_animal), is.na(max_conf_person), is.na(max_conf_group), is.na(max_conf_vehicle)) == 4 ~ 1)) %&gt;% ungroup() MD can detect and classify more than one object in an picture, and each object will be assigned its own confidence value. We want to keep each of these classifications in a separate row. To do that, we first change the data set from “wide” to “long” format using the pivot_longer function (Wickham and Henry 2018). This function will create two new variables, name and value, that will hold the classification (“Human”, “Animal”, “Vehicle”, “Blank”) and associated confidence values, respectively. cv_long &lt;- cv_wide %&gt;% select(-detections) %&gt;% # remove the &quot;detections&quot; column pivot_longer(c(&quot;max_conf_animal&quot;, &quot;max_conf_person&quot;, &quot;max_conf_group&quot;, &quot;max_conf_vehicle&quot;, &quot;max_conf_blank&quot;)) cv_long ## # A tibble: 561,235 × 4 ## filename max_confidence name value ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 jan2020/A01/01080001.JPG 0.998 max_conf_animal NA ## 2 jan2020/A01/01080001.JPG 0.998 max_conf_person 0.998 ## 3 jan2020/A01/01080001.JPG 0.998 max_conf_group NA ## 4 jan2020/A01/01080001.JPG 0.998 max_conf_vehicle NA ## 5 jan2020/A01/01080001.JPG 0.998 max_conf_blank NA ## 6 jan2020/A01/01080002.JPG 0.97 max_conf_animal NA ## 7 jan2020/A01/01080002.JPG 0.97 max_conf_person 0.97 ## 8 jan2020/A01/01080002.JPG 0.97 max_conf_group NA ## 9 jan2020/A01/01080002.JPG 0.97 max_conf_vehicle NA ## 10 jan2020/A01/01080002.JPG 0.97 max_conf_blank NA ## # … with 561,225 more rows Next, we drop all rows where the confidence value is equal to NA and rename our classification variable to class. Additionally, we remove images with the “Human” class; human images in this data set predominately correspond to when the cameras were initially being set up. To simplify things, we also change the “Group” label to “Animal”. We do this for 3 reasons: 1) we found that there were very few “Group” classifications in our data set; 2) these predictions were not very accurate; and 3) MD will most often be used to separate blank images from those that have at least one animal present, and thus, the “Group” label is not all that informative. cv &lt;- cv_long %&gt;% filter(is.na(value) != TRUE) %&gt;% mutate(name = replace(name, name == &quot;max_conf_blank&quot; , &quot;Blank&quot;), name = replace(name, name == &quot;max_conf_animal&quot;, &quot;Animal&quot;), name = replace(name, name == &quot;max_conf_person&quot;, &quot;Human&quot;), name = replace(name, name == &quot;max_conf_group&quot;, &quot;Animal&quot;), name = replace(name, name == &quot;max_conf_vehicle&quot;, &quot;Vehicle&quot;)) %&gt;% # rename predictions rename(class = name) %&gt;% # rename column with predicted classes filter(!class == &quot;Human&quot;) Changing the “Group” label to “Animal” results in some images having multiple predictions of “Animal” for the same image. In these cases, we use the slice function to select the first record (Wickham et al. 2019). cv &lt;- cv %&gt;% group_by(filename, class) %&gt;% slice(1) Comparing output from human and computer vision also requires that the classification variables have the same levels attribute. We first create a vector, all_levels, containing the names of the classes. Then, we use the factor function (R Core Team 2021) to convert common names into factor classes and assign the levels to the class column. # Create a vector with levels of predicted categories all_levels &lt;- c(&quot;Animal&quot;, &quot;Blank&quot;) # Assign the levels attribute to the class column cv$class &lt;- factor(as.character(cv$class, levels = all_levels)) Lastly, we add a variable that will indicate if there are multiple detections within the same image. cv &lt;- cv %&gt;% group_by(filename) %&gt;% mutate(multiple_det = n() &gt; 1) %&gt;% select(filename, class, value, multiple_det) %&gt;% # select columns of interest ungroup() 4.6.3 Format human vision data set The human vision data set (ìmages_hv_jan2020.csv) was previously cleaned to remove duplicated records and to summarise multiple rows that reference animals of the same species identified in the same image (see Chapter 3 for details about these steps). We begin by inspecting all the species names contained in the human vision data set using the unique function (R Core Team 2021). We will eventually need to create a variable that can be compared to MD output (i.e., broad classes identifying “Blank” and “Animal”). hv_sp &lt;- mycsv %&gt;% pluck(2) # select human vision data frame # check all species present in the data set unique(hv_sp$common_name) %&gt;% sort() ## [1] &quot;Alouatta Species&quot; &quot;Amazonian Motmot&quot; ## [3] &quot;Ants&quot; &quot;Bird&quot; ## [5] &quot;Black Agouti&quot; &quot;Blank&quot; ## [7] &quot;Bos Species&quot; &quot;Bush Dog&quot; ## [9] &quot;Caprimulgidae Family&quot; &quot;Capybara&quot; ## [11] &quot;Cervidae Family&quot; &quot;Collared Peccary&quot; ## [13] &quot;Common Green Iguana&quot; &quot;Crab-eating Fox&quot; ## [15] &quot;Crestless Curassow&quot; &quot;Dasypus Species&quot; ## [17] &quot;Domestic Dog&quot; &quot;Domestic Horse&quot; ## [19] &quot;Fasciated Tiger-heron&quot; &quot;Giant Anteater&quot; ## [21] &quot;Giant Armadillo&quot; &quot;Giant Otter&quot; ## [23] &quot;Insect&quot; &quot;Jaguar&quot; ## [25] &quot;Jaguarundi&quot; &quot;Lizards and Snakes&quot; ## [27] &quot;Lowland Tapir&quot; &quot;Mammal&quot; ## [29] &quot;Margarita Island Capuchin&quot; &quot;Margay&quot; ## [31] &quot;Neotropical Otter&quot; &quot;Northern Amazon Red Squirrel&quot; ## [33] &quot;Ocelot&quot; &quot;Ornate Tití Monkey&quot; ## [35] &quot;Pecari Species&quot; &quot;Possum Family&quot; ## [37] &quot;Puma&quot; &quot;Red Brocket&quot; ## [39] &quot;Rodent&quot; &quot;Saimiri Species&quot; ## [41] &quot;South American Coati&quot; &quot;Southern Tamandua&quot; ## [43] &quot;Spix&#39;s Guan&quot; &quot;Spotted Paca&quot; ## [45] &quot;Tayra&quot; &quot;Turkey Vulture&quot; ## [47] &quot;Turtle Order&quot; &quot;Unknown species&quot; ## [49] &quot;Weasel Family&quot; &quot;White-lipped Peccary&quot; ## [51] &quot;White-tailed Deer&quot; MD output represents filenames as “camera_location/image_filename”. If camera location and filename are contained in different columns of your human vision data set, we can combine them using the unite function (Wickham et al. 2019). Before doing so, we use the str_replace function to reformat the deployment_id variable so that it matches the format of the MD data set. These steps will allow us to later join our human and computer vision data sets using the common variable, filename. hv_sp$deployment_id &lt;- hv_sp$deployment_id %&gt;% str_replace(pattern = &quot;-.*&quot;, &quot;/&quot;) # assign the same patterns found in MD filename column. hv_sp &lt;- hv_sp %&gt;% mutate(deployment_id = paste0(&quot;jan2020/&quot;, deployment_id)) %&gt;% unite(filename, c(&quot;deployment_id&quot;, &quot;filename&quot;), sep = &quot;&quot;) We use the case_when function (Wickham et al. 2019) to implement multiple conditional statements to create a variable, class_hv, containing the classes found in MD output. We select the columns of interest, including a newly created variable (multiple_det) that will keep track of images that have two or more species present in the same image, each recorded in a different row. For the multiple detections, we keep a single row using the slice function (Wickham et al. 2019). We also assign a levels attribute to the class variable in human vision so that it is comparable to the class variable in the computer vision data set. hv &lt;- hv_sp %&gt;% group_by(filename, timestamp) %&gt;% mutate(multiple_det = n() &gt; 1) %&gt;% # create multiple detections column (TRUE/FALSE). mutate(class = case_when(common_name == &quot;Blank&quot; ~ common_name, TRUE ~ &quot;Animal&quot;)) %&gt;% select(filename, timestamp, class, multiple_det) # select columns of interest hv &lt;- hv %&gt;% group_by(filename, timestamp, class) %&gt;% slice(1) %&gt;% ungroup() # Assign the levels attribute to the class column hv$class &lt;- factor(as.character(hv$class), levels = all_levels) 4.6.4 Merging computer and human vision data sets Now that we have the same format for both human and computer vision data sets, we can use various “joins” (Wickham et al. 2019) to merge the two data sets together so that we can evaluate the accuracy of MD. First, however, we will eliminate any pictures that were not processed by both humans and AI. # Determine which images have been viewed by both methods ind1 &lt;- cv$filename %in% hv$filename # in both ind2 &lt;- hv$filename %in% cv$filename # in both cv &lt;- cv[ind1,] # eliminate images not processed by human vision hv &lt;- hv[ind2,] # eliminate images not processed by computer vision # Number of photos eliminated sum(ind1 != TRUE) # in computer vision but not in hv ## [1] 5397 sum(ind2 != TRUE) # in human vision but not in cv ## [1] 2940 Now, we can use: an inner_join with filename and class to determine images that have correct predictions (i.e., images with the same class assigned by computer and human vision) an anti_join with filename and class to determine which records in the human vision data set have incorrect predictions from computer vision. an anti_join with filename and class to determine which records in the computer vision data set have incorrect predictions. We assume the classifications from human vision to be correct and distinguish them from MD predictions. The MD predictions will be correct if they match a class assigned by human vision for a particular record and incorrect if the classes assigned by the two visions differ. # correct predictions matched &lt;- cv %&gt;% inner_join(y = hv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% mutate(class_hv = class) %&gt;% rename(class_cv = class) %&gt;% select(filename, value, class_cv, class_hv, multiple_det_cv, multiple_det_hv) # records in the human vision data set whose predictions are incorrect hv_only &lt;- hv %&gt;% anti_join(y = cv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) %&gt;% rename(class_hv = class) %&gt;% rename(multiple_det_hv = multiple_det) # records in the computer vision data set whose predictions are incorrect cv_only&lt;- cv %&gt;% anti_join(y = hv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% rename(class_cv = class) We then use left_join to merge the predictions from the cv_only (computer vision) data set onto the records from the hv_only (human vision) data set. hv_mismatch &lt;- hv_only %&gt;% left_join(cv_only, by = &quot;filename&quot;) %&gt;% rename(multiple_det_cv = multiple_det) %&gt;% select(filename, value, class_cv, class_hv, multiple_det_cv, multiple_det_hv) We then check for any computer vision records that are not yet accounted for in our data sets containing records with correct or incorrect predictions, i.e., matched and hv_mismatch, respectively. cv_others &lt;- cv_only[cv_only$filename %in% hv_mismatch$filename != TRUE,] table(cv_others$multiple_det) ## &lt; table of extent 0 &gt; Then, we select only the variables we need, combine the matched and mismatched data sets, and again make sure that the classifications have the same factor levels attribute. matched &lt;- matched %&gt;% select(filename, value, class_cv, class_hv) hv_mismatch &lt;- hv_mismatch %&gt;% select(filename, value, class_cv, class_hv) both_visions &lt;- rbind(matched, hv_mismatch) both_visions$class_cv &lt;- factor(as.character(both_visions$class_cv), levels = all_levels) both_visions$class_hv &lt;- factor(as.character(both_visions$class_hv), levels = all_levels) 4.6.5 Confusion matrix and performance measures Finally, we can proceed with estimating a confusion matrix and various AI performance measures using the confusionMatrix function from the caret package (Kuhn 2021) and specifying a 0.65 confidence threshold to accept MD predictions. We can then plot the confusion matrix using ggplot2 (Wickham et al. 2018). The confusionMatrix function requires a data argument for predicted classes and a reference for true classifications, both as factor classes and with the same factor levels. We specify mode = &quot;prec_recall&quot; when calling the confusionMatrix function (Kuhn 2021) to generate estimates of precision and recall. library(caret) # to inspect model performance library(ggplot2) # Estimate confusion matrix both_visions_0.65 &lt;- both_visions both_visions_0.65$class_cv[both_visions_0.65$value &lt; 0.65] &lt;- &quot;Blank&quot; cm_md_0.65 &lt;- confusionMatrix(data = both_visions_0.65$class_cv, reference = both_visions_0.65$class_hv, mode = &quot;prec_recall&quot;) cm_md_0.65_blank &lt;- confusionMatrix(data = both_visions_0.65$class_cv, reference = both_visions_0.65$class_hv, mode = &quot;prec_recall&quot;, positive = &quot;Blank&quot;) # Plot confusion matrix plot_cm_md_0.65 &lt;- cm_md_0.65 %&gt;% pluck(&quot;table&quot;) %&gt;% as.data.frame() %&gt;% rename(Frequency = Freq) %&gt;% ggplot(aes(y=Prediction, x=Reference, fill=Frequency)) + geom_raster() + scale_fill_gradient(low = &quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + theme(axis.text.x = element_text(angle = 90)) + geom_text(aes(label = Frequency), size = 3) #set size to 3 plot_cm_md_0.65 Figure 4.8: Confusion matrix applied to classifications from MegaDetector using a confidence threshold of 0.65. Now we can use the confusion matrix to estimate metrics of model performance including accuracy, precision, recall and F-1 score (See Chapter 1 for metrics description). (overall_accuracy &lt;- cm_md_0.65 %&gt;% pluck(&quot;overall&quot;, &quot;Accuracy&quot;) %&gt;% round(., 2)) ## [1] 0.93 classes_metrics_md_0.65 &lt;- list(cm_md_0.65, cm_md_0.65_blank) classes_metrics_md_0.65 &lt;- classes_metrics_md_0.65 %&gt;% map(~pluck(.x, &quot;byClass&quot;) %&gt;% t() %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% mutate(across(where(is.numeric), ~round(., 2)))) classes_metrics_md_0.65 &lt;- rbind(pluck(classes_metrics_md_0.65, 1), pluck(classes_metrics_md_0.65,2)) %&gt;% mutate(Class = c(&quot;Animal&quot;, &quot;Blank&quot;), .before = Precision) Table 4.1: Model performance metrics for the classes detected by MegaDetector using a 0.65 confidence threshold. We see that the model is really good at picking up animals, with a precision of 98% at a 93% recall. Thus, we expect that 93% of the animals present in our images will be picked up by MD. Further, only 2% of the images flagged as having an animal will not. We can also inspect model performance for a range of confidence thresholds as we demonstrate in the next section. 4.6.6 Confidence thresholds Lets begin by looking at the confidence values associated with each MD classification using the geom_bar function (Wickham et al. 2018). Before plotting, we filter the both_visions data frame to remove blanks as they do not have associated confidence values. both_visions_nb &lt;- both_visions %&gt;% filter(class_cv != &quot;Blank&quot;) # Plot confidence values both_visions_nb %&gt;% ggplot(aes(value, group = class_cv, colour = class_cv)) + geom_bar() + facet_wrap(~class_cv, scales = &quot;free&quot;) + labs(y = &quot;Empirical distribution&quot;, x = &quot;Confidence values&quot;) + scale_color_viridis_d() Figure 4.9: Distribution of confidence values for animals predicted by MegaDetector. We see that the distribution of confidence values for “Animal” classifications is left skewed with most records having high confidence values suggesting that the AI prediction is presumed to be correct most of the time. To inspect how precision and recall change when different confidence thresholds are established for assigning a class predicted by computer vision, we define a function that will calculate these metrics for a user-defined confidence threshold. This function will assign a “Blank” label whenever the confidence for a computer vision prediction is below a particular confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall change with the confidence threshold and choose a threshold that balances these two performance metrics. threshold_for_metrics &lt;- function(conf_threshold = 0.7) { tmp &lt;- both_visions tmp$class_cv[tmp$value &lt; conf_threshold] &lt;- &quot;Blank&quot; # assign a &quot;No Detection&quot; (i.e., Blank) whenever the confidence value of # a prediction (max_confidence) is lower than the threshold provided as # an argument in the function cm &lt;- confusionMatrix(data = tmp$class_cv, reference = tmp$class_hv, mode = &quot;prec_recall&quot;) # use the confusionMatrix function from the caret package using the # class_cv containing the new labels according to a particular # confidence threshold classes_metrics &lt;- cm %&gt;% # get confusion matrix pluck(&quot;byClass&quot;) %&gt;% # get metrics by class t() %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% mutate(conf_threshold = conf_threshold) return(classes_metrics) # return a data frame with metrics for every class } Let’s estimate model performance metrics for confidence values ranging from 0.1 to 0.99 using the map_df function (Henry and Wickham 2020) . The map_df function (Henry and Wickham 2020) returns a dataframe object. Once we get a dataframe of model performance metrics for a range of confidence values, we can plot the results using the ggplot2 package (Wickham et al. 2018). conf_vector = seq(0.1, 0.99, length=100) metrics_all_confs &lt;- map_df(conf_vector, threshold_for_metrics) prec_rec_md &lt;- metrics_all_confs %&gt;% rename(Confidence_threshold = conf_threshold) %&gt;% ggplot(aes(x = Recall, y = Precision)) + geom_point(aes(size = Confidence_threshold, color = &quot;#8ecae6&quot;)) + scale_size(range = c(0.1,3)) + labs(x = &quot;Recall&quot;, y = &quot;Precision&quot;)+ geom_line(color = &quot;#8ecae6&quot;) + scale_color_manual(values = &quot;#8ecae6&quot;, name = &quot;Class&quot;, labels = &quot;Animal&quot;) prec_rec_md Figure 4.10: Precision and recall for different confidence thresholds for the animal class predicted by MegaDetector. We see that as we increase the confidence threshold, precision increases and recall decreases for the “Animal” class (Figure 4.10). Ideally, we would like to choose a confidence threshold that maximizes both precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the class is truly present when AI identifies the class as being present in an image (Chapter 1). If AI suffers from low precision, then we may have to manually review photos that AI tags as having a class present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a class in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing a class that is truly present. To remedy this problem, we would need to review images where AI says the class is absent in order to reduce false negatives. Let’s say that we were willing to miss only 3% of the animals present. In this case, we could pick the confidence threshold that maximizes precision under the constraint that recall does not fall below 97%. # Thresholds that meet our criterion that Recall &gt;= 0.97 conf_meets &lt;- metrics_all_confs %&gt;% mutate(across(where(is.numeric), ~round(., 2))) %&gt;% filter(Recall &gt;= 0.97) Table 4.2: Performance metrics using a confidence threshold that maximizes precision for a 97% recall. Precision Recall F1 conf_threshold 0.95 0.97 0.96 0.1 We see that we can increase Precision to 95% (while keeping Recall = 97%) by using a confidence threshold of 0.1. Thus, if we integrate MD output with Timelapse 2, filtering using a confidence threshold of 0.1, we expect to capture 97% of Animals that are truly present and 95% of the flagged images should actually include one or more animals. Let’s examine the confusion matrix using this threshold. threshold &lt;- conf_meets[which.max(conf_meets$Precision), 4] conf_red &lt;- both_visions conf_red$class_cv[conf_red$value &lt; threshold] &lt;- &quot;Blank&quot; cm_md_th &lt;- confusionMatrix(data = conf_red$class_cv, reference = conf_red$class_hv, mode = &quot;prec_recall&quot;) plot_cm_md_th &lt;- cm_md_th %&gt;% pluck(&quot;table&quot;) %&gt;% as.data.frame() %&gt;% rename(Frequency = Freq) %&gt;% ggplot(aes(y=Prediction, x=Reference, fill=Frequency)) + geom_raster() + scale_fill_gradient(low = &quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + theme(axis.text.x = element_text(angle = 90)) + geom_text(aes(label = Frequency), size = 3) library(patchwork) plot_cm_md_0.65 &lt;- plot_cm_md_0.65 + theme(legend.position = &quot;None&quot;) plot_cm_md_th &lt;- plot_cm_md_th + theme(legend.position = &quot;None&quot;) (plots_cms_md &lt;- plot_cm_md_0.65 + plot_cm_md_th + plot_annotation(tag_levels = &#39;A&#39;)) Figure 4.11: Confusion matrices processed using 0.65 (A) and 0.1 (B) as confidence thresholds. Comparing the confusion matrix with our original (using a 0.65 confidence threshold; Figure 4.11), we see that we have decreased the false negatives using a confidence threshold of 0.1 (i.e., cases where MD suggests a blank image but an animal is present; last row, first column) but increased the number of false positives (i.e., cases where MD suggests an animal is present, but the image is blank; first row, second column). 4.7 Conclusions We have seen how to use MD and how to integrate its output with Timelapse 2 for using AI while processing camera trap data. Additionally, we illustrated how to evaluate MD performance by comparing true classifications with computer predictions. We found that MD has a high performance to detect animals in images. Thus, the human labor required to review photos can be reliably focused on images with the “Animal” class predictions. We also showed how users can explore MD performance by comparing confusion matrices estimated using different confidence thresholds. This comparison will be useful to understand the trade-off between precision and recall, and help users choose a confidence threshold that maximizes these metrics using their particular data sets. References "],["mlwic2-machine-learning-for-wildlife-image-classification.html", "Chapter 5 MLWIC2: Machine Learning for Wildlife Image Classification 5.1 Set-up 5.2 Upload/format data 5.3 Process images - AI module 5.4 Assessing AI performance 5.5 Model training 5.6 Classify using a trained model 5.7 Conclusion", " Chapter 5 MLWIC2: Machine Learning for Wildlife Image Classification MLWIC2 is an R package that allows you either to use trained models for identifying species from North America (using the species model) or to identify empty images (using the empty_animal model) (Tabak et al. 2020). The model was trained using images from 10 states across the United States but was also tested in out-of sample data sets obtaining a 91% accuracy for species from Canada and 91% - 94% for classifying empty images on samples from different continents (Tabak et al. 2020). Documentation for using MLWIC2 and the list of species that the model identifies can be found in the GitHub repository https://github.com/mikeyEcology/MLWIC2. In this chapter we illustrate the use of the package and data preparation for model training. 5.1 Set-up First, you will need to install R software, Anaconda Navigator (https://docs.anaconda.com/anaconda/navigator/), Python (3.5, 3.6 or 3.7) and Rtools (only for Windows computers). Then you will have to install Tensorflow 1.14 and find the path location of Python on your computer. You can find more installation details in the GitHub repository (https://github.com/mikeyEcology/MLWIC2), as well as an example with installation steps for Windows users (https://github.com/mikeyEcology/MLWIC_examples/blob/master/MLWIC_Windows_Set_up.md). Make sure to install the required versions listed above. Mac users can use the Terminal Application to install Python and Tensorflow. You can use the conda package (https://docs.conda.io/en/latest/) to create an environment that can host a specific version of Python and keep it separated from other packages or dependencies. In the Terminal type: conda create -n ecology python=3.5 conda activate ecology conda install -c conda-forge tensorflow=1.14 Once you complete the installation of Python and Tensorflow, use the command-line utility to find the location of Python. Windows users should type: where python. Mac users should type conda activate ecology and then which python. The output Python location will look something like this: /Users/julianavelez/opt/anaconda3/envs/ecology/bin/python In R, install the devtools package and MLWIC2 packages. Then, setup your environment using the setup function (Tabak et al. 2020) as shown below, making sure to change the python_loc argument to point to the output path provided in the previous step. You will only need to specify this setup once. # Uncomment and only run this code once # if (!require(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) # devtools::install_github(&quot;mikeyEcology/MLWIC2&quot;) # MLWIC2::setup(python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;) Next, download MLWIC2 helper files from here: https://drive.google.com/file/d/1VkIBdA-oIsQ_Y83y0OWL6Afw6S9AQAbh/view. Note, this folder is not included in the repository because of its large size. You must download it and store it in the data/mlwic2 directory. 5.2 Upload/format data Once the package is installed and the python location is setup, you can run a MLWIC2 model to obtain computer vision predictions of the species present in your images. To run MLWIC2 models, you should use the classify function (see Section 5.3), which requires arguments specifying the path (i.e., location on your computer) for the following three inputs: The images that will be classified. The filenames of your images. The location of the MLWIC2 helper files that contain the model information. To create the filenames (input 2 above), you will need to create an input file using the make_input function (Tabak et al. 2020). This will create a CSV file with two columns, one with the filenames and the other one with a list of class_ID’s required to use MLWIC2 for classifying images or training a model. When using the make_input function to create the CSV file, you can select different options depending whether or not you already have filenames and images classified (type ?make_input in R for more options). We will use option = 4 to find filenames associated with each photo using MLWIC2 and recursive = TRUE to specify that photos are in subfolders organized by camera location. We then read the output using the read_csv function (Wickham 2017). Let’s first load required libraries for reading data and to use MLWIC2 functions. library(tidyverse) # for data wrangling and visualization, includes dplyr library(MLWIC2) library(here) # to allow the use of relative paths while reading data When using the make_input function with your data, you must provide the paths indicating the location of your images (path_prefix) and the output directory where you want to store the output. The make_input function will output a file named image_labels.csv which we renamed as images_names_classify.csv and included it with the files associated with this repository. We provide a full illustration of the use of make_input and classify in Section 5.6 using a small image set included in the repository. # The code below won&#39;t be executed. For using it you must replace: # - the &quot;path_prefix&quot; with the path to the directory holding your images # - the &quot;output_directory&quot; with the directory where you want to store the output # from running MLWIC2 make_input(path_prefix = &quot;/Volumes/ct-data/CH1/jan2020&quot;, recursive = TRUE, option = 4, find_file_names = TRUE, output_dir = here(&quot;data&quot;, &quot;mlwic2&quot;)) file.rename(from = &quot;data/mlwic2/image_labels.csv&quot;, to = &quot;data/mlwic2/images_names_classify.csv&quot;) We then read in this file containing the image filenames and look at the first few records. We will use the here package (Müller 2017) to tell R that our file lives in the ./data/mlwic2 directory and specify the name of our file images_names_classify.csv. # inspect output from the make_input function image_labels &lt;- read_csv(here(&quot;data&quot;,&quot;mlwic2&quot;,&quot;images_names_classify.csv&quot;)) head(image_labels) ## # A tibble: 6 × 2 ## `A01/01080001.JPG` `0` ## &lt;chr&gt; &lt;dbl&gt; ## 1 A01/01080002.JPG 0 ## 2 A01/01080003.JPG 0 ## 3 A01/01080004.JPG 0 ## 4 A01/01080005.JPG 0 ## 5 A01/01080006.JPG 0 ## 6 A01/01080007.JPG 0 5.3 Process images - AI module Once you have the CSV file with image filenames (images_names_classify.csv), you can proceed to run the MLWIC2 models. It is possible to use parallel computing to run the models more efficiently. This will require specifying a number of cores that you want to use while running the models. To do that, first you need to know how many cores (i.e., processors in your computer) are available, which you can determine using the detectCores function in the parallel package (R Core Team 2021). library(parallel) detectCores() # detects number of cores in your computer ## [1] 4 If you have 4 cores, then you can use 3 cores for running the MLWIC2 model using the classify function (Tabak et al. 2020) and the num_cores argument. This assures that you leave one core for your computer to perform other tasks. Other arguments for the classify function include: path_prefix: absolute path of the location of your camera trap photos. data_info: absolute path of the images_names_classify.csv file (i.e., the output from the make_input function). model_dir: absolute path of MLWIC2 helper files folder. python_loc: absolute path of Python on your computer. os: specification of your operating system (here “Mac”). make_output: use TRUE for a ready-to-read CSV output. To complete this step, you would need to change the file paths to indicate where the files are located on your computer. Note that this step took approximately 5 hours to process 112,247 photos (run on a macOS Mojave with a 2.5 GHz Intel Core i5 Processor and 8GB 1600 MHz DDR3 of RAM). classify(path_prefix = &quot;/Volumes/ct-data/CH1/jan2020/&quot;, data_info = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images_names_classify.csv&quot;), model_dir = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_helper_files/&quot;), python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;, # absolute path of Python on your computer os = &quot;Mac&quot;, # specify your operating system; &quot;Windows&quot; or &quot;Mac&quot; make_output = TRUE, num_cores = 3 ) 5.4 Assessing AI performance As with other AI platforms, it is recommended to evaluate model performance with your particular data set before using an AI model to classify all your images. This involves classifying a subset of your photos and comparing those classifications with predictions provided by MLWIC2 (i.e., we will compare human vs. computer vision). Let’s start by formatting the human vision data set. 5.4.1 Format human vision data set To read the file containing the human vision classifications for a subset of images, we tell R the path (i.e., directory name) that holds our file. Again, we use the here package (Müller 2017) to tell R that our file lives in the ./data/mlwic2 directory and specify the name of our file images_hv_jan2020.csv. You may, alternatively, type in the full path to the file folder or a relative path from the root directory if you are using a project in Rstudio. We then read our file using the read_csv function (Wickham 2017). The human vision data set (images_hv_jan2020.csv) was previously cleaned to remove duplicated records and to summarise multiple rows that reference animals of the same species identified in the same image (see Chapter 3 for details about these steps). If there are multiple species in an image, then you may have multiple rows associated with each detection. In the Wildlife Insights, we demonstrated methods for evaluating AI performance that considered the possibility of detecting multiple species in an image. However, we can simplify the process of evaluating AI performance if we filter the data to include a single record (e.g., the first object) for each image. To keep a single record per image, we first group rows by deployment_id, filename, timestamp (key columns that identify an image) and then use the slice function (Wickham et al. 2019). This step drops 44 observations, reducing the data set from 104,826 classifications to 104,782 classifications. We use “pipes” (%&gt;%) from the magrittr package (Bache and Wickham 2020). Pipes (%&gt;%) provide a way to execute a sequence of data operations, organized so that the operations can be read from left to right (e.g., “Take the file and then group rows using group_by”). human_vision &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images_hv_jan2020.csv&quot;)) # number of rows prior to dropping records associated with multiple species # in an image nrow(human_vision) ## [1] 104826 human_vision &lt;- human_vision %&gt;% group_by(deployment_id, filename, timestamp) %&gt;% # select first row slice(1) # Keep only one row per image # Number of records after dropping records associated with multiple species # in an image nrow(human_vision) ## [1] 104782 # Inspect the first few rows of the data set head(human_vision) ## # A tibble: 6 × 4 ## # Groups: deployment_id, filename, timestamp [6] ## deployment_id filename timestamp common_name ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 A01-Jan2020-Jul2020 01090079.JPG 2020-01-09 10:54:39 Blank ## 2 A01-Jan2020-Jul2020 01090080.JPG 2020-01-09 10:54:40 Blank ## 3 A01-Jan2020-Jul2020 01090081.JPG 2020-01-09 10:54:41 Blank ## 4 A01-Jan2020-Jul2020 01090082.JPG 2020-01-09 10:54:49 Blank ## 5 A01-Jan2020-Jul2020 01090083.JPG 2020-01-09 10:54:50 Blank ## 6 A01-Jan2020-Jul2020 01090084.JPG 2020-01-09 10:54:51 Blank MLWIC2 outputs columns with filename and the top-5 predictions along with their associated confidence values. Filenames are represented as camera_location/image_filename. We will need to create a variable in the human vision data set that has this same format so that we can merge the data sets containing human and computer vision classifications. In the human vision data set, the camera location and filename are contained in different columns, so we combine them using the unite function (Wickham et al. 2019). Before doing so, we use the str_replace function to reformat the deployment_id variable so that it matches the format of the MLWIC2 data set. These steps will allow us to later join our human and computer vision data sets using the common variable, filename. We also select the columns of interest. human_vision$deployment_id &lt;- human_vision$deployment_id %&gt;% str_replace(pattern = &quot;-.*&quot;, &quot;/&quot;) human_vision &lt;- human_vision %&gt;% unite(filename, c(&quot;deployment_id&quot;, &quot;filename&quot;), sep = &quot;&quot;) hv &lt;- human_vision %&gt;% group_by(filename, timestamp) %&gt;% select(filename, timestamp, common_name) # select columns of interest MLWIC2 provides predictions for species from North America (see list of predicted species here https://github.com/mikeyEcology/MLWIC2/blob/master/speciesID.csv). However, you can also use the MLWIC2 empty_animal model for distinguishing blanks from images containing an object. For our example with species from South America, we will use the species_model as we want to evaluate model performance for predicting species present both in North and South America. These species include cattle (in MLWIC2 species list “Cow”), armadillos, opossums, horses, humans, dogs, white-tailed deer and mountain lions (i.e., pumas). Let’s inspect the species names in our data set, as we might have to replace some names according to MLWIC2 classes. unique(human_vision$common_name) %&gt;% sort() ## [1] &quot;Alouatta Species&quot; &quot;Amazonian Motmot&quot; ## [3] &quot;Ants&quot; &quot;Bird&quot; ## [5] &quot;Black Agouti&quot; &quot;Blank&quot; ## [7] &quot;Bos Species&quot; &quot;Bush Dog&quot; ## [9] &quot;Caprimulgidae Family&quot; &quot;Capybara&quot; ## [11] &quot;Cervidae Family&quot; &quot;Collared Peccary&quot; ## [13] &quot;Common Green Iguana&quot; &quot;Crab-eating Fox&quot; ## [15] &quot;Crestless Curassow&quot; &quot;Dasypus Species&quot; ## [17] &quot;Domestic Dog&quot; &quot;Domestic Horse&quot; ## [19] &quot;Fasciated Tiger-heron&quot; &quot;Giant Anteater&quot; ## [21] &quot;Giant Armadillo&quot; &quot;Giant Otter&quot; ## [23] &quot;Insect&quot; &quot;Jaguar&quot; ## [25] &quot;Jaguarundi&quot; &quot;Lizards and Snakes&quot; ## [27] &quot;Lowland Tapir&quot; &quot;Mammal&quot; ## [29] &quot;Margarita Island Capuchin&quot; &quot;Margay&quot; ## [31] &quot;Neotropical Otter&quot; &quot;Northern Amazon Red Squirrel&quot; ## [33] &quot;Ocelot&quot; &quot;Ornate Tití Monkey&quot; ## [35] &quot;Pecari Species&quot; &quot;Possum Family&quot; ## [37] &quot;Puma&quot; &quot;Red Brocket&quot; ## [39] &quot;Rodent&quot; &quot;Saimiri Species&quot; ## [41] &quot;South American Coati&quot; &quot;Southern Tamandua&quot; ## [43] &quot;Spix&#39;s Guan&quot; &quot;Spotted Paca&quot; ## [45] &quot;Tayra&quot; &quot;Turkey Vulture&quot; ## [47] &quot;Turtle Order&quot; &quot;Unknown species&quot; ## [49] &quot;Weasel Family&quot; &quot;White-lipped Peccary&quot; ## [51] &quot;White-tailed Deer&quot; Checking the species list, we can identify the species of interest present in North and South America. To replace species names with names used by MLWIC2, we use the case_when function (Wickham et al. 2019) to implement multiple conditional statements, creating a variable, class, containing the replaced names; this makes sure that we are using the same species names in both visions. We include a class Other_hv for classes identified by human vision that are not identified by MLWIC2 computer vision. hv &lt;- hv %&gt;% mutate(class = factor(case_when(common_name == &quot;Blank&quot; ~ &quot;Blank&quot;, common_name == &quot;Human&quot; | common_name == &quot;Human-Camera Trapper&quot; ~ &quot;Human&quot;, common_name == &quot;Bos Species&quot; ~ &quot;Cattle&quot;, common_name == &quot;Dasypus Species&quot; ~ &quot;Armadillo&quot;, common_name == &quot;Possum Family&quot; ~ &quot;Opossum&quot;, common_name == &quot;Domestic Horse&quot; ~ &quot;Horse&quot;, common_name == &quot;Domestic Dog&quot; ~ &quot;Dog&quot;, common_name == &quot;White-tailed Deer&quot; | common_name ==&quot;Cervidae Family&quot; ~ &quot;White-tailed Deer&quot;, common_name == &quot;Puma&quot; ~ &quot;Puma&quot;, TRUE ~ &quot;Other_hv&quot;))) %&gt;% ungroup() 5.4.2 Format computer vision data set Let’s read the MLWIC2 output, remove unwanted patterns in the filenames using str_remove and replace species codes with species names using case_when. We include a class Other_cv for classes identified by computer vision that were not identified by human vision. We also use the factor and the levels function (R Core Team 2021) to convert the classification variables into factors with the same levels in both data sets. Note that we have moved the model output to the data/mlwic2 folder from its original location (in the same folder as the MLWIC2_helper_files that you previously downloaded). computer_vision &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_output.csv&quot;)) cv &lt;- computer_vision %&gt;% mutate(across(c(guess1, guess2, guess3, guess4, guess5), ~ case_when(. == 27 ~ &quot;Blank&quot;, . == 11 ~ &quot;Human&quot;, . == 1 ~ &quot;Cattle&quot;, . == 7 ~ &quot;Armadillo&quot;, . == 9 ~ &quot;Opossum&quot;, . == 10 ~ &quot;Horse&quot;, . == 15 ~ &quot;Dog&quot;, . == 18 ~ &quot;White-tailed Deer&quot;, . == 20 ~ &quot;Puma&quot;, TRUE ~ &quot;Other_cv&quot;))) # The code below will remove the path of each picture and other unwanted patterns. # It will only leave the location name and filename. cv$fileName &lt;- cv$fileName %&gt;% str_remove(pattern = &quot;b&#39;/Volumes/ct-data/CH1/jan2020/&quot;) %&gt;% str_remove(pattern = &quot;&#39;&quot;) %&gt;% str_remove(pattern = &quot;detections/&quot;) cv$fileName &lt;- sub(&quot;/1.*113&quot;, &quot;&quot;, cv$fileName) When creating the computer vision label, we will only consider MLWIC2’s top guess (i.e., guess1). cv &lt;- cv %&gt;% rename(filename = fileName, class = guess1) all_levels &lt;- append(levels(factor(cv$class)), levels(factor(hv$class))) %&gt;% unique() %&gt;% sort() cv$class &lt;- factor(as.character(cv$class), levels = all_levels) hv$class &lt;- factor(as.character(hv$class), levels = all_levels) 5.4.3 Merging computer and human vision data sets Now that we have the same format for both human and computer vision data sets, we can use various “joins” (Wickham et al. 2019) to merge the two data sets together so that we can evaluate the accuracy of MLWIC2. First, however, we will eliminate any images that were not processed by both humans and AI. # Determine which images have been viewed by both methods ind1 &lt;- cv$filename %in% hv$filename # in both ind2 &lt;- hv$filename %in% cv$filename # in both cv &lt;- cv[ind1,] # eliminate images not processed by human vision hv &lt;- hv[ind2,] # eliminate images not processed by computer vision # Number of photos eliminated sum(ind1 != TRUE) # in computer vision but not in hv ## [1] 8231 sum(ind2 != TRUE) # in human vision but not in cv ## [1] 766 Now, we can use: an inner_join with filename and class to determine images that have correct predictions (i.e., images with the same class assigned by computer and human vision) an anti_join with filename and class to determine which records in the human vision data set have incorrect predictions from computer vision. an anti_join with filename and class to determine which records in the computer vision data set have incorrect predictions. We assume the classifications from human vision to be correct and distinguish them from MLWIC2 predictions. The MLWIC2 predictions will be correct if they match a class assigned by human vision for a particular record and incorrect if the classes assigned by the two visions differ. # correct predictions matched &lt;- cv %&gt;% inner_join(y = hv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% mutate(class_hv = class) %&gt;% rename(class_cv = class) %&gt;% select(filename, confidence1, class_cv, class_hv) # records in the human vision data set whose predictions are incorrect hv_only &lt;- hv %&gt;% anti_join(y = cv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) %&gt;% rename(class_hv = class) # records in the computer vision data set whose predictions are incorrect cv_only &lt;- cv %&gt;% anti_join(y = hv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% rename(class_cv = class) We then use left_join to merge the predictions from the cv_only (computer vision) data set onto the records from the hv_only (human vision) data set. hv_mismatch &lt;- hv_only %&gt;% left_join(cv_only, by = &quot;filename&quot;) %&gt;% select(filename, confidence1, class_cv, class_hv) We then check for any computer vision records that are not yet accounted for in our data sets containing records with correct or incorrect predictions, i.e., matched and hv_mismatch, respectively. cv_others &lt;- cv_only[cv_only$filename %in% hv_mismatch$filename != TRUE,] cv_others ## # A tibble: 0 × 13 ## # … with 13 variables: ...1 &lt;dbl&gt;, filename &lt;chr&gt;, answer &lt;dbl&gt;, ## # class_cv &lt;fct&gt;, guess2 &lt;chr&gt;, guess3 &lt;chr&gt;, guess4 &lt;chr&gt;, guess5 &lt;chr&gt;, ## # confidence1 &lt;dbl&gt;, confidence2 &lt;dbl&gt;, confidence3 &lt;dbl&gt;, confidence4 &lt;dbl&gt;, ## # confidence5 &lt;dbl&gt; Finally, we select only the variables we need, and combine the matched and mismatched data sets. Additionally, we remove the “Human” class from the data set; human images in this data set predominately correspond to images taken during camera setup. matched &lt;- matched %&gt;% select(filename, confidence1, class_cv, class_hv) hv_mismatch &lt;- hv_mismatch %&gt;% select(filename, confidence1, class_cv, class_hv) both_visions &lt;- rbind(matched, hv_mismatch) both_visions &lt;- both_visions %&gt;% filter(class_cv != &quot;Human&quot; &amp; class_hv != &quot;Human&quot;) 5.4.4 Summarizing human and computer vision records by class Next, we count the number of records of each class separately for human and computer vision. We will group the data by class and then count the number of observations using n() inside the summarise function. Then, we use the left_join function to join the class counts in computer and human vision using the class to match observations in the two data sets. Lastly, we add a suffix to the class variable to distinguish the counts of human and computer vision. Table 5.1 contains the class counts for the two visions. sp_counts_cv &lt;- both_visions %&gt;% group_by(class_cv) %&gt;% summarise(n = n()) %&gt;% rename(class = class_cv) sp_counts_hv &lt;- both_visions %&gt;% group_by(class_hv) %&gt;% summarise(n = n()) %&gt;% rename(class = class_hv) sp_counts &lt;- full_join(x = sp_counts_cv, y = sp_counts_hv, by = &quot;class&quot;, suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% arrange(class) Table 5.1: Counts of images classified by MLWIC2 (n_cv) and humans (n_hv) for each class in the data set. 5.4.5 Confusion matrix and performance measures Finally, we can proceed with estimating a confusion matrix and various AI performance measures using the confusionMatrix function from the caret package (Kuhn 2021) and specifying a 0.65 confidence threshold to accept MLWIC2 predictions. We can then plot the confusion matrix using ggplot2 (Wickham et al. 2018). The confusionMatrix function requires a data argument for predicted classes and a reference for true classifications, both as factor classes and with the same factor levels. We include mode=&quot;prec_recall&quot; when calling the confusionMatrix function (Kuhn 2021) to estimate precision and recall. library(caret) # to inspect model performance library(ggplot2) # Estimate confusion matrix both_visions_0.65 &lt;- both_visions both_visions_0.65$class_cv[both_visions_0.65$confidence1 &lt; 0.65] &lt;- &quot;Blank&quot; cm_mlwic2_0.65 &lt;- confusionMatrix(data = both_visions_0.65$class_cv, reference = (both_visions_0.65$class_hv), mode = &quot;prec_recall&quot;) # Plot confusion matrix plot_cm_mlwic2_0.65 &lt;- cm_mlwic2_0.65 %&gt;% pluck(&quot;table&quot;) %&gt;% as.data.frame() %&gt;% rename(Frequency = Freq) %&gt;% ggplot(aes(y=Prediction, x=Reference, fill=Frequency)) + geom_raster() + scale_fill_gradient(low = &quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + geom_text(aes(label = Frequency), size = 2.5) +# size for matrix counts theme(legend.position = &quot;bottom&quot;, axis.text.x = element_text(angle = 90), # define angle for x axis text) legend.text = element_text(size = 6), legend.key.width = unit(1, &quot;cm&quot;)) plot_cm_mlwic2_0.65 Figure 5.1: Confusion matrix applied to classifictions from MLWIC2 using a confidence threshold of 0.65. Now we can use the confusion matrix to estimate metrics of model performance including accuracy, precision, recall and F-1 score (See Chapter 1 for metrics description). (overall_accuracy &lt;- cm_mlwic2_0.65 %&gt;% pluck(&quot;overall&quot;, &quot;Accuracy&quot;) %&gt;% round(., 2)) ## [1] 0.11 classes_metrics_mlwic2_0.65 &lt;- cm_mlwic2_0.65 %&gt;% pluck(&quot;byClass&quot;) %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% rownames_to_column() %&gt;% rename(class = rowname) %&gt;% mutate(across(is.numeric, ~round(., 2))) %&gt;% filter(!is.na(Precision | Recall)) classes_metrics_mlwic2_0.65$class &lt;- str_remove(string = classes_metrics_mlwic2_0.65$class, pattern = &quot;Class: &quot;) Table 4.1: Model performance metrics for each class in the data set using a 0.65 confidence threshold. We see that the model has the highest precision values for the “puma” (80% precision at 3% recall) and “cattle” (76% precision at 3% recall) classes. The “Blank” class has the highest recall value of 53% at a 24% precision. We can inspect how model performance changes if we select a different confidence threshold than 0.65, as we demonstrate in the next section. 5.4.6 Confidence thresholds Lets begin by looking at the distribution of confidence values associated with each MLWIC2 classification using the geom_bar function (Wickham et al. 2018). We first identify the species that have at least 30 records for both classification methods (human and computer vision). sp_plots &lt;- sp_counts %&gt;% filter(n_cv &gt; 30 &amp; n_hv &gt; 30) both_visions_sp &lt;- both_visions %&gt;% filter(class_cv %in% sp_plots$class) %&gt;% rename(Species = class_cv) # Plot confidence values both_visions_sp %&gt;% ggplot(aes(confidence1, group = Species, colour = Species)) + geom_bar() + facet_wrap(~Species, scales = &quot;free&quot;) + labs(y = &quot;Empirical distribution&quot;, x = &quot;Confidence values&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_color_viridis_d() Figure 5.2: Distribution of confidence values associated with classes predicted by MLWIC2. Most of the classes (e.g., “cattle”, “opossum”, “puma” and “white-tailed deer”) have a uniform distribution with records distributed along the full range of confidence values. The “Blank” class has a bell-shaped distribution, with most of the records associated with medium confidence values. To inspect how precision and recall change when different confidence thresholds are established for assigning a class predicted by computer vision, we define a function that will calculate these metrics for a user-defined confidence threshold. This function will assign a “Blank” label whenever the confidence for a computer vision prediction is below a particular confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall change with the confidence threshold and choose a threshold that balances these two performance metrics. threshold_for_metrics &lt;- function(conf_threshold = 0.7) { tmp &lt;- both_visions tmp$class_cv[tmp$confidence1 &lt; conf_threshold] &lt;- &quot;Blank&quot; # assign a &quot;Blank&quot; class whenever the confidence value # of a prediction is lower than the threshold provided as # an argument in the function cm &lt;- confusionMatrix(data = tmp$class_cv, reference = tmp$class_hv, mode = &quot;prec_recall&quot;) # use the confusionMatrix function from the caret package using the # class_cv containing the new labels according to a particular # confidence threshold classes_metrics &lt;- cm %&gt;% # get confusion matrix pluck(&quot;byClass&quot;) %&gt;% # get metrics by class as.data.frame() %&gt;% # assign a data frame object select(Precision, Recall, F1) %&gt;% # select metrics of interest rownames_to_column() %&gt;% # format data frame rename(class = rowname) %&gt;% # rename class column mutate(conf_threshold = conf_threshold) classes_metrics$class &lt;- str_remove(string = classes_metrics$class, pattern = &quot;Class: &quot;) return(classes_metrics) # return a data frame with metrics for every class } Let’s estimate model performance metrics for confidence values ranging from 0.1 to 0.99 using the map_df function (Henry and Wickham 2020) . The map_df function (Henry and Wickham 2020) returns a data frame object. Once we get a dataframe of model performance metrics for a range of confidence values, we can plot the results using the ggplot2 package (Wickham et al. 2018). conf_vector = seq(0.1, 0.99, length=100) metrics_all_confs &lt;- map_df(conf_vector, threshold_for_metrics) metrics_all_confs &lt;- metrics_all_confs %&gt;% mutate_if(is.numeric, round, digits = 2) prec_rec_mlwic2 &lt;- metrics_all_confs %&gt;% filter(class %in% sp_plots$class) %&gt;% rename(Class = class, Confidence_threshold = conf_threshold) %&gt;% ggplot(aes(x = Recall, y = Precision, group = Class, colour = Class)) + geom_point(aes(size = Confidence_threshold)) + scale_size(range = c(0.1,3)) + labs(x = &quot;Recall&quot;, y = &quot;Precision&quot;, ) + scale_color_viridis_d() + geom_line() prec_rec_mlwic2 Figure 5.3: Precision and recall for different confidence thresholds for classes predicted by MLWIC2. We see that as we increase the confidence threshold, precision associated with the different species labels usually increases and recall decreases (Figure 5.3); the opposite pattern occurs for the “Blank” class. Ideally, we would like AI to have high precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the class is truly present when AI identifies the class as being present in an image (Chapter 1). If AI suffers from low precision, then we may have to manually review photos that AI tags as having a class present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a class in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing a class that is truly present. To remedy this problem, we would need to review images where AI says the class is absent in order to reduce false negatives. To compare confusion matrices estimated using different confidence thresholds please refer to Chapter 4. 5.5 Model training For training a model, we also need to provide a CSV file containing image filenames. For illustration, we will get the filenames from a small set of images included in the repository (images/train folder), but you will want to train a model with at least 1,000 labeled images per species (Schneider et al. 2020). To get the filenames for those images we can use the make_input function (See Section 5.2); in the argument path_prefix you should provide the path of the directory containing the images. The make_input function will create the image_labels.csv file in the directory provided in the output_dir argument. We rename this file as images_names_train_temp.csv. # this will read filenames from each image and create a CSV file with them make_input(path_prefix = here(&quot;data&quot;,&quot;mlwic2&quot;,&quot;images&quot;, &quot;train&quot;), option = 4, find_file_names = TRUE, output_dir = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;)) ## Your file is located at &#39;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/mlwic2/training/image_labels.csv&#39;. file.rename(from = &quot;data/mlwic2/training/image_labels.csv&quot;, to = &quot;data/mlwic2/training/images_names_train_temp.csv&quot;) ## [1] TRUE Once we have the filenames for the training set, we add the corresponding human vision labels for each image using a left_join. Additionally, we recode our species names with numbers as required by the MLWIC2 package; these numbers must be consecutive and should start with 0 (Tabak et al. 2020). We save this file images_names_train.csv with recoded species names using the write_csv function. img_labs &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_names_train_temp.csv&quot;), col_names = c(&quot;filename&quot;, &quot;class&quot;)) hv_train &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_hv.csv&quot;)) imgs_train &lt;- img_labs %&gt;% left_join(hv_train, by = &quot;filename&quot;) %&gt;% select(filename, common_name) %&gt;% mutate(class_ID = factor(case_when(common_name == &quot;Blank&quot; ~ 1, common_name == &quot;Black Agouti&quot; ~ 2, common_name == &quot;Bos Species&quot; ~ 3, common_name == &quot;Cervidae Family&quot; | common_name == &quot;White-tailed Deer&quot; ~ 4, common_name == &quot;Collared Peccary&quot; ~ 5, common_name == &quot;Dasypus Species&quot; ~ 6, common_name == &quot;Domestic Dog&quot; ~ 7, common_name == &quot;Domestic Horse&quot; ~ 8, common_name == &quot;Giant Anteater&quot; ~ 9, common_name == &quot;Giant Armadillo&quot; ~ 10, common_name == &quot;Human&quot; | common_name == &quot;Human-Camera Trapper&quot; ~ 11, common_name == &quot;Jaguar&quot; ~ 12, common_name == &quot;Lowland Tapir&quot; ~ 13, common_name == &quot;Nine-banded Armadillo&quot; ~ 14, common_name == &quot;Ocelot&quot; ~ 15, common_name == &quot;Possum Family&quot; ~ 16, common_name == &quot;Puma&quot; ~ 17, common_name == &quot;South American Coati&quot; ~ 18, common_name == &quot;Southern Tamandua&quot; ~ 19, common_name == &quot;Spotted Paca&quot; ~ 20, common_name == &quot;White-lipped Peccary&quot; ~ 21, TRUE ~ 0))) %&gt;% ungroup() %&gt;% select(filename, class_ID) write_csv(imgs_train, file = &quot;data/mlwic2/training/images_names_train.csv&quot;, col_names = FALSE) We then use the train function to train a new model, where we need to specify arguments that were also used with the classify function (see Section 5.3); these include the path_prefix, model_dir, python_loc, os and num_cores. In the data_info argument we pass the images_names_train.csv file. We also need to specify the number of classes that we want to train the model to predict (e.g., these classes might differ from the 58 classes predicted when using MLWIC2’s built-in AI species model). We can use retrain = FALSE to train a model from scratch or retrain = TRUE if we want to retrain a pre-specified model using transfer learning.1 For more references on model training see https://github.com/mikeyEcology/MLWIC2. We also need to specify the number of epochs (i.e., the number of times the learning algorithm will iterate on the training data set) using thenum_epochs argument; we use the recommended default of 55 (Tabak et al. 2020). Lastly, we specify the directory where we want to store the model using the log_dir_train argument; we use “SA” for “South America”. train(path_prefix = here(&quot;data&quot;,&quot;mlwic2&quot;, &quot;images&quot;, &quot;train&quot;), data_info = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_names_train.csv&quot;), # absolute path to the make_input output model_dir = here(&quot;data&quot;,&quot;mlwic2&quot;, &quot;MLWIC2_helper_files/&quot;), python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;, # absolute path of Python on your computer os = &quot;Mac&quot;, # specify your operating system; &quot;Windows&quot; or &quot;Mac&quot; num_cores = 3, num_classes = 22, # number of classes in your data set retrain = FALSE, num_epochs = 55, # initially you can use a smaller number for num_epochs to verify model training; it will take a long time to run. log_dir_train =&quot;SA&quot; ) 5.6 Classify using a trained model Then you can run the model that you trained using a test image set. You should also get filenames for these images (renamed as images_names_test.csv) and pass it when running the model with the classify function. make_input(path_prefix = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images&quot;, &quot;test&quot;), option = 4, find_file_names = TRUE, output_dir = here(&quot;data&quot;,&quot;mlwic2&quot;,&quot;training&quot;)) ## Your file is located at &#39;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/mlwic2/training/image_labels.csv&#39;. file.rename(from = &quot;data/mlwic2/training/image_labels.csv&quot;, to = &quot;data/mlwic2/training/images_names_test.csv&quot;) ## [1] TRUE classify(path_prefix = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images&quot;, &quot;test&quot;), # absolute path to test images directory data_info = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_names_test.csv&quot;), # absolute path to your image_labels.csv file model_dir = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_helper_files&quot;), log_dir = &quot;SA&quot;, # model name python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;, # absolute path of Python on your computer os = &quot;Mac&quot;, # specify your operating system; &quot;Windows&quot; or &quot;Mac&quot; make_output = TRUE, num_cores = 3, output_name = &quot;output_SA.csv&quot;, # name for model output num_classes = 22 ) Finally, you can read the trained model output to see how it looks like. It contains a column with the human vision label, answer, and top-5 predictions for each image with their associated confidence values. Once you have this output, you can verify model performance as described along Section 5.4. output_trained &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_helper_files&quot;, &quot;output_SA.csv&quot;)) 5.7 Conclusion We have seen how to set-up MLWIC2, prepare the required input files, and run its built-in AI model. Additionally, we illustrated how one can evaluate MLWIC2 performance by comparing true classifications with computer predictions, for species or groups found in North and South America (e.g., “Blank”, “cattle”,“armadillo”, “opossum”, “horse”, “dog”, “white-tailed deer” and “puma”). For our data set, MLWIC2 classifications had low precision and recall, probably due to strong differences between the training and the test data. Thus, we would need to train our own models using the tools provided by the MLWIC2 package to improve model performance with our data. References "],["conservation-ai.html", "Chapter 6 Conservation AI 6.1 Set-up 6.2 Upload/format data 6.3 Image tagging 6.4 Process images - AI module", " Chapter 6 Conservation AI Conservation AI is a platform aimed at facilitating the use of AI to solve conservation problems. It is developed by research experts in Machine Learning, Computer Science and Conservation Biology from the Liverpool John Moores University and NVIDIA (a technology company that developed the NVIDIA CUDA®, a collection of libraries and tools for using AI; more information can be found here https://developer.nvidia.com/gpu-accelerated-libraries). Currently, Conservation AI can detect species from the UK, North America and from Sub-Saharan Africa. However, it also provides a user-friendly interface for creating a data set that can be used to train an AI model to detect species from your particular region. To create the training data set you will need to tag a subset of your images. This step requires creating a bounding box around animals in the images and assigning a species label. We illustrate the tagging process using the Conservation AI infrastructure. 6.1 Set-up Create an account here https://www.conservationai.co.uk/register/ Once you create an account, it needs to be activated by the Conservation AI team. To do that you can contact them using their message center https://www.conservationai.co.uk/contact/ After being approved, you will have a dashboard were you have sections for uploading your files, tagging images with the corresponding classification, and checking AI results (Figure 6.1). Figure 6.1: Conservation AI dashboard with sections for uploading images, tagging images and checking AI results. 6.2 Upload/format data The Conservation AI team will help you to set up a project, after which you will be able to upload images and begin the tagging process. You will need to submit a list of species contained in your images to the Conservation AI team, and they will create the tagging project for you. For categories that are not of interest or species that are difficult to identify, you can include in your list higher taxonomic levels (e.g., Class Aves, Order Rodentia). Additionally, you will have to share a subset of images that will be used for training; you can do that either using the “Upload Files” section (Figure 6.1) or using other transfer methods like Google Drive to share your images with the Conservation AI team. The first option is recommended as it allows users to maintain a more independent workflow for image upload; otherwise, you will have to request the Conservation AI team to upload the images for you, which might slow down your tagging process. Uploads are currently limited to 500 images per batch but should increase to 10,000 images per batch in early 2022. Once uploaded, you will find the images in your tagging interface (Figure 6.4). 6.3 Image tagging In the tagging section, you can find a report of species that have been tagged within the Conservation AI platform and the number of tags for each species (Figure 6.2). Here you can check if your species of interest already contains tags from other users. Additionally, after sharing your species list with the Conservation AI team, you will find your species included in this list and see updates of the number of tags for each species as you advance in the tagging process. Figure 6.2: Report of species tagged within Conservation AI and number of tags per species. To begin image tagging, you can access the tagging interface by clicking on the “Start Tagging” green tab (Figure 6.2), which will direct you to a list of projects (Figure 6.3) where you should search for the project created for you. Figure 6.3: List of projects that can be accessed to perform image tagging. Accessing your project will direct you to your tagging interface (Figure 6.4), where you will see images to be tagged at the left, an image viewer at the center, and a tag list at the right (this is the same list that you shared with the Conservation AI team). As you detect and tag species in an image, make sure you select the “Detection” tab in the top right. Figure 6.4: Tagging interface for drawing bounding boxes around animals detected in the images and for assigning species labels. You will inspect your images using the image viewer, where you can zoom in and out and enter the full-screen mode using the controls below the image. (Figure 6.4). To tag the image, you must draw a bounding box by clicking and dragging a rectangle on top of the animal detected (Figure 6.5). Figure 6.5: A bounding box is drawn around an animal detected in the image. Once you draw the bounding box, you must assign a species label by selecting it from the tagging list (Figure 6.6) and save your tag using the blue “Save” tab. Once you save your tag, the image will be moved to the tagged images list at the bottom of the interface (Figure 6.7). Figure 6.6: Species labels can be assigned by selecting the species name from the Tag List and saved using the blue “Save” tab. Figure 6.7: Images tagged will be collected at the bottom of the tagging interface. The species might also contain higher taxonomic levels or other categories of interest (e.g., vehicles, people, etc.). It will also contain a “No Good” category that must be used for tagging empty or blurred images (Figure 6.8), or images containing small fragments of animal’s body parts. The “No Good” images will not be used for model training. Figure 6.8: Use of the “No Good” tag for empty images. When more than one animal is present, multiple tags can be assigned to the image (Figure 6.9). Remember to save all your tags. Figure 6.9: Tags used for multiple animals in an image. Instead of selecting species labels from the species list at the right, after sketching the bounding box, you can type the shortcuts represented by the letters, numbers or symbols at the right of each species name in the Tag list. You can edit these shortcuts by clicking the ... icon, as we show with the Class Aves, to which we assign the + sign as shortcut (Figure 6.10). You can also use shortcuts to hide/show bounding boxes, enter full-screen mode and for saving your tags (Figure 6.11). Figure 6.10: Shortcuts can be assigned or editted for every species or group in the Tag List. You can use these shortcuts for quickly assigning species labels. Figure 6.11: Shortcuts for bounding boxes display, entering full-screen mode and saving your tags are also available below the image viewer. Once you finish tagging your batch of 500 images, you need to upload another batch to your tagging site. If you shared your images via Google Drive with the Conservation AI team, you can contact them to upload the images for you. The upload of each of these batches by the Conservation AI team can take up a few weeks depending on developers’ availability, so it’s recommended that you upload your images directly into the platform. 6.4 Process images - AI module Once the tagging stage is complete, you can contact the Conservation AI team (at admin@conservationai.co.uk) to have them train models using a transfer learning approach using the training data set that you provided in the image-tagging stage. Metadata, such as the time/date of the image and filename, are automatically read once images are uploaded for classification. Thus, you do not need to enter any metadata other than a sensor id if applicable. Once the classification stage is complete, the results can be accessed in the platform’s analytical dashboard. Completion of model training will depend on the Conservation AI team’s availability, so it’s important to schedule model training with developers that will re-train models with your data. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
