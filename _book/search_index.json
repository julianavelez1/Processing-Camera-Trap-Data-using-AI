[["index.html", "Guide for using artificial intelligence platforms for camera-trap data processing", " Guide for using artificial intelligence platforms for camera-trap data processing Juliana Velez and John Fieberg 2022-08-10 Cover photograph: Camera-trap images collected in the reserves El Rey Zamuro and Las Unamas, located in the Meta department in the Colombian Orinoquia. Suggested Citation: Velez, J. and J. Fieberg. 2022. Guide for using artificial intelligence platforms for camera-trap data processing. https://ai-camtraps.netlify.app/. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Our objectives in writing this guideline are to: Describe the steps needed to set up and process camera trap data using popular artificial intelligence (AI) platforms, including Wildlife Insights, MegaDetector, MLWIC2, and Conservation AI. Demonstrate common workflows for analyzing camera trap data using these platforms via a case study in which we process data collected by the lead author. The aim of the case study project is to develop a joint species distribution model integrating data from camera traps and acoustic sensors to understand interactions between wildlife species in multi-functional landscapes in Colombia. Each chapter covers a different AI platform, and we provide appropriate links to instruction manuals and other resources for researchers looking for additional documentation. We describe the steps required to set up the platforms, upload pictures (e.g., required folder structure), and include and format metadata (e.g., geographical coordinates of locations, deployment dates, and other deployment information such as camera height, use of bait, etc.). We then provide guidance on how to use the artificial intelligence platforms for object detection (e.g., to separate blanks from non-blanks) and species classification. Importantly, we also demonstrate methods for evaluating the performance of AI platforms. Before AI platforms can be evaluated, users will need to manually label a subset of images which can then be compared with AI output. This labeling can be done using a variety of available software (e.g., Scotson et al. 2017), but the resulting data should include, at minimum, the 1) image filename, 2) camera location and 3) species name. The first two variables (e.g., filename and location) are needed to match records from the human-labeled and AI-labeled data sets (hereafter human and computer vision, respectively), and the third variable will allow one to compare human and AI-generated labels. Having a subset of labeled images will allow you to assess how a particular AI model is performing with your data set and determine appropriate use given its performance. We provide annotated R code and examples demonstrating how to compute model performance metrics estimated using categories described in Table 1.1 that classify correct and incorrect predictions. Table 1.1: Notation and categories of classifications used to estimate model performance metrics. NotationDescription TP - True PositivesNumber of observations where the species was correctly identified as being present in the photo. TN - True NegativesNumber of observations where the species was correctly identified as being absent in the photo. FP - False PositivesNumber of observations where the species was absent, but the AI classified the species as being present. FN - False NegativesNumber of observations where the species was present, but the AI classified the species as being absent. Performance metrics include model accuracy, precision, recall and F1 score (Table 1.2; Sokolova and Lapalme 2009). To describe these metrics, we will refer to AI classifications as “predictions” and human vision classifications as “true classifications”. Accuracy is the proportion of correct AI predictions in the data set (Kuhn and Vaughan 2021), precision is the probability that the species is present given it is predicted to be present, and recall is the probability a species is predicted to be present given it is truly present; F1 score is a weighted average of precision and recall (Table 1.2). When inspecting model performance, it can be useful to calculate these metrics separately for each species. ` Table 1.2: Metrics used to assess model performance MetricsEquationInterpretation Accuracy(TP+TN)/(TP+FP+TN+FN)Proportion of correct predictions in a data set. PrecisionTP/(TP+FP)Probability the species is correctly classified as present given that the AI system classified it as present. RecallTP/(TP+FN)Probability the species is correctly classified as present given that the species truly is present. F1 Score2*precision*recall / (precision + recall)Weighted average of precision and recall. AI platforms typically assign a confidence level to each classification, with higher values reflective of more certain classifications. These confidence levels can be used to post-process the data in a way that trades off precision and recall. For example, one can choose to only accept classifications that have a high level of confidence. Doing so will typically reduce the number of false positives, leading to high levels of precision (i.e., users can be more confident that the species is truly present when AI returns a species classification). The number of true positives, and thus recall, may also be reduced but hopefully to a lesser extent. References "],["camera-trap-data.html", "Chapter 2 Camera-trap data", " Chapter 2 Camera-trap data We evaluated model performance using data from a camera trap survey performed from January - July 2020 for wildlife detection within the private natural reserves El Rey Zamuro (31 km2) and Las Unamas (40 km2), located in the Meta department in the Orinoquía region in central Colombia. During the survey period, we collected 112,247 images from a 50-camera-trap array, with cameras spaced 1-km apart; 20 percent of the images were blank and 80 percent contained at least one animal. Records containing the “Vehicle” and “Human” classes were removed from the data set; these were predominately associated with images during camera setup. Images were stored and reviewed by experts using the Wildlife Insights platform. Wildlife Insights was chosen because it provides advanced processing capabilities that helped to accelerate image review (e.g., multiple image selection, image editing and infrastructure for collaborative data processing). Expert (i.e., human vision) labels were compared to predictions derived from AI models associated with Wildlife Insights (downloaded in July 2022), MegaDetector (MDv4.1) and MLWIC2 (v1.0) platforms to determine how well these models would perform when applied to data that were not included in the training data set. "],["wildlife-insights.html", "Chapter 3 Wildlife Insights 3.1 Set-up 3.2 Upload/format data 3.3 Upload/enter metadata 3.4 Processing images - AI module 3.5 Post-AI image processing 3.6 Using AI output 3.7 Assessing AI performance 3.8 Conclusions", " Chapter 3 Wildlife Insights Wildlife Insights (WI) is an initiative developed by Conservation International in partnership with the Wildlife Conservation Society, World Wildlife Fund, Zoological Society of London, The Smithsonian Institution, North Carolina Museum of Natural Sciences, Yale University and Google (Ahumada et al. 2019). WI provides an interface and tools to support workflows for processing, visualizing, and analyzing camera trap data. These tools include infrastructure to store, review and process images, AI to classify species and blanks, and an analysis engine for implementing common statistical methods with camera trap data (e.g., estimation of species’ activity patterns, occupancy, and diversity indices) (Ahumada et al. 2019). Beyond serving as an interface for image processing and data analysis, WI was also conceived as a data repository for hosting camera trap data collected worldwide; images and associated metadata stored in WI can be downloaded by the public after sensitive content (e.g., images with people or endangered species) has been removed and once an embargo time (maximum 48 months) provided by data providers has passed (Ahumada et al. 2019). WI provides comprehensive guides for navigating the platform and tutorials showing step-by-step usage of the system’s features. This documentation can be found here: https://www.wildlifeinsights.org/get-started. Additionally, WI provides references describing how their AI models work along with a table listing species used for model training and performance metrics for each species, which users can consult here: https://www.wildlifeinsights.org/about-wildlife-insights-ai. We synthesize some of the key navigation steps and illustrate how one can evaluate performance of built-in AI models. Specifically, we provide code for comparing human classifications with model predictions for a subset of your images. This comparison will be useful to better understand how AI models are likely to perform with your particular images and whether AI may be able to provide accurate enough classification for some of your species. Before we get started: if you plan to compare model predictions with human classifications, you should download computer vision predictions right after uploading images to the WI platform (see Section 3.7). It is important that you have a record of the WI predictions before you do any processing on your data. 3.1 Set-up Create an account here https://app.wildlifeinsights.org/join In WI, you can structure your data hierarchically. Below, we provide an example structure from our camera trapping project in Colombia: Organization: University of Minnesota Initiative: Wildlife monitoring in South America using camera traps. Project: Large mammals Colombia. Subprojects: Jul2019-Jan2020 Deployment Jan2020-Jul2020 Deployment Location: We use alphanumerical codes to name each camera trap location, with letters representing different areas within our study area (Figure 3.1). Each location has its own geographical coordinates. Deployment: Information for deployments include temporal record (start and end dates) of a camera trap survey within a particular location. In our example, for our location “A08” a camera trap was deployed from 2019-07-10 to 2020-01-08 (Figure 3.2). This data structure allows you to manage multiple collaborators and data sets collected by different organizations and teams but associated with a single purpose (Initiatives 2021). For example, an “Initiative” allows you to share a project between different organizations (Initiatives 2021) which facilitates data management and labor distribution. Hierarchical data storage also might help to organize and filter subsets of images or data. For example “Projects” can contain “Subprojects” that might represent groups of deployments and/or locations. See the Glossary page for more terms found in WI https://www.wildlifeinsights.org/get-started/glossary. Figure 3.1: Locations of camera traps mapped in the Wildlife Insights platform along with location names and geographical coordinates. Figure 3.2: Deployments showing “Start date” and “End date” for each camera location. 3.2 Upload/format data You can upload already labeled images (e.g., for storing and managing images in the cloud) or unlabeled images to be processed. For labeled images, you will need to (re)format images’ metadata using the WI batch upload templates and transfer images from a public URL (e.g., Google Drive) or directly to the Google Cloud Platform. See https://www.wildlifeinsights.org/get-started/upload/bulk-data-uploads For unlabeled images, you can upload images via the WI platform. Images will then be stored in the Google Cloud Platform and displayed in the user’s project. See https://www.wildlifeinsights.org/get-started/upload/upload-new-data 3.3 Upload/enter metadata Metadata, such as the time and date each image was taken and the filename are automatically read once unlabeled images are uploaded. Figure 3.3: Image metadata displayed after uploading an image to Wildlife Insights online platform. You can provide additional metadata, including coordinates for the camera trap or other features associated with camera deployment (e.g., dates, camera height, settings, use of bait, etc.). This information can be entered manually for each deployment or you can use a CSV file formatted using the WI template for a bulk deployment upload. See the deployments guide https://www.wildlifeinsights.org/get-started/manage-metadata/deployments. 3.4 Processing images - AI module While images are uploaded, they will be processed by WI’s AI model. Computer vision predictions will be available in the WI platform as soon as your images are uploaded. Additionally, after images are uploaded, you can download the output (see download tab in the upper right corner of Figure 3.4). The output will include a CSV file with the AI predictions (See Section 3.6 for more information on WI output); you will receive an email with a link for downloading the output. Again, to facilitate evaluation of AI performance, we recommend downloading this CSV file before you do any other manipulations in the WI platform. Your project in WI will include the uploaded images in the “Identify” tab. Species predictions will be shown along with their confidence values associated. You will be able to verify if these predictions are correct, after which they will be moved to the “Catalogued” tab (Figure 3.4). Figure 3.4: WI processing module after uploading images. 3.5 Post-AI image processing WI provides a platform with multiple tools for reviewing images, allowing users to verify AI output. In addition, users can: Sort images by “Date taken”, “Upload date” or “Last modification” (the latter only for “Catalogued” images) (Figure 3.4). Filter images by categories such as Subprojects, Deployments, Species, Status (e.g., Blank or Not blank) or Photos (e.g., Highlighted images for quick access or Not highlighted) (Figure 3.4). Edit under- or over-exposed images by adjusting brightness, contrast and saturation. Edit identifications using bulk actions by selecting and entering information for multiple images at a time (e.g., for 100 or 200 images). Group images within a Burst defined by a time frame to perform bulk actions. Manage collaborations for data processing by assigning different roles with different levels of data access (e.g., project owner, editor, contributor, tagger, viewer). 3.6 Using AI output All the above mentioned processing tools can be used to review and verify AI output presented in the “Identify” tab. You can approve computer vision predictions or edit them in the processing module. You can also include additional identifications if more than one animal (of the same or different species) is present in the image and add other identifying information (e.g., sex, age, markings for each individual) or other remarks (e.g., comments or observations) that may be useful. When you download the resulting output, you will receive four different CSV files that capture data related to your cameras, their deployments, and your projects: cameras.csv: contains metadata related to the cameras, including camera_id, make, model, serial_number and year purchased. deployments.csv: contains deployment dates, geographical coordinates, details of camera trap placement and camera settings. images.csv: includes classifications and features recorded for each image. projects.csv: includes project details such as project objectives, licenses for metadata and images, and information about the sampling design used for camera trap deployment. You can quickly inspect AI results in the Summary tab (upper left of Figure 3.4). You will see a map with your camera locations and a summary of the species in your data set (Figure 3.5). Figure 3.5: WI summary of images by type and identified species. 3.7 Assessing AI performance AI model performance for classification of camera trap images is still highly variable, both across study sites and species (Tabak et al. 2018). Thus, it is extremely important to evaluate model performance with your data set. Before reviewing all of your images, you can classify a subset of images and compare these classifications with AI predictions. This step will allow you to determine how well the model is working for various species of interest and also to determine if there are particular species or locations where model performance is particularly poor. Below, we demonstrate a step-by-step workflow for how to get WI output into R, join computer and human vision identifications, and estimate model performance metrics for each species. Throughout, we will use the purrr package in R (Henry and Wickham 2020; R Core Team 2021) to repeatedly apply the same function to objects in a list or column in a nested data frame efficiently and without the need for writing loops. Readers unfamiliar with purrr syntax, may want to view one or more of the tutorials, below, or make use of the purrr cheat sheet. http://www.rebeccabarter.com/blog/2019-08-19_purrr/ https://www.r-bloggers.com/2020/05/one-stop-tutorial-on-purrr-package-in-r/ https://jennybc.github.io/purrr-tutorial/index.html Right after uploading images to the WI platform and before doing any image processing (i.e., identification), download the WI output. WI’s computer vision predictions will be contained in the images.csv file (Figure 3.6). Save that file as images_cv.csv. Figure 3.6: Predictions provided by computer vision. Use the WI processing module to verify a subset of your images (e.g., ~100,000) and either accept the computer vision prediction as correct or edit the prediction with the correct species label. The identified_by column will change according to the new identifier (Figure 3.7). Figure 3.7: Identifications after verification by an expert. Once you finish identifying a subset of your images, download the data from WI and change the name of the images.csv file to images_hv.csv. Create a data folder to store your two CSV files images_cv.csv and images_hv.csv that refer to classifications of computer and human vision, respectively. We provide an example of both files with the repository associated with this guide, named images_cv_jan2020_raw.csv and images_hv_jan2020_raw.csv. Process the two data files using the R code provided in the sections below. 3.7.1 Reading in data, introduction to the Purrr package Before comparing computer and human vision we need to do some data cleaning. This cleaning includes removing duplicated uploads to the WI platform and making sure to keep a single record for each image, as WI creates multiple rows when more than one animal (or object) is identified in an image. First, we load required libraries. library(tidyverse) # for data wrangling and visualization, includes dplyr and purrr library(here) # to allow use of relative paths library(DT) # for viewing data tables Next, we tell R the path (i.e., directory name) that holds our files. We will use the here package (Müller 2017) to tell R that our files live in the “./data/wi” directory. You may, alternatively, type in the full path to the file folder or a relative path from the root directory if you are using a project in RStudio. # Create filefolder&#39;s path. This should point to the folder name # where you stored your CSV files downloaded from WI filesfolder &lt;- here(&quot;data&quot;, &quot;wi&quot;) filesfolder ## [1] &quot;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/wi&quot; Next, we use the dir function to list the files contained in the “filesfolder” directory. files &lt;- dir(filesfolder, pattern = &quot;*.csv&quot;) files ## [1] &quot;images_cv_jan2020_raw.csv.zip&quot; &quot;images_hv_jan2020_raw.csv.zip&quot; We then use the map function in the purrr package to read in all of the files and store them in a list object named mycsv. The first argument to map is a list (here, files) which is “piped in” using %&gt;% from the magrittr package (Bache and Wickham 2020). Pipes (%&gt;%) provide a way to execute a sequence of data operations, organized so that the operations can be read from left to right (e.g., “Take this set of files and then read them in using read_csv”). The second argument to map is a function, in this case read_csv, to be applied to the list. The map function iterates over the two files stored in the files object, reads in the data files and then stores them in a new list named mycsv. We use ~ to refer to our function and use .x to refer to the list object that is passed to the function as an additional argument. # Read both CSV files mycsv &lt;- files %&gt;% map(~ read_csv(file.path(filesfolder, .x))) mycsv ## [[1]] ## # A tibble: 104,235 × 25 ## project_id deployment_id image_id filename is_blank identified_by wi_taxon_id ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2004246 None 84c573d… A03/041… 1 Computer vis… f1856211-c… ## 2 2004246 None 2754d55… M04/040… 1 Computer vis… f1856211-c… ## 3 2004246 None 2a8aec6… M01/062… 0 Computer vis… 1f689929-8… ## 4 2004246 None 1c75bdc… N29/011… 1 Computer vis… f1856211-c… ## 5 2004246 None e39d3ae… M04/021… 0 Computer vis… f2efdae9-e… ## 6 2004246 None acfa3bc… A07/021… 0 Computer vis… 3d80f1d6-b… ## 7 2004246 None ad80a12… M01/062… 0 Computer vis… f2d233e3-8… ## 8 2004246 None 702a38f… N05/062… 0 Computer vis… 988b8b2d-6… ## 9 2004246 None 473b772… A06/030… 0 Computer vis… 837e12c2-f… ## 10 2004246 None 6534fac… M05/020… 0 Computer vis… 627c919c-2… ## # … with 104,225 more rows, and 18 more variables: class &lt;chr&gt;, order &lt;chr&gt;, ## # family &lt;chr&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, common_name &lt;chr&gt;, ## # uncertainty &lt;lgl&gt;, timestamp &lt;dttm&gt;, number_of_objects &lt;dbl&gt;, age &lt;lgl&gt;, ## # sex &lt;lgl&gt;, animal_recognizable &lt;lgl&gt;, individual_id &lt;lgl&gt;, ## # individual_animal_notes &lt;lgl&gt;, highlighted &lt;lgl&gt;, markings &lt;lgl&gt;, ## # cv_confidence &lt;dbl&gt;, license &lt;chr&gt; ## ## [[2]] ## # A tibble: 112,425 × 26 ## X1 project_id filename image_id is_blank identified_by wi_taxon_id class ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2000949 N25/0331… 902b671f… 0 Paula Castib… 5acf63fb-3… Mamm… ## 2 3 2000949 N29/0331… e727dc42… 0 Paula Castib… 5acf63fb-3… Mamm… ## 3 4 2000949 A06/0602… db3c3213… 0 Paula Castib… d62cd43d-a… Mamm… ## 4 5 2000949 A02/0310… c7e33138… 1 Juliana Velez f1856211-c… Unkn… ## 5 6 2000949 A04/0418… 52f77e0c… 0 Paula Castib… dfea7f7a-a… Mamm… ## 6 7 2000949 A06/0302… 94a5b596… 0 Paula Castib… 6071f8bb-7… Aves ## 7 8 2000949 A06/0619… 063c0153… 0 Paula Castib… 575008c3-8… Mamm… ## 8 9 2000949 A06/0625… 4b6f9c4e… 0 Paula Castib… 39e5ea73-3… Mamm… ## 9 10 2000949 A07/0509… 15c4e9f4… 0 Paula Castib… 5109acb4-e… Mamm… ## 10 11 2000949 A09/0203… cb56ac06… 0 Paula Castib… dfea7f7a-a… Mamm… ## # … with 112,415 more rows, and 18 more variables: order &lt;chr&gt;, family &lt;chr&gt;, ## # genus &lt;chr&gt;, species &lt;chr&gt;, common_name &lt;chr&gt;, uncertainty &lt;lgl&gt;, ## # timestamp &lt;dttm&gt;, age &lt;chr&gt;, sex &lt;chr&gt;, animal_recognizable &lt;lgl&gt;, ## # individual_id &lt;lgl&gt;, number_of_objects &lt;dbl&gt;, ## # individual_animal_notes &lt;chr&gt;, highlighted &lt;lgl&gt;, markings &lt;chr&gt;, ## # cv_confidence &lt;lgl&gt;, license &lt;chr&gt;, sequence_id &lt;chr&gt; We remove images without a classification (i.e., common_name = “NA”), and with the “Human” and “Vehicle” classes, as these images predominately correspond to camera-set-up images. mycsv &lt;- mycsv %&gt;% map(~.x %&gt;% filter(!is.na(common_name)) %&gt;% filter(!common_name %in% &#39;Vehicle&#39; &amp; !common_name %in% &quot;Human&quot; &amp; !common_name %in% &quot;Human-Camera Trapper&quot;)) 3.7.2 Removing duplicated images Before discussing how to join the two data sets corresponding to computer and human vision, we need to remove duplicated rows that might result from accidentally uploading the same image more than once to the WI platform. We can identify these duplicated uploads as they contain the same information in all the columns except for image_id. Figure 3.8 shows two different examples of duplicated uploads. Figure 3.8: Duplicated image uploads in the images.csv WI output. We will use filename and timestamp as key columns to uniquely identify each image event. To remove duplicates in our data sets, we first use the group_by function to group rows using these key columns and store our grouped data frame as mycsv_grouped. We use the summarise function (Wickham et al. 2019) to create a new column uploads that contains the count of unique image_id values for each group of rows. Groups that have more than one image_id indicate duplicated uploads (Figure 3.8). We then filter by counts &gt; 1 to inspect duplicated images, remove duplicated rows in our data sets using the unique function (R Core Team 2021), and store the remaining data in the no_duplicates list object. mycsv_grouped &lt;- mycsv %&gt;% map(~.x %&gt;% group_by(filename, timestamp)) # Create data set with duplicated images duplicated_images &lt;- mycsv_grouped %&gt;% map(~.x %&gt;% summarise(uploads = length(unique(image_id))) %&gt;% # count unique ID&#39;s filter(uploads &gt; 1)) # for inspecting duplicated uploads # Create a data set without the duplicated images no_duplicates &lt;- mycsv_grouped %&gt;% map(~.x %&gt;% filter(image_id == unique(image_id)[1])) 3.7.3 Images with multiple observations of the same species WI creates extra rows when you identify more than one animal per image. Thus, it is likely that your two CSV files will differ in number of rows. After removing duplicated records, we need to drop extra rows in the human vision data frame that result from identifying more than one animal of the same species in the same image. As an example, Figure 3.9 displays records for both a juvenile and adult Southern tamandua detected in the same image. Figure 3.9: Highlighted rows represent detections of one juvenile and an adult of the Southern Tamandua in the same image. There are many variables that we will not need when evaluating AI performance. We use the select function to only keep the variables of interest: filename: image filename. timestamp: time of a camera trigger. image_id: WI identifier for images uploaded to the platform. common_name: species’ common name labeled either by computer or human vision. cv_confidence: confidence value associated with the computer vision label. We regroup the data by filename, adding common_name. We then apply the distinct function to reduce multiple records of the same species in the same image to a single record. If the same filename has multiple rows corresponding to a species and a “Blank” label, we only keep the species classification. no_duplicates &lt;- no_duplicates %&gt;% map(~.x %&gt;% select(filename, timestamp, image_id, common_name, cv_confidence)) hv &lt;- no_duplicates %&gt;% pluck(2) %&gt;% group_by(filename, common_name) %&gt;% distinct(filename, common_name, .keep_all = TRUE) %&gt;% # keep single record for same filename and common_name group_by(filename) %&gt;% mutate(sp_num = length(unique(common_name))) %&gt;% group_by(filename) %&gt;% filter(!(common_name == &quot;Blank&quot; &amp; sp_num &gt; 1)) # for groups with num_classes &gt; 1, remove blanks We will use the human vision data set contained in the hv object as the ground truth when evaluating model performance for all platforms reviewed in this GitBook. This object has been saved as images_hv_jan2020.csv using the following code. write_csv(hv %&gt;% select(-cv_confidence), file = here(&quot;data&quot;, &quot;md&quot;, &quot;images_hv_jan2020.csv&quot;)) write_csv(hv %&gt;% select(-cv_confidence), file = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images_hv_jan2020.csv&quot;)) For a filename with multiple predictions of the same common_name in computer vision, we only keep the record with the highest confidence value using the top_n function. We also create a sp_num column, which can then be used to identify records with more than one species in an image sp_num &gt; 1. cv &lt;- no_duplicates %&gt;% pluck(1) %&gt;% group_by(filename, common_name) %&gt;% top_n(1, cv_confidence) %&gt;% # keep highest value of same class group_by(filename, common_name) %&gt;% distinct(filename, common_name, cv_confidence, .keep_all = TRUE) # for filenames with the same confidence value, keep a single record cv &lt;- cv %&gt;% group_by(filename) %&gt;% mutate(sp_num = length(unique(common_name))) %&gt;% group_by(filename) %&gt;% filter(!(common_name == &quot;Blank&quot; &amp; sp_num &gt; 1)) %&gt;% # for groups with num_classes &gt; 1, remove blanks mutate(sp_num = length(unique(common_name))) # re-estimate sp_num with no blanks 3.7.4 Merging computer and human vision data sets Now that we removed duplicated uploads and kept a single record for multiple animals of the same species in an image, we can use various “joins” (Wickham et al. 2019) to merge computer and human vision together so that we can evaluate the accuracy of WI. First, however, we will eliminate any images that were not processed by both humans and AI. # Determine which images have been viewed by both methods ind1 &lt;- cv$filename %in% hv$filename # in both ind2 &lt;- hv$filename %in% cv$filename # in both cv &lt;- cv[ind1,] # eliminate images not processed by hv hv &lt;- hv[ind2,] # eliminate images not processed by cv # Number of photos eliminated sum(ind1 != TRUE) # in cv but not in hv ## [1] 1796 sum(ind2 != TRUE) # in hv but not in cv ## [1] 620 Now, we can use: an inner_join with filename and common_name to determine images that have correct predictions (i.e., images with the same class assigned by computer and human vision) an anti_join with filename and common_name to determine which records in the human vision data set have incorrect predictions from computer vision. an anti_join with filename and common_name to determine which records in the computer vision data set have incorrect predictions. We assume the classifications from human vision to be correct and distinguish them from WI predictions. The WI predictions will be correct if they match a class assigned by human vision for a particular record and incorrect if the classes assigned by the two visions differ. # correct predictions matched &lt;- cv %&gt;% inner_join(hv, by = c(&quot;filename&quot;, &quot;common_name&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% mutate(class_hv = common_name) %&gt;% mutate(class_cv = common_name) # incorrect predictions in hv set hv_only &lt;- hv %&gt;% anti_join(cv, by = c(&quot;filename&quot;, &quot;common_name&quot;), suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) %&gt;% rename(class_hv = common_name) %&gt;% rename(multiple_det_hv = sp_num) # incorrect predictions in cv set cv_only&lt;- cv %&gt;% anti_join(hv, by = c(&quot;filename&quot;, &quot;common_name&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% rename(class_cv = common_name) %&gt;% rename(multiple_det_cv = sp_num) We then use left_join to merge the predictions from the cv_only (computer vision) data set onto the records from the hv_only (human vision) data set. hv_mismatch &lt;- hv_only %&gt;% left_join(cv_only, by = &quot;filename&quot;, suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) We combine the matched and mismatched data sets. Then, we set a 0.65 confidence threshold to assign WI predictions and evaluate model performance. both_visions &lt;- rbind(matched, hv_mismatch) both_visions_65 &lt;- both_visions both_visions_65$class_cv[both_visions_65$cv_confidence_cv &lt; 0.65] &lt;- &quot;No CV Result&quot; 3.7.5 Confusion matrix and performance measures Using the both_visions_65 data frame, we can estimate a confusion matrix using the confusionMatrix function from the caret package (Kuhn 2021). The confusionMatrix function requires a data argument of a table with predicted and observed classes, both as factors and with the same levels. We use the factor function (R Core Team 2021) to convert class names into factor classes. We specify mode = &quot;prec_recall&quot; when calling the confusionMatrix function (Kuhn 2021) to estimate the precision and recall for the WI classifications. library(caret) # to inspect model performance library(ggplot2) # to plot results all_class &lt;- union(both_visions_65$class_cv, both_visions_65$class_hv) cm_wi &lt;- table(factor(both_visions_65$class_cv, all_class), factor(both_visions_65$class_hv, all_class)) # table(pred, truth) cm_wi &lt;- confusionMatrix(cm_wi, mode=&quot;prec_recall&quot;) We then group the data by class_cv andclass_hv and count the number of observations using n() inside summarise. Then, we filter classes with at least 20 records and use the intersect function to get the pool of classes shared in the output of computer and human vision. Finally, we keep records containing classifications at the species level, but the confusion matrix and model performance metrics can also be estimated for higher taxonomic levels. class_num_cv &lt;- both_visions_65 %&gt;% group_by(class_cv) %&gt;% summarise(n = n()) %&gt;% filter(n &gt;= 20) %&gt;% pull(&quot;class_cv&quot;) %&gt;% unique() class_num_hv &lt;- both_visions_65 %&gt;% group_by(class_hv) %&gt;% summarise(n = n()) %&gt;% filter(n &gt;= 20) %&gt;% pull(&quot;class_hv&quot;) %&gt;% unique() (class_num &lt;- intersect(class_num_cv, class_num_hv)) ## [1] &quot;Bird&quot; &quot;Blank&quot; &quot;Cervidae Family&quot; &quot;Collared Peccary&quot; ## [5] &quot;Dasypus Species&quot; &quot;Domestic Horse&quot; &quot;Possum Family&quot; &quot;Puma&quot; ## [9] &quot;Rodent&quot; Collared peccary, domestic horse and puma are the species shared in the output of computer and human vision, and with at least 20 records in each data set. sp_num &lt;- class_num[class_num %in% c(&quot;Collared Peccary&quot;, &quot;Domestic Horse&quot;, &quot;Puma&quot;)] Now we can use the confusion matrix to estimate model performance metrics including accuracy, precision, recall and F1 score (See Chapter 1 for metrics description). (overall_accuracy &lt;- cm_wi %&gt;% pluck(&quot;overall&quot;, &quot;Accuracy&quot;) %&gt;% round(., 2)) ## [1] 0.23 classes_metrics &lt;- cm_wi %&gt;% pluck(&quot;byClass&quot;) %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% rownames_to_column() %&gt;% rename(species = rowname) %&gt;% mutate(species = str_remove(species, pattern = &quot;Class: &quot;)) %&gt;% filter(species %in% sp_num) %&gt;% mutate(across(is.numeric, ~round(., 2))) Table 3.1: Model performance metrics for species shared by computer and human vision, and with at least 20 records in each data set. We used a confidence threshold of 0.65 for determining the classifications. 3.7.6 Confidence thresholds Finally, we define a function that allows us to inspect how precision and recall change when different confidence thresholds are used for assigning a prediction made by computer vision. Our function will assign a “No CV Result” label whenever the confidence for a computer vision prediction is below a user-specified confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall for each species change with the confidence threshold and identify a threshold that balances precision and recall for the different species. threshold_for_metrics &lt;- function(conf_threshold = 0.7) { df &lt;- both_visions df$class_cv[df$cv_confidence_cv &lt; conf_threshold] &lt;- &quot;No CV Result&quot; all_class &lt;- union(df$class_cv, df$class_hv) newtable &lt;- table(factor(df$class_cv, all_class), factor(df$class_hv, all_class)) # table(pred, truth) cm &lt;- confusionMatrix(newtable, mode=&quot;prec_recall&quot;) # metrics classes_metrics &lt;- cm %&gt;% # get confusion matrix pluck(&quot;byClass&quot;) %&gt;% # get metrics by class as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% rownames_to_column() %&gt;% rename(class = rowname) %&gt;% mutate(conf_threshold = conf_threshold) classes_metrics$class &lt;- str_remove(string = classes_metrics$class, pattern = &quot;Class: &quot;) return(classes_metrics) # return a data frame with metrics for every class } Let’s look at the distribution of confidence values associated with each species using the geom_bar function (Wickham et al. 2018). # Plot confidence values both_visions %&gt;% filter(class_cv %in% sp_num &amp; class_hv %in% sp_num) %&gt;% ggplot(aes(cv_confidence_cv, group = class_cv, colour = class_cv)) + geom_bar() + facet_wrap(~class_cv, scales = &quot;free&quot;) + labs(y = &quot;Empirical distribution&quot;, x = &quot;Confidence values&quot;)+ theme(legend.position=&quot;bottom&quot;) + scale_color_viridis_d() Figure 3.10: Distribution of confidence values associated with species shared by computer and human vision, and with at least 20 records in each data set. We can see that the distribution of confidence values is left skewed for all the species with most records having high confidence values, suggesting that the AI predictions are presumed to be correct most of the time. Let’s estimate model performance metrics for confidence values ranging from 0.1 to 0.99 using the map_df function (Henry and Wickham 2020) . The map_df function (Henry and Wickham 2020) returns a data frame object. Once we get a data frame of model performance metrics for a range of confidence values, we can plot the results using the ggplot2 package (Wickham et al. 2018). conf_vector = seq(0.1, 0.99, length=100) metrics_all_confs &lt;- map_df(conf_vector, threshold_for_metrics) # Plot Precision and Recall prec_rec_wi &lt;- metrics_all_confs %&gt;% mutate_if(is.numeric, round, digits = 2) %&gt;% filter(class %in% sp_num) %&gt;% rename(Species = class, `Confidence threshold` = conf_threshold) %&gt;% ggplot(aes(x = Recall, y = Precision, group = Species, colour = Species)) + geom_point(aes(size = `Confidence threshold`)) + scale_size(range = c(0.01,3)) + geom_line() + scale_color_viridis_d() + xlim(0, 1) + ylim(0, 1) prec_rec_wi Figure 3.11: Precision and recall for different confidence thresholds for species shared by computer and human vision, and with at least 20 records in each data set. Point sizes represent the confidence thresholds used to accept AI predictions. We see that as we increase the confidence threshold, precision usually increases and recall decreases (Figure 3.11). Ideally, we would like AI to have high precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the species is truly present when AI identifies the species as being present in an image (Chapter 1). If AI suffers from low precision, then we may have to manually review photos that AI tags as having species present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a species in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing images of species that are truly present. To remedy this problem, we would need to review images where AI says the species is absent in order to reduce false negatives. For this particular data set, AI would be most useful for classifying domestic horses and pumas. For example, we can be confident that WI is correctly labeling domestic horse, with a precision of 89% at a 68% recall, using a 0.65 confidence threshold (Table 3.1). We can also be confident that WI is correctly labeling pumas with a precision of 90%, and it will help us to spot 53% of the actual records for the species, also using a 0.65 confidence threshold. Yet, the low to moderate recall for these species suggests that we will still have to review all images to identify other records of animals that are not detected by WI. 3.8 Conclusions We have seen how to set up a project, upload and process camera trap photos using WI’s platform. WI provides an infrastructure for data organization and management, features for image review and annotation, and facilitates collaborative work through a multi-user environment. WI also serves as an image repository and provides advanced reporting and analytical capabilities. We provided R code for evaluating per-species model performance. Although we found that WI was able to classify some species with high levels of precision, recall values were typically low; thus, experts will still need to review images to find the animals missed by computer vision. Projects incorporating AI in their workflows would benefit from examining a range of confidence thresholds to determine how they impact precision and recall for classification of different species. References "],["megadetector.html", "Chapter 4 MegaDetector 4.1 Upload/format data 4.2 Upload/enter metadata 4.3 Process images - AI module 4.4 Image processing with Timelapse 4.5 Using AI output 4.6 Assessing AI performance 4.7 Conclusions", " Chapter 4 MegaDetector The MegaDetector (MD) model detects objects (animals, people, and vehicles) in camera trap images, and can be used to separate images into categories of “Empty”, “Animal”, “Human” and “Vehicles” (Beery, Morris, and Yang 2019). MD can be run 1. locally by running Python code at the command line, 2. assisted by MegaDetector developers after data are transferred, 3. using a batch processing Application Programming Interface (recommended for large data sets with millions of images), or 4. through Camelot or Zooniverse. To choose one of these options, you can contact MD developers at cameratraps@lila.science to see which approach is right for you; this decision will depend on the number of images that you have, whether data can be shared with third parties, and how comfortable you are running Python code (Beery, Morris, and Yang 2019). We will describe a workflow for a user that will receive assistance to run the model and integrate MD output with the image processing software Timelapse (Greenberg, Godin, and Whittington 2019). Sections 4.1-4.3 go through the steps required to run MD and interpret its output. Sections 4.4 and 4.5 focus on how MD output can be processed in Timelapse and how AI output can be used to accelerate image review and identification by humans. Specifically, AI identifications with high confidence values (i.e., likely to be correct) can be filtered, subset, and subsequently identified by humans using batch operations (see Section 4.5). Section 4.6 focuses on assessing the performance of MD (MDv4.1) by comparing human labels with the ones provided by MD across different confidence thresholds used for assigning predictions. MD will continue to be updated and should lead to better model performance over time. A recent MegaDetector version release (MDv5) incorporated additional training data to improve detection of the “vehicle” class, artificial objects (e.g., bait stations), and particular taxa (rodents, reptiles and small birds). 4.1 Upload/format data Requirement of data upload will depend on how the model is run. For local model runs, no data need to be transferred. For assisted model runs, MD developers will provide specific instructions for data transfer. Users using Camelot and Zooniverse must import data directly to each of these platforms. 4.2 Upload/enter metadata When using MD, there is no need to enter metadata before processing your data using AI. You will enter metadata during the post-AI image processing stage, when MD output is integrated with other image processing tools such as Timelapse, Camelot or Zooniverse. 4.3 Process images - AI module For assisted model runs, the MD developers will share with you the model output (a JSON file) or a CSV file if requested. This file will contain image filenames (e.g., “A01/01020108.JPG”, where A01 represents the subfolder of a camera trap location and 01020108.JPG is the image filename), maximum confidence values associated with all detections within an image, and the maximum confidence value for each category (animal, person, or vehicle) (Figure 4.1). Figure 4.1: Predictions provided by Megadetector. 4.4 Image processing with Timelapse Timelapse is a software for image processing that can be run in all versions of Microsoft Windows or other operating systems running Windows emulators. Timelapse facilitates image review and annotation of species names or other features of interest (e.g., animals’ sex, behavior, condition, etc.). To run Timelapse, your images must be organized in subfolders (e.g., by study area and camera trap location; Figure 4.2). Timelapse software, video tutorials and a detailed manual with all the software features explained can be downloaded here: http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.Download2 Figure 4.2: Example of image organization in folders and subfolders to load them in Timelapse (Greenberg 2020). Before using Timelapse, you will need to create a data schema (i.e., a template with a .tdb file extension) that identifies the data fields that will be recorded when processing your camera trap data (e.g., species name, sex, behavior, etc.). This step can be accomplished by using the Timelapse Template Editor that is downloaded along with Timelapse. The Template Editor allows you to create the .tdb file that will be read in Timelapse and will adjust the software image processing interface to capture the different data fields you specify in the .tdb file (Greenberg 2020). Once photos are imported to Timelapse, you can review them using different processing tools provided by the software. The Timelapse manual (Greenberg 2020) and documentation about the software design (Greenberg, Godin, and Whittington 2019) describe in detail the processing tools available in the software, which include features to: Magnify images and explore image difference extraction tools (e.g., to identify small animals that are otherwise difficult to spot). Select multiple images quickly from small thumbnails and classify them all at once. Automate data entry via metadata extraction (time/date, filename, temperature sensed by the camera) and copy annotated information to other images. Sort images by date, species or any other annotated feature, to easily review and verify this information. 4.5 Using AI output To integrate MD output with Timelapse, you will need to have the same folder and subfolder structure (Figure 4.2) that you used when running MD. Using the same folder structure allows Timelapse to match each image with MD output using relative paths associated with each filename. In Timelapse, you must activate the option for working with image recognition data and import computer vision results stored in the JSON file (Greenberg 2020). You will see bounding boxes that can facilitate image review once the automatic image recognition is activated. To use AI results to accelerate image review, you can filter MD output categories detected with high confidence levels that are likely correct. To do that, you must provide a confidence range to accept predictions made by computer vision (Figure 4.3). For example, if you choose “Empty” as the detected entity and a high confidence range (e.g, from 0.65 to 1.00), your data set will be filtered to display images predicted as “Empty” by MD and likely to be correct. Figure 4.3: When activating the use of AI detections, users can filter which images are displayed depending on the range of confidence values provided (Greenberg 2020). After subsetting images to be displayed depending on their confidence values, you can easily inspect images in the overview mode in Timelapse and select multiple images at a time and assign an “Empty” category to them if the AI output is correct (Figure 4.4). If AI output is not correct, you can edit those classifications. Figure 4.4: Timelapse overview mode where users can perform bulk actions (e.g., selection of multiple images) to accept or edit AI predictions (Greenberg 2020). After, processing images with Timelapse, you can export the data as a CSV File. This file (TimelapseData.csv) will contain all the data entries that you specified in the template (Figure 4.5). Figure 4.5: Output contained in a CSV file exported from Timelapse software (Greenberg 2020). 4.6 Assessing AI performance Before exploring model performance with your data, let us recapitulate. MD runs AI models and outputs broad categories of predictions for images (in a JSON file). These AI predictions can be integrated with Timelapse to further process camera trap images (i.e., identifying images with the help of AI output). However, if you have previously classified images (e.g., identified using your software or platform of preference), you can explore MD performance before integrating AI results using Timelapse. To evaluate MD performance, you will need to 1) classify a subset of your images and export the classification results to a CSV file (containing at minimum, the image filename, camera location and species name), and 2) obtain MD output in CSV format. You can then follow the steps below to evaluate the performance of MD’s AI model. We also discuss how one can select an appropriate confidence threshold for filtering images to accelerate the image review process (e.g., by focusing only on images that likely contain an animal). 4.6.1 Reading in data, introduction to the Purrr package Below, we demonstrate a step-by-step workflow for how to get MD output into R, join computer and human vision identifications, and estimate model performance metrics for each class. Throughout, we will use the purrr package in R (Henry and Wickham 2020; R Core Team 2021) to repeatedly apply the same function to objects in a list or column in a nested data frame efficiently and without the need for writing loops. Readers unfamiliar with purrr syntax, may want to view one or more of the tutorials, below, or make use of the purrr cheat sheet. http://www.rebeccabarter.com/blog/2019-08-19_purrr/ https://www.r-bloggers.com/2020/05/one-stop-tutorial-on-purrr-package-in-r/ https://jennybc.github.io/purrr-tutorial/index.html Once you finish the identification of a subset of your images using your platform of preference, export your classifications as a CSV file and name it as images_hv.csv. The other CSV file containing the MD results can be named as images_cv.csv. Create a data folder to store your two CSV files images_cv.csv and images_hv.csv that refer to classifications of computer and human vision, respectively (note, we provide an example of both files with the repository associated with this guide named images_cv_jan2020.csv and images_hv_jan2020.csv). Process the two data files using the R code provided below. First, we load required libraries and open files. library(tidyverse) # for data wrangling and visualization, includes dplyr and purrr library(here) # to allow use of relative paths Next, we tell R the path (i.e., directory name) that holds our files. We will use the here package (Müller 2017) to tell R that our files live in the “./data/md” directory. You may, alternatively, type in the full path to the file folder or a relative path from the root directory if you are using a project in RStudio. # Create filefolder&#39;s path. This should point to the folder name # where you stored your CSV files, one with classifications of some of your images # and the other with MD output filesfolder &lt;- here(&quot;data&quot;, &quot;md&quot;) filesfolder ## [1] &quot;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/md&quot; # List your files contained in the filesfolder directory. This code will # list all your CSV files (i.e., images_cv.csv and images_hv.csv) files &lt;- dir(filesfolder, pattern = &quot;*.csv&quot;) files ## [1] &quot;images_cv_jan2020.csv&quot; &quot;images_hv_jan2020.csv&quot; We then use the map function in the purrr library to read in all of the files and store them in a list object named mycsv. The first argument to map is a list (here, files) which is “piped in” using %&gt;% from the magrittr package (Bache and Wickham 2020). Pipes (%&gt;%) provide a way to execute a sequence of data operations, organized so that the operations can be read from left to right (e.g., “Take the set of files and then read them in using read_csv”). The second argument to map is a function, in this case read_csv, to be applied to the list. The map function iterates over the two files stored in the files object, reads in the data files and then stores them in a new list named mycsv. We use ~ to refer to our function and use .x to refer to the list object that is passed to the function as an additional argument. # Read both CSV files mycsv &lt;- files %&gt;% map(~read_csv(file.path(filesfolder, .x))) # Inspect how the data sets look like mycsv ## [[1]] ## # A tibble: 112,247 × 7 ## image_path max_confidence detections max_conf_animal max_conf_person ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 jan2020/A01/01080001.JPG 0.998 NA NA 0.998 ## 2 jan2020/A01/01080002.JPG 0.97 NA NA 0.97 ## 3 jan2020/A01/01080003.JPG 0.996 NA NA 0.996 ## 4 jan2020/A01/01080004.JPG 0.985 NA 0.202 0.985 ## 5 jan2020/A01/01080005.JPG 0.939 NA 0.209 0.939 ## 6 jan2020/A01/01080006.JPG 0.996 NA NA 0.996 ## 7 jan2020/A01/01080007.JPG 0.999 NA NA 0.999 ## 8 jan2020/A01/01080008.JPG 0.997 NA NA 0.997 ## 9 jan2020/A01/01080009.JPG 0.914 NA 0.428 0.914 ## 10 jan2020/A01/01080010.JPG 0.992 NA NA 0.992 ## # … with 112,237 more rows, and 2 more variables: max_conf_group &lt;dbl&gt;, ## # max_conf_vehicle &lt;lgl&gt; ## ## [[2]] ## # A tibble: 103,053 × 5 ## filename timestamp image_id common_name sp_num ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 N25/03310082.JPG 2020-03-31 14:28:14 902b671f-58b9-4cb… Collared Pecc… 1 ## 2 N29/03310288.JPG 2020-03-31 06:49:17 e727dc42-5ebb-46a… Collared Pecc… 1 ## 3 A06/06020479.JPG 2020-06-02 08:12:17 db3c3213-5ad9-4bf… Black Agouti 1 ## 4 A02/03100387.JPG 2020-03-10 06:58:27 c7e33138-08ac-461… Unknown speci… 1 ## 5 A04/04180034.JPG 2020-04-18 05:37:56 52f77e0c-7023-408… Bos Species 1 ## 6 A06/03020343.JPG 2020-03-02 12:00:45 94a5b596-1685-40a… Spix&#39;s Guan 1 ## 7 A06/06190148.JPG 2020-06-19 11:40:33 063c0153-d8fd-46b… White-lipped … 1 ## 8 A06/06250315.JPG 2020-06-25 07:08:56 4b6f9c4e-e8bb-4a9… Lowland Tapir 1 ## 9 A07/05090248.JPG 2020-05-09 15:05:11 15c4e9f4-7a68-4bd… Domestic Horse 1 ## 10 A09/02030454.JPG 2020-02-03 10:34:41 cb56ac06-6520-47c… Bos Species 1 ## # … with 103,043 more rows 4.6.2 Format computer vision data set Columns of interest in the the MD data set include: filename: contains camera location and filename (e.g., A01/01010461.JPG). max_confidence: contains the maximum confidence value found for a detection in an image. max_conf_animal, max_conf_person, max_conf_group, max_conf_vehicle: each of these columns contain a confidence value associated with a prediction by computer vision for the different classes. We begin by creating a max_conf_blank variable, which we will use to identify blank images. We assign a value of 1 to this variable whenever the observation has missing values for all of the other max_conf variables. cv_wide &lt;- mycsv %&gt;% pluck(1) %&gt;% #extract the computer vision images_cv.csv file rename(filename = image_path) %&gt;% group_by(filename) %&gt;% mutate(max_conf_blank = case_when(sum(is.na(max_conf_animal), is.na(max_conf_person), is.na(max_conf_group), is.na(max_conf_vehicle)) == 4 ~ 1)) %&gt;% ungroup() MD can detect and classify more than one object in an image, and each object will be assigned its own confidence value. We want to keep each of these classifications in a separate row. To do that, we first change the data set from “wide” to “long” format using the pivot_longer function (Wickham and Henry 2018). This function will create two new variables, name and value, that will hold the classification (“Human”, “Animal”, “Vehicle”, “Blank”) and associated confidence values, respectively. cv_long &lt;- cv_wide %&gt;% select(-detections) %&gt;% pivot_longer(c(&quot;max_conf_animal&quot;, &quot;max_conf_person&quot;, &quot;max_conf_group&quot;, &quot;max_conf_vehicle&quot;, &quot;max_conf_blank&quot;)) cv_long ## # A tibble: 561,235 × 4 ## filename max_confidence name value ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 jan2020/A01/01080001.JPG 0.998 max_conf_animal NA ## 2 jan2020/A01/01080001.JPG 0.998 max_conf_person 0.998 ## 3 jan2020/A01/01080001.JPG 0.998 max_conf_group NA ## 4 jan2020/A01/01080001.JPG 0.998 max_conf_vehicle NA ## 5 jan2020/A01/01080001.JPG 0.998 max_conf_blank NA ## 6 jan2020/A01/01080002.JPG 0.97 max_conf_animal NA ## 7 jan2020/A01/01080002.JPG 0.97 max_conf_person 0.97 ## 8 jan2020/A01/01080002.JPG 0.97 max_conf_group NA ## 9 jan2020/A01/01080002.JPG 0.97 max_conf_vehicle NA ## 10 jan2020/A01/01080002.JPG 0.97 max_conf_blank NA ## # … with 561,225 more rows Next, we drop all rows where the confidence value is equal to “NA” and rename our classification variable to class. Additionally, we remove images with the “Vehicle” and “Human” classes. To simplify things, we also change the “Group” label to “Animal”. We do this for 3 reasons: 1) we found that there were very few “Group” classifications in our data set; 2) these predictions were not very accurate; and 3) MD will most often be used to separate blank images from those that have at least one animal present, and thus, the “Group” label is not all that informative. cv &lt;- cv_long %&gt;% filter(is.na(value) != TRUE) %&gt;% mutate(name = replace(name, name == &quot;max_conf_blank&quot; , &quot;Blank&quot;), name = replace(name, name == &quot;max_conf_animal&quot;, &quot;Animal&quot;), name = replace(name, name == &quot;max_conf_person&quot;, &quot;Human&quot;), name = replace(name, name == &quot;max_conf_group&quot;, &quot;Animal&quot;), name = replace(name, name == &quot;max_conf_vehicle&quot;, &quot;Vehicle&quot;)) %&gt;% rename(class = name) %&gt;% filter(!class %in% &quot;Human&quot; &amp; !class %in% &quot;Vehicle&quot;) Changing the “Group” label to “Animal” results in some images having multiple predictions of “Animal” for the same image. In these cases, we use the top_n function to select the record with the highest confidence value (Wickham et al. 2019). cv &lt;- cv %&gt;% group_by(filename, class) %&gt;% top_n(1, value) %&gt;% # keep highest value of same class group_by(filename, class) %&gt;% distinct(filename, class, value, .keep_all = TRUE) # for filenames with the same confidence value, keep a single record Comparing output from human and computer vision also requires that the classification variables have the same levels attribute. We first create a vector, all_levels, containing the names of the classes. Then, we use the factor function (R Core Team 2021) to convert common names into factor classes and assign the levels to the class column. # Create a vector with levels of predicted categories all_levels &lt;- c(&quot;Animal&quot;, &quot;Blank&quot;) # Assign the levels attribute to the class column cv$class &lt;- factor(as.character(cv$class, levels = all_levels)) Lastly, we add a variable that will indicate if there are multiple detections within the same image. cv &lt;- cv %&gt;% group_by(filename) %&gt;% mutate(multiple_det = n() &gt; 1) %&gt;% select(filename, class, value, multiple_det) %&gt;% ungroup() 4.6.3 Format human vision data set The human vision data set (ìmages_hv_jan2020.csv) was previously cleaned to remove duplicated records and to summarize multiple rows that reference animals of the same species identified in the same image (see Chapter 3) for details about these steps). We begin by inspecting all the species names contained in the human vision data set using the unique function (R Core Team 2021). We will eventually need to create a variable that can be compared to MD output (i.e., broad classes identifying “Blank” and “Animal”). hv_sp &lt;- mycsv %&gt;% pluck(2) # select human vision data frame # check all species present in the data set unique(hv_sp$common_name) %&gt;% sort() ## [1] &quot;Alouatta Species&quot; &quot;Amazonian Motmot&quot; ## [3] &quot;Ants&quot; &quot;Bird&quot; ## [5] &quot;Black Agouti&quot; &quot;Blank&quot; ## [7] &quot;Bos Species&quot; &quot;Bush Dog&quot; ## [9] &quot;Caprimulgidae Family&quot; &quot;Capybara&quot; ## [11] &quot;Cervidae Family&quot; &quot;Collared Peccary&quot; ## [13] &quot;Common Green Iguana&quot; &quot;Crab-eating Fox&quot; ## [15] &quot;Crestless Curassow&quot; &quot;Dasypus Species&quot; ## [17] &quot;Domestic Dog&quot; &quot;Domestic Horse&quot; ## [19] &quot;Fasciated Tiger-heron&quot; &quot;Giant Anteater&quot; ## [21] &quot;Giant Armadillo&quot; &quot;Giant Otter&quot; ## [23] &quot;Insect&quot; &quot;Jaguar&quot; ## [25] &quot;Jaguarundi&quot; &quot;Lizards and Snakes&quot; ## [27] &quot;Lowland Tapir&quot; &quot;Mammal&quot; ## [29] &quot;Margarita Island Capuchin&quot; &quot;Margay&quot; ## [31] &quot;Northern Amazon Red Squirrel&quot; &quot;Ocelot&quot; ## [33] &quot;Ornate Tití Monkey&quot; &quot;Pecari Species&quot; ## [35] &quot;Possum Family&quot; &quot;Puma&quot; ## [37] &quot;Red Brocket&quot; &quot;Rodent&quot; ## [39] &quot;Saimiri Species&quot; &quot;South American Coati&quot; ## [41] &quot;Southern Tamandua&quot; &quot;Spix&#39;s Guan&quot; ## [43] &quot;Spotted Paca&quot; &quot;Tayra&quot; ## [45] &quot;Turkey Vulture&quot; &quot;Turtle Order&quot; ## [47] &quot;Unknown species&quot; &quot;Weasel Family&quot; ## [49] &quot;White-lipped Peccary&quot; &quot;White-tailed Deer&quot; MD output represents filenames as “deployment/camera_location/image_filename”, based on folder structure. We add the deployment name to the filename variable in human vision output so that it matches the format of the MD data set. These steps will allow us to later join our human and computer vision data sets using the common variable, filename. hv_sp &lt;- hv_sp %&gt;% mutate(filename = paste0(&quot;jan2020/&quot;, filename)) We use the case_when function (Wickham et al. 2019) to implement multiple conditional statements to create a variable, class_hv, containing the classes found in MD output. We select the columns of interest, including a newly created variable (multiple_det) that will keep track of images that have two or more species present in the same image, each recorded in a different row. For the multiple detections, we keep a single row using the distinct function (Wickham et al. 2019). We also assign a levels attribute to the class variable in human vision so that it is comparable to the class variable in the computer vision data set. hv &lt;- hv_sp %&gt;% group_by(filename, timestamp) %&gt;% mutate(multiple_det = n() &gt; 1) %&gt;% # create multiple detections column (TRUE/FALSE). mutate(class = case_when(common_name == &quot;Blank&quot; ~ common_name, TRUE ~ &quot;Animal&quot;)) %&gt;% select(filename, timestamp, class, multiple_det) hv &lt;- hv %&gt;% group_by(filename, class) %&gt;% distinct(filename, class, .keep_all = TRUE) %&gt;% ungroup() # Assign the levels attribute to the class column hv$class &lt;- factor(as.character(hv$class), levels = all_levels) 4.6.4 Merging computer and human vision data sets Now that we have the same format for both human and computer vision data sets, we can use various “joins” (Wickham et al. 2019) to merge the two data sets together so that we can evaluate the accuracy of MD. First, however, we will eliminate any images that were not processed by both humans and AI. # Determine which images have been viewed by both methods ind1 &lt;- cv$filename %in% hv$filename # in both ind2 &lt;- hv$filename %in% cv$filename # in both cv &lt;- cv[ind1,] # eliminate images not processed by hv hv &lt;- hv[ind2,] # eliminate images not processed by cv # Number of photos eliminated sum(ind1 != TRUE) # in cv but not in hv ## [1] 5397 sum(ind2 != TRUE) # in hv but not in cv ## [1] 1168 Now, we can use: an inner_join with filename and class to determine images that have correct predictions (i.e., images with the same class assigned by computer and human vision) an anti_join with filename and class to determine which records in the human vision data set have incorrect predictions from computer vision. an anti_join with filename and class to determine which records in the computer vision data set have incorrect predictions. We assume the classifications from human vision to be correct and distinguish them from MD predictions. The MD predictions will be correct if they match a class assigned by human vision for a particular record and incorrect if the classes assigned by the two visions differ. # correct predictions matched &lt;- cv %&gt;% inner_join(y = hv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% mutate(class_hv = class) %&gt;% rename(class_cv = class) %&gt;% select(filename, value, class_cv, class_hv, multiple_det_cv, multiple_det_hv) # incorrect predictions in hv set hv_only &lt;- hv %&gt;% anti_join(y = cv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) %&gt;% rename(class_hv = class) %&gt;% rename(multiple_det_hv = multiple_det) # incorrect predictions in cv set cv_only&lt;- cv %&gt;% anti_join(y = hv, by = c(&quot;filename&quot;, &quot;class&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% rename(class_cv = class) We then use left_join to merge the predictions from the cv_only (computer vision) data set onto the records from the hv_only (human vision) data set. hv_mismatch &lt;- hv_only %&gt;% left_join(cv_only, by = &quot;filename&quot;) %&gt;% rename(multiple_det_cv = multiple_det) %&gt;% select(filename, value, class_cv, class_hv, multiple_det_cv, multiple_det_hv) We then check for any computer vision records that are not yet accounted for in our data sets containing records with correct or incorrect predictions, i.e., matched and hv_mismatch, respectively. cv_others &lt;- cv_only[cv_only$filename %in% matched$filename != TRUE,] cv_others &lt;- cv_only[cv_only$filename %in% hv_mismatch$filename != TRUE,] table(cv_others$multiple_det) ## &lt; table of extent 0 &gt; table(cv_others$multiple_det) ## &lt; table of extent 0 &gt; Then, we select only the variables we need, combine the matched and mismatched data sets, and again make sure that the classifications have the same factor levels attribute. matched &lt;- matched %&gt;% select(filename, value, class_cv, class_hv) hv_mismatch &lt;- hv_mismatch %&gt;% select(filename, value, class_cv, class_hv) both_visions &lt;- rbind(matched, hv_mismatch) both_visions$class_cv &lt;- factor(as.character(both_visions$class_cv), levels = all_levels) both_visions$class_hv &lt;- factor(as.character(both_visions$class_hv), levels = all_levels) 4.6.5 Confusion matrix and performance measures Finally, we can proceed with estimating a confusion matrix and various AI performance measures using the confusionMatrix function from the caret package (Kuhn 2021) and specifying a 0.65 confidence threshold to accept MD predictions. We can then plot the confusion matrix using ggplot2 (Wickham et al. 2018). The confusionMatrix function requires a data argument for predicted classes and a reference for true classifications, both as factor classes and with the same factor levels. We specify mode = &quot;prec_recall&quot; when calling the confusionMatrix function (Kuhn 2021) to generate estimates of precision and recall. library(caret) # to inspect model performance library(ggplot2) # Estimate confusion matrix both_visions_0.65 &lt;- both_visions both_visions_0.65$class_cv[both_visions_0.65$value &lt; 0.65] &lt;- &quot;Blank&quot; cm_md_0.65 &lt;- confusionMatrix(data = both_visions_0.65$class_cv, reference = both_visions_0.65$class_hv, mode = &quot;prec_recall&quot;) # Plot confusion matrix plot_cm_md_0.65 &lt;- cm_md_0.65 %&gt;% pluck(&quot;table&quot;) %&gt;% as.data.frame() %&gt;% rename(Frequency = Freq) %&gt;% ggplot(aes(y=Prediction, x=Reference, fill=Frequency)) + geom_raster() + scale_fill_gradient(low = &quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + theme(axis.text.x = element_text(angle = 90)) + geom_text(aes(label = Frequency), size = 3) #set size to 3 plot_cm_md_0.65 Figure 4.6: Confusion matrix applied to classifications from MegaDetector using a confidence threshold of 0.65. Now we can use the confusion matrix to estimate metrics of model performance including accuracy, precision, recall and F1 score (See Chapter 1 for metrics description). (overall_accuracy &lt;- cm_md_0.65 %&gt;% pluck(&quot;overall&quot;, &quot;Accuracy&quot;) %&gt;% round(., 2)) ## [1] 0.93 classes_metrics_md_0.65 &lt;- cm_md_0.65 %&gt;% pluck(&quot;byClass&quot;) %&gt;% t() %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% mutate(across(where(is.numeric), ~round(., 2))) classes_metrics_md_0.65 &lt;- classes_metrics_md_0.65 %&gt;% mutate(Class = &quot;Animal&quot;, .before = Precision) Table 4.1: Model performance metrics for the detection of animals in images using MegaDetector and a 0.65 confidence threshold. We see that the model is really good at picking up animals, with a precision of 98% at a 93% recall. Thus, we expect that 93% of the animals present in our images will be picked up by MD. Further, only 2% of the images flagged as having an animal will not. We can also inspect model performance for a range of confidence thresholds as we demonstrate in the next section. 4.6.6 Confidence thresholds Lets begin by looking at the confidence values associated with each MD classification using the geom_bar function (Wickham et al. 2018). Before plotting, we filter the both_visions data frame to remove blanks as they do not have associated confidence values. # Plot confidence values both_visions %&gt;% filter(class_cv != &quot;Blank&quot;) %&gt;% ggplot(aes(value, group = class_cv, colour = class_cv)) + geom_bar() + facet_wrap(~class_cv, scales = &quot;free&quot;) + labs(y = &quot;Empirical distribution&quot;, x = &quot;Confidence values&quot;) + scale_color_viridis_d() Figure 4.7: Distribution of confidence values for animals predicted by MegaDetector. We see that the distribution of confidence values for “Animal” classifications is left skewed with most records having high confidence values suggesting that the AI prediction is presumed to be correct most of the time. To inspect how precision and recall change when different confidence thresholds are established for assigning a class predicted by computer vision, we define a function that will calculate these metrics for a user-defined confidence threshold. This function will assign a “Blank” label whenever the confidence for a computer vision prediction is below a particular confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall change with the confidence threshold and choose a threshold that balances these two performance metrics. threshold_for_metrics &lt;- function(conf_threshold = 0.7) { tmp &lt;- both_visions tmp$class_cv[tmp$value &lt; conf_threshold] &lt;- &quot;Blank&quot; cm &lt;- confusionMatrix(data = tmp$class_cv, reference = tmp$class_hv, mode = &quot;prec_recall&quot;) classes_metrics &lt;- cm %&gt;% # get confusion matrix pluck(&quot;byClass&quot;) %&gt;% # get metrics by class t() %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% mutate(conf_threshold = conf_threshold) return(classes_metrics) # return a data frame with metrics for every class } Let’s estimate model performance metrics for confidence values ranging from 0.1 to 0.99 using the map_df function (Henry and Wickham 2020) . The map_df function (Henry and Wickham 2020) returns a data frame object. Once we get a data frame of model performance metrics for a range of confidence values, we can plot the results using the ggplot2 package (Wickham et al. 2018). conf_vector = seq(0.1, 0.99, length=100) metrics_all_confs &lt;- map_df(conf_vector, threshold_for_metrics) prec_rec_md &lt;- metrics_all_confs %&gt;% rename(`Confidence threshold` = conf_threshold) %&gt;% ggplot(aes(x = Recall, y = Precision)) + geom_point(aes(size = `Confidence threshold`, color = &quot;#440154FF&quot;)) + scale_size(range = c(0.1,3)) + labs(x = &quot;Recall&quot;, y = &quot;Precision&quot;)+ geom_line(color = &quot;#440154FF&quot;) + scale_color_manual(values = &quot;#440154FF&quot;, name = &quot;Class&quot;, labels = &quot;Animal&quot;) + xlim(0, 1) + ylim(0, 1) prec_rec_md Figure 4.8: Precision and recall for different confidence thresholds for the animal class predicted by MegaDetector. We see that as we increase the confidence threshold, precision increases and recall decreases for the “Animal” class (Figure 4.8). Ideally, we would like to choose a confidence threshold that maximizes both precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the class is truly present when AI identifies the class as being present in an image (Chapter 1). If AI suffers from low precision, then we may have to manually review photos that AI tags as having a class present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a class in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing a class that is truly present. To remedy this problem, we would need to review images where AI says the class is absent in order to reduce false negatives. Let’s say that we were willing to miss only 3% of the animals present. In this case, we could pick the confidence threshold that maximizes precision under the constraint that recall does not fall below 97%. conf_meets &lt;- metrics_all_confs %&gt;% mutate(across(where(is.numeric), ~round(., 2))) %&gt;% filter(Recall &gt;= 0.97) Table 4.2: Performance metrics using a confidence threshold that maximizes precision for a 97% recall. Precision Recall F1 conf_threshold 0.95 0.97 0.96 0.1 We see that we can obtain a precision to 95% (while keeping recall = 97%) by using a confidence threshold of 0.1. Thus, if we integrate MD output with Timelapse, filtering using a confidence threshold of 0.1, we expect to capture 97% of Animals that are truly present and 95% of the flagged images should actually include one or more animals. Let’s examine the confusion matrix using this threshold. threshold &lt;- conf_meets[which.max(conf_meets$Precision), 4] conf_red &lt;- both_visions conf_red$class_cv[conf_red$value &lt; threshold] &lt;- &quot;Blank&quot; cm_md_th &lt;- confusionMatrix(data = conf_red$class_cv, reference = conf_red$class_hv, mode = &quot;prec_recall&quot;) plot_cm_md_th &lt;- cm_md_th %&gt;% pluck(&quot;table&quot;) %&gt;% as.data.frame() %&gt;% rename(Frequency = Freq) %&gt;% ggplot(aes(y=Prediction, x=Reference, fill=Frequency)) + geom_raster() + scale_fill_gradient(low = &quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + theme(axis.text.x = element_text(angle = 90)) + geom_text(aes(label = Frequency), size = 3) library(patchwork) plot_cm_md_0.65 &lt;- plot_cm_md_0.65 + theme(legend.position = &quot;None&quot;) plot_cm_md_th &lt;- plot_cm_md_th + theme(legend.position = &quot;None&quot;) (plots_cms_md &lt;- plot_cm_md_0.65 + plot_cm_md_th + plot_annotation(tag_levels = &#39;A&#39;)) Figure 4.9: Confusion matrices applied to classifications from MegaDetector using a confidence threshold of 0.65 (A) and 0.1 (B). Comparing the confusion matrix with our original (using a 0.65 confidence threshold; Figure 4.9), we see that we have decreased the false negatives using a confidence threshold of 0.1 (i.e., cases where MD suggests a blank image but an animal is present; last row, first column) but increased the number of false positives (i.e., cases where MD suggests an animal is present, but the image is blank; first row, second column). 4.7 Conclusions We have seen how to use MD and how to integrate its output with Timelapse for using AI while processing camera trap data. Additionally, we illustrated how to evaluate MD performance by comparing true classifications with computer predictions. We found that MD has a high performance to detect animals in images. Thus, the human labor required to review photos can be reliably focused on images with the “Animal” class predictions. We also showed how users can explore MD performance by comparing confusion matrices estimated using different confidence thresholds. This comparison will be useful to understand the trade-off between precision and recall, and help users choose a confidence threshold that maximizes these metrics using their particular data sets. References "],["mlwic2-machine-learning-for-wildlife-image-classification.html", "Chapter 5 MLWIC2: Machine Learning for Wildlife Image Classification 5.1 Set-up 5.2 Upload/format data 5.3 Process images - AI module 5.4 Assessing AI performance 5.5 Model training 5.6 Classify using a trained model 5.7 Conclusions", " Chapter 5 MLWIC2: Machine Learning for Wildlife Image Classification MLWIC2 is an R package that allows you either to use trained models for identifying species from North America (using the “species_model”) or to identify if an image is empty or if it contains an animal (using the “empty_animal” model) (Tabak et al. 2020). The model was trained using images from 10 states across the United States but was also tested in out-of sample data sets obtaining a 91% accuracy for species from Canada and 91% - 94% for classifying empty images on samples from different continents (Tabak et al. 2020). Documentation for using MLWIC2 and the list of species that the model identifies can be found in the GitHub repository https://github.com/mikeyEcology/MLWIC2. In this chapter we illustrate the use of the package and data preparation for model training. 5.1 Set-up First, you will need to install R software, Anaconda Navigator (https://docs.anaconda.com/anaconda/navigator/), Python (3.5, 3.6 or 3.7) and Rtools (only for Windows computers). Then you will have to install TensorFlow 1.14 and find the path location of Python on your computer. You can find more installation details in the GitHub repository (https://github.com/mikeyEcology/MLWIC2), as well as an example with installation steps for Windows users (https://github.com/mikeyEcology/MLWIC_examples/blob/master/MLWIC_Windows_Set_up.md). Make sure to install the required versions listed above. Mac users can use the Terminal Application to install Python and TensorFlow. You can use the conda package (https://docs.conda.io/en/latest/) to create an environment that can host a specific version of Python and keep it separated from other packages or dependencies. In the Terminal type: conda create -n ecology python=3.5 conda activate ecology conda install -c conda-forge tensorflow=1.14 Once you complete the installation of Python and TensorFlow, use the command-line utility to find the location of Python. Windows users should type: where python. Mac users should type conda activate ecology and then which python. The output Python location will look something like this: /Users/julianavelez/opt/anaconda3/envs/ecology/bin/python In R, install the devtools package and MLWIC2 packages. Then, setup your environment using the setup function (Tabak et al. 2020) as shown below, making sure to change the python_loc argument to point to the output path provided in the previous step. You will only need to specify this setup once. # Uncomment and only run this code once # if (!require(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) # devtools::install_github(&quot;mikeyEcology/MLWIC2&quot;) # MLWIC2::setup(python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;) Next, download MLWIC2 helper files from here: https://drive.google.com/file/d/1VkIBdA-oIsQ_Y83y0OWL6Afw6S9AQAbh/view. Note, this folder is not included in the repository because of its large size. You must download it and store it in the data/mlwic2 directory. 5.2 Upload/format data Once the package is installed and the python location is setup, you can run a MLWIC2 model to obtain computer vision predictions of the species present in your images. To run MLWIC2 models, you should use the classify function (see Section 5.3), which requires arguments specifying the path (i.e., location on your computer) for the following three inputs: The images that will be classified. The filenames of your images. The location of the MLWIC2 helper files that contain the model information. To create the filenames (input 2 above), you will need to create an input file using the make_input function (Tabak et al. 2020). This will create a CSV file with two columns, one with the filenames and the other one with a list of class_ID’s required to use MLWIC2 for classifying images or training a model. When using the make_input function to create the CSV file, you can select different options depending whether or not you already have filenames and images classified (type ?make_input in R for more options). We will use option = 4 to find filenames associated with each photo using MLWIC2 and recursive = TRUE to specify that photos are in subfolders organized by camera location. We then read the output using the read_csv function (Wickham 2017). Let’s first load required libraries for reading data and to use MLWIC2 functions. library(tidyverse) # for data wrangling and visualization, includes dplyr library(MLWIC2) library(here) # to allow the use of relative paths while reading data When using the make_input function with your data, you must provide the paths indicating the location of your images (path_prefix) and the output directory where you want to store the output. The make_input function will output a file named image_labels.csv which we renamed as images_names_classify.csv and included it with the files associated with this repository. We provide a full illustration of the use of make_input and classify in Section 5.6 using a small image set included in the repository. # The code below won&#39;t be executed. For using it you must replace: # - the &quot;path_prefix&quot; with the path to the directory holding your images # - the &quot;output_directory&quot; with the directory where you want to store the output # from running MLWIC2 make_input(path_prefix = &quot;/Volumes/ct-data/CH1/jan2020&quot;, recursive = TRUE, option = 4, find_file_names = TRUE, output_dir = here(&quot;data&quot;, &quot;mlwic2&quot;)) file.rename(from = &quot;data/mlwic2/image_labels.csv&quot;, to = &quot;data/mlwic2/images_names_classify.csv&quot;) We then read in this file containing the image filenames and look at the first few records. We will use the here package (Müller 2017) to tell R that our file lives in the ./data/mlwic2 directory and specify the name of our file images_names_classify.csv. # inspect output from the make_input function image_labels &lt;- read_csv(here(&quot;data&quot;,&quot;mlwic2&quot;,&quot;images_names_classify.csv&quot;)) head(image_labels) ## # A tibble: 6 × 2 ## `A01/01080001.JPG` `0` ## &lt;chr&gt; &lt;dbl&gt; ## 1 A01/01080002.JPG 0 ## 2 A01/01080003.JPG 0 ## 3 A01/01080004.JPG 0 ## 4 A01/01080005.JPG 0 ## 5 A01/01080006.JPG 0 ## 6 A01/01080007.JPG 0 5.3 Process images - AI module Once you have the CSV file with image filenames (images_names_classify.csv), you can proceed to run the MLWIC2 models. It is possible to use parallel computing to run the models more efficiently. This will require specifying a number of cores that you want to use while running the models. To do that, first you need to know how many cores (i.e., processors in your computer) are available, which you can determine using the detectCores function in the parallel package (R Core Team 2021). library(parallel) detectCores() # detects number of cores in your computer ## [1] 4 If you have 4 cores, then you can use 3 cores for running the MLWIC2 model using the classify function (Tabak et al. 2020) and the num_cores argument. This assures that you leave one core for your computer to perform other tasks. Other arguments for the classify function include: path_prefix: absolute path of the location of your camera trap photos. data_info: absolute path of the images_names_classify.csv file (i.e., the output from the make_input function). model_dir: absolute path of MLWIC2 helper files folder. python_loc: absolute path of Python on your computer. os: specification of your operating system (here “Mac”). make_output: use TRUE for a ready-to-read CSV output. To complete this step, you would need to change the file paths to indicate where the files are located on your computer. Note that this step took approximately 5 hours to process 110,457 photos (run on a macOS Mojave with a 2.5 GHz Intel Core i5 Processor and 8GB 1600 MHz DDR3 of RAM). classify(path_prefix = &quot;/Volumes/ct-data/CH1/jan2020/&quot;, data_info = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images_names_classify.csv&quot;), model_dir = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_helper_files/&quot;), python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;, # absolute path of Python on your computer os = &quot;Mac&quot;, # specify your operating system; &quot;Windows&quot; or &quot;Mac&quot; make_output = TRUE, num_cores = 3 ) 5.4 Assessing AI performance As with other AI platforms, it is recommended to evaluate model performance with your particular data set before using an AI model to classify all your images. This involves classifying a subset of your photos and comparing those classifications with predictions provided by MLWIC2 (i.e., we will compare human vs. computer vision). Let’s start by formatting the human vision data set. 5.4.1 Format human vision data set To read the file containing the human vision classifications for a subset of images, we tell R the path (i.e., directory name) that holds our file. Again, we use the here package (Müller 2017) to tell R that our file lives in the ./data/mlwic2 directory and specify the name of our file images_hv_jan2020.csv. We then read our file using the read_csv function (Wickham 2017). human_vision &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images_hv_jan2020.csv&quot;)) The human vision data set (images_hv_jan2020.csv) was previously cleaned to remove duplicated records and to summarize multiple rows that reference animals of the same species identified in the same image (see Chapter 3) for details about these steps). MLWIC2 provides predictions for species from North America (see list of predicted species here https://github.com/mikeyEcology/MLWIC2/blob/master/speciesID.csv). However, you can also use the MLWIC2 empty_animal model for distinguishing blanks from images containing an animal. For our example with species from South America, we will use the species_model as we want to evaluate model performance for predicting species present both in North and South America (e.g., white-tailed deer and puma). We created two CSV files containing the taxonomy for species in computer and human vision. These taxonomy files have the same format for species name (scientific notation stored in the species column). We join the taxonomy files to our records to be able to evaluate model performance based on common scientific notation. We join the taxonomy to each visions using the common_name column. tax_ori &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;tax_hv_ori.csv&quot;)) hv_tax &lt;- human_vision %&gt;% left_join(tax_ori, by = &quot;common_name&quot;) As the “Blank” category is represented as “NA” in the species column, we make sure it is correctly labeled in that column. hv &lt;- hv_tax %&gt;% mutate(species = case_when(common_name == &#39;Blank&#39; ~ &#39;Blank&#39;, TRUE ~ species)) 5.4.2 Format computer vision data set Let’s read the MLWIC2 output and the taxonomy file, and remove unwanted patterns in the filenames using str_remove and gsub. MLWIC2 outputs columns with filename and the top-5 predictions along with their associated confidence values. Filenames are represented as camera_location/image_filename. Note that we have moved the model output to the data/mlwic2 folder from its original location (in the same folder as the MLWIC2_helper_files that you previously downloaded). We join the taxonomy file to MLWIC2 output, remove the “Vehicle” and the “Human” records, and assign the “Blank” label in the species column for empty images. mlwic2 &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_output_ori.csv&quot;)) tax_mlwic2 &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;tax_mlwic2.csv&quot;)) mlwic2 &lt;- mlwic2 %&gt;% select(fileName, guess1, confidence1) %&gt;% rename(common_name = guess1, filename = fileName, confidence_cv = confidence1) %&gt;% mutate(filename = str_remove(filename, pattern = &quot;&#39;&quot;)) %&gt;% mutate(filename = str_remove(filename, &quot;b/Volumes/ct-data/CH1/jan2020/&quot;), filename = str_remove(filename, pattern = &quot;&#39;&quot;)) %&gt;% mutate(filename = gsub(x=filename, pattern =&quot;[0-9]*EK113/&quot;, replacement = &quot;&quot;)) mlwic2_tax &lt;- mlwic2 %&gt;% left_join(tax_mlwic2, by = &quot;common_name&quot;) mlwic2_tax &lt;- mlwic2_tax %&gt;% filter(!common_name %in% 25 &amp; !common_name %in% 11) %&gt;% mutate(species = case_when(common_name == 27 ~ &#39;Blank&#39;, TRUE ~ species)) We check if there are multiple predictions for the same filename in computer vision (see Chapter 3 for details about performing this step with human vision). For a filename with multiple predictions of the same species, we only keep the record with the highest confidence value using the top_n function. We also create a sp_num column, which can then be used to identify records with more than one species in an image sp_num &gt; 1. cv_seq &lt;- mlwic2_tax %&gt;% group_by(filename, species) %&gt;% top_n(1, confidence_cv) %&gt;% # keep highest value of same class group_by(filename, species) %&gt;% distinct(filename, species, confidence_cv, .keep_all = TRUE) # for filenames with the same confidence value, keep a single record cv &lt;- cv_seq %&gt;% group_by(filename) %&gt;% mutate(sp_num = length(unique(common_name))) %&gt;% group_by(filename) %&gt;% filter(!(common_name == &quot;Blank&quot; &amp; sp_num &gt; 1)) %&gt;% # for groups with num_classes &gt; 1, remove blanks mutate(sp_num = length(unique(common_name))) # re-estimate sp_num with no blanks 5.4.3 Merging computer and human vision data sets We can use various “joins” (Wickham et al. 2019) to merge computer and human vision together so that we can evaluate the accuracy of MLWIC2. First, however, we will eliminate any images that were not processed by both humans and AI. # Determine which images have been viewed by both methods ind1 &lt;- cv$filename %in% hv$filename # in both ind2 &lt;- hv$filename %in% cv$filename # in both cv &lt;- cv[ind1,] # eliminate images not processed by hv hv &lt;- hv[ind2,] # eliminate images not processed by cv # Number of photos eliminated sum(ind1 != TRUE) # in cv but not in hv ## [1] 4690 sum(ind2 != TRUE) # in hv but not in cv ## [1] 4612 Now, we can use: an inner_join with filename and species to determine images that have correct predictions (i.e., images with the same class assigned by computer and human vision) an anti_join with filename and species to determine which records in the human vision data set have incorrect predictions from computer vision. an anti_join with filename and species to determine which records in the computer vision data set have incorrect predictions. We assume the classifications from human vision to be correct and distinguish them from MLWIC2 predictions. The MLWIC2 predictions will be correct if they match a class assigned by human vision for a particular record and incorrect if the classes assigned by the two visions differ. hv &lt;- hv %&gt;% select(filename, species) cv &lt;- cv %&gt;% select(filename, species, confidence_cv) # correct predictions matched &lt;- cv %&gt;% inner_join(hv, by = c(&quot;filename&quot;, &quot;species&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% mutate(class_hv = species) %&gt;% mutate(class_cv = species) # incorrect predictions in hv set hv_only &lt;- hv %&gt;% anti_join(cv, by = c(&quot;filename&quot;, &quot;species&quot;), suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) %&gt;% rename(class_hv = species) # incorrect predictions in cv set cv_only&lt;- cv %&gt;% anti_join(hv, by = c(&quot;filename&quot;, &quot;species&quot;), suffix = c(&quot;_cv&quot;, &quot;_hv&quot;)) %&gt;% rename(class_cv = species) We then use left_join to merge the predictions from the cv_only (computer vision) data set onto the records from the hv_only (human vision) data set. hv_mismatch &lt;- hv_only %&gt;% left_join(cv_only, by = &quot;filename&quot;, suffix = c(&quot;_hv&quot;, &quot;_cv&quot;)) We combine the matched and mismatched data sets. Then, we set a 0.65 confidence threshold to assign MLWIC2 predictions and evaluate model performance. “NA” values in the species column correspond to categories representing higher taxonomic levels with no species classification available. both_visions &lt;- rbind(matched, hv_mismatch) both_visions &lt;- both_visions %&gt;% mutate(class_hv = case_when(is.na(class_hv) ~ &quot;Higher tax level&quot;, TRUE ~ class_hv)) %&gt;% mutate(class_cv = case_when(is.na(class_cv) ~ &quot;Higher tax level&quot;, TRUE ~ class_cv)) both_visions_65 &lt;- both_visions both_visions_65$class_cv[both_visions_65$confidence_cv &lt; 0.65] &lt;- &quot;No CV Result&quot; 5.4.4 Confusion matrix and performance measures Using the both_visions_65 data frame, we can estimate a confusion matrix using the confusionMatrix function from the caret package (Kuhn 2021). The confusionMatrix function requires a data argument of a table with predicted and observed classes, both as factors and with the same levels. We use the factor function (R Core Team 2021) to convert class names into factor classes. We specify mode = &quot;prec_recall&quot; when calling the confusionMatrix function (Kuhn 2021) to estimate the precision and recall for the MLWIC2 classifications. library(caret) # to inspect model performance library(ggplot2) # to plot results all_class &lt;- union(both_visions_65$class_cv, both_visions_65$class_hv) cm_mlwic2 &lt;- table(factor(both_visions_65$class_cv, all_class), factor(both_visions_65$class_hv, all_class)) # table(pred, truth) cm_mlwic2 &lt;- confusionMatrix(cm_mlwic2, mode=&quot;prec_recall&quot;) We then group the data by class_cv andclass_hv and count the number of observations using n() inside summarise. Then, we filter classes with at least 20 records and use the intersect function to get the pool of classes shared in the output of computer and human vision. Finally, we keep records containing classifications at the species level, but the confusion matrix and model performance metrics can also be estimated for higher taxonomic levels. class_num_cv &lt;- both_visions %&gt;% group_by(class_cv) %&gt;% summarise(n = n()) %&gt;% filter(n &gt;= 20) %&gt;% pull(&quot;class_cv&quot;) %&gt;% unique() class_num_hv &lt;- both_visions %&gt;% group_by(class_hv) %&gt;% summarise(n = n()) %&gt;% filter(n &gt;= 20) %&gt;% pull(&quot;class_hv&quot;) %&gt;% unique() (class_num &lt;- intersect(class_num_cv, class_num_hv)) ## [1] &quot;Blank&quot; &quot;Higher tax level&quot; &quot;Odocoileus virginianus&quot; ## [4] &quot;Puma concolor&quot; White-tailed deer and puma are the species shared in the output of computer and human vision, and with at least 20 records in each data set. sp_num &lt;- class_num[class_num %in% c(&quot;Puma concolor&quot;, &quot;Odocoileus virginianus&quot;)] Now we can use the confusion matrix to estimate model performance metrics including accuracy, precision, recall and F1 score (See Chapter 1 for metrics description). (overall_accuracy &lt;- cm_mlwic2 %&gt;% pluck(&quot;overall&quot;, &quot;Accuracy&quot;) %&gt;% round(., 2)) ## [1] 0.03 classes_metrics &lt;- cm_mlwic2 %&gt;% pluck(&quot;byClass&quot;) %&gt;% as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% rownames_to_column() %&gt;% rename(species = rowname) %&gt;% mutate(species = str_remove(species, pattern = &quot;Class: &quot;)) %&gt;% filter(species %in% sp_num) %&gt;% mutate(across(is.numeric, ~round(., 2))) Table 3.1: Model performance metrics for species shared by computer and human vision, and with at least 20 records in each data set. We used a confidence threshold of 0.65 for determining the classifications. 5.4.5 Confidence thresholds Finally, we define a function that allows us to inspect how precision and recall change when different confidence thresholds are used for assigning a prediction made by computer vision. Our function will assign a “No CV Result” label whenever the confidence for a computer vision prediction is below a user-specified confidence threshold. Higher thresholds should reduce the number of false positives but at the expense of more false negatives. We then estimate the same performance metrics for the specified confidence threshold. By repeating this process for several different thresholds, users can evaluate how precision and recall for each species change with the confidence threshold and identify a threshold that balances precision and recall for the different species. threshold_for_metrics &lt;- function(conf_threshold = 0.7) { df &lt;- both_visions df$class_cv[df$confidence_cv &lt; conf_threshold] &lt;- &quot;No CV Result&quot; all_class &lt;- union(df$class_cv, df$class_hv) newtable &lt;- table(factor(df$class_cv, all_class), factor(df$class_hv, all_class)) # table(pred, truth) cm &lt;- confusionMatrix(newtable, mode=&quot;prec_recall&quot;) # metrics classes_metrics &lt;- cm %&gt;% # get confusion matrix pluck(&quot;byClass&quot;) %&gt;% # get metrics by class as.data.frame() %&gt;% select(Precision, Recall, F1) %&gt;% rownames_to_column() %&gt;% rename(class = rowname) %&gt;% mutate(conf_threshold = conf_threshold) classes_metrics$class &lt;- str_remove(string = classes_metrics$class, pattern = &quot;Class: &quot;) return(classes_metrics) # return a data frame with metrics for every class } Let’s look at the distribution of confidence values associated with each species using the geom_bar function (Wickham et al. 2018). # Plot confidence values both_visions %&gt;% filter(class_cv %in% sp_num &amp; class_hv %in% sp_num) %&gt;% ggplot(aes(confidence_cv, group = class_cv, colour = class_cv)) + geom_bar() + facet_wrap(~class_cv, scales = &quot;free&quot;) + labs(y = &quot;Empirical distribution&quot;, x = &quot;Confidence values&quot;)+ theme(legend.position=&quot;bottom&quot;) + scale_color_viridis_d() Figure 5.1: Distribution of confidence values associated with species shared by computer and human vision, and with at least 20 records in each data set. We can see a uniform distribution for both species with records distributed along the full range of confidence values. Let’s estimate model performance metrics for confidence values ranging from 0.1 to 0.99 using the map_df function (Henry and Wickham 2020) . The map_df function (Henry and Wickham 2020) returns a data frame object. Once we get a data frame of model performance metrics for a range of confidence values, we can plot the results using the ggplot2 package (Wickham et al. 2018). conf_vector = seq(0.1, 0.99, length=100) metrics_all_confs &lt;- map_df(conf_vector, threshold_for_metrics) # Plot Precision and Recall prec_rec_mlwic2 &lt;- metrics_all_confs %&gt;% mutate_if(is.numeric, round, digits = 2) %&gt;% filter(class %in% sp_num) %&gt;% rename(Species = class, `Confidence threshold` = conf_threshold) %&gt;% ggplot(aes(x = Recall, y = Precision, group = Species, colour = Species)) + geom_point(aes(size = `Confidence threshold`)) + scale_size(range = c(0.01,3)) + geom_line() + scale_color_viridis_d() + xlim(0, 1) + ylim(0, 1) prec_rec_mlwic2 Figure 3.11: Precision and recall for different confidence thresholds for species shared by computer and human vision, and with at least 20 records in each data set. Point sizes represent the confidence thresholds used to accept AI predictions. We see that as we increase the confidence threshold, precision usually increases and recall decreases (Figure 3.11). Ideally, we would like AI to have high precision and recall, though the latter is likely to be more important in most cases. Remember that precision tells us the probability that the species is truly present when AI identifies the species as being present in an image (Chapter 1). If AI suffers from low precision, then we may have to manually review photos that AI tags as having species present in order to remove false positives. Recall, on the other hand, tells us how likely AI is to find a species in the image when it is truly present. If AI suffers from low recall, then it will miss many photos containing images of species that are truly present. To remedy this problem, we would need to review images where AI says the species is absent in order to reduce false negatives. Predictions from MLWIC2 present low recall for both species likely due to strong differences between the training and test data sets. However, MLWIC2 contains a training module where users can input manually classified images to train a new model and increase accuracy. 5.5 Model training For training a model, we also need to provide a CSV file containing image filenames. For illustration, we will get the filenames from a small set of images included in the repository (images/train folder), but you will want to train a model with at least 1,000 labeled images per species (Schneider et al. 2020). To get the filenames for those images we can use the make_input function (See Section 5.2); in the argument path_prefix you should provide the path of the directory containing the images. The make_input function will create the image_labels.csv file in the directory provided in the output_dir argument. We rename this file as images_names_train_temp.csv. # create CSV file with filenames make_input(path_prefix = here(&quot;data&quot;,&quot;mlwic2&quot;,&quot;images&quot;, &quot;train&quot;), option = 4, find_file_names = TRUE, output_dir = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;)) file.rename(from = &quot;data/mlwic2/training/image_labels.csv&quot;, to = &quot;data/mlwic2/training/images_names_train_temp.csv&quot;) Once we have the filenames for the training set, we add the corresponding human vision labels for each image using a left_join. Additionally, we recode our species names with numbers as required by the MLWIC2 package and save them as images_names_train.csv; these numbers must be consecutive and should start with 0 (Tabak et al. 2020). img_labs &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_names_train_temp.csv&quot;), col_names = c(&quot;filename&quot;, &quot;class&quot;)) hv_train &lt;- read_csv(here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_hv.csv&quot;)) imgs_train &lt;- img_labs %&gt;% left_join(hv_train, by = &quot;filename&quot;) %&gt;% select(filename, common_name) %&gt;% drop_na(common_name) names_nums &lt;- data.frame(common_name = unique(imgs_train$common_name), class_ID = seq(from = 0, to = 9)) imgs_train &lt;- imgs_train %&gt;% left_join(names_nums, by = &quot;common_name&quot;) %&gt;% ungroup() %&gt;% select(filename, class_ID) write_csv(imgs_train, file = &quot;data/mlwic2/training/images_names_train.csv&quot;, col_names = FALSE) We then use the train function to train a new model, where we need to specify arguments that were also used with the classify function (see Section 5.3); these include the path_prefix, model_dir, python_loc, os and num_cores. In the data_info argument we pass the images_names_train.csv file. We also need to specify the number of classes that we want to train the model to predict (e.g., these classes might differ from the 58 classes predicted when using MLWIC2’s built-in AI species model). We can use retrain = FALSE to train a model from scratch or retrain = TRUE if we want to retrain a pre-specified model using transfer learning.1 For more references on model training see https://github.com/mikeyEcology/MLWIC2. We specify 55 number of epochs (i.e., the number of times the learning algorithm will iterate on the training data set) (Tabak et al. 2020). Lastly, we specify the directory where we want to store the model using the log_dir_train argument; we use “SA” for “South America”. train(path_prefix = here(&quot;data&quot;,&quot;mlwic2&quot;, &quot;images&quot;, &quot;train&quot;), data_info = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_names_train.csv&quot;), # absolute path to the make_input output model_dir = here(&quot;data&quot;,&quot;mlwic2&quot;, &quot;MLWIC2_helper_files/&quot;), python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;, # absolute path of Python on your computer os = &quot;Mac&quot;, # specify your operating system; &quot;Windows&quot; or &quot;Mac&quot; num_cores = 3, num_classes = 22, # number of classes in your data set retrain = FALSE, num_epochs = 55, log_dir_train =&quot;SA&quot; ) 5.6 Classify using a trained model Finally you can run the model that you trained using a test image set. You should also get filenames for these images (renamed as images_names_test.csv) and pass it when running the model with the classify function. Model output will contain top-5 predictions for each image with their associated confidence values. Once you have this output, you can verify model performance as described along Section 5.4. make_input(path_prefix = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images&quot;, &quot;test&quot;), option = 4, find_file_names = TRUE, output_dir = here(&quot;data&quot;,&quot;mlwic2&quot;,&quot;training&quot;)) ## Your file is located at &#39;/Users/julianavelez/Documents/GitHub/Processing-Camera-Trap-Data-using-AI/data/mlwic2/training/image_labels.csv&#39;. file.rename(from = &quot;data/mlwic2/training/image_labels.csv&quot;, to = &quot;data/mlwic2/training/images_names_test.csv&quot;) ## [1] TRUE classify(path_prefix = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;images&quot;, &quot;test&quot;), # absolute path to test images directory data_info = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;training&quot;, &quot;images_names_test.csv&quot;), # absolute path to your image_labels.csv file model_dir = here(&quot;data&quot;, &quot;mlwic2&quot;, &quot;MLWIC2_helper_files&quot;), log_dir = &quot;SA&quot;, # model name python_loc = &quot;/Users/julianavelez/opt/anaconda3/envs/ecology/bin/&quot;, # absolute path of Python on your computer os = &quot;Mac&quot;, # specify your operating system; &quot;Windows&quot; or &quot;Mac&quot; make_output = TRUE, num_cores = 3, output_name = &quot;output_SA.csv&quot;, # name for model output num_classes = 22 ) 5.7 Conclusions We have seen how to set-up MLWIC2, prepare the required input files, and run its built-in species_model. Additionally, we illustrated how one can evaluate MLWIC2 performance by comparing true classifications with model predictions, for species found in North and South America. For our data set, MLWIC2 had low model performance, probably due to strong differences between the training and the test data. Thus, we would need to train a new model using the tools provided by the MLWIC2 package to improve model performance with our data. References "],["conservation-ai.html", "Chapter 6 Conservation AI 6.1 Set-up 6.2 Upload/format data 6.3 Image tagging 6.4 Process images - AI module 6.5 Conclusions", " Chapter 6 Conservation AI Conservation AI is a platform aimed at facilitating the use of AI to solve conservation problems. It is developed by research experts in Machine Learning, Computer Science and Conservation Biology from the Liverpool John Moores University and NVIDIA (a technology company that developed the NVIDIA CUDA®, a collection of libraries and tools for using AI; more information can be found here https://developer.nvidia.com/gpu-accelerated-libraries). Currently, Conservation AI can detect species from the United Kingdom, North America and from Sub-Saharan Africa. However, it also provides a user-friendly interface for creating a data set that can be used to train an AI model to detect species from your particular region. To create the training data set you will need to tag a subset of your images. This step requires creating a bounding box around animals in the images and assigning a species label. We illustrate the tagging process using the Conservation AI infrastructure. 6.1 Set-up Create an account here https://www.conservationai.co.uk/register/ Once you create an account, it needs to be activated by the Conservation AI team. To do that you can contact them using their message center https://www.conservationai.co.uk/contact/ After being approved, you will have a dashboard were you have sections for uploading your files, tagging images with the corresponding classification, and checking AI results (Figure 6.1). Figure 6.1: Conservation AI dashboard with sections for uploading images, tagging images and checking AI results. 6.2 Upload/format data The Conservation AI team will help you to set up a project, after which you will be able to upload images and begin the tagging process. You will need to submit a list of species contained in your images to the Conservation AI team, and they will create the tagging project for you. For categories that are not of interest or species that are difficult to identify, you can include in your list higher taxonomic levels (e.g., Class Aves, Order Rodentia). Additionally, you will have to share a subset of images that will be used for training; you can do that either using the “Upload Files” section (Figure 6.1) or using other transfer methods like Google Drive to share your images with the Conservation AI team. The first option is recommended as it allows users to maintain a more independent workflow for image upload; otherwise, you will have to request the Conservation AI team to upload the images for you, which might slow down your tagging process. Uploads are currently limited to 500 images per batch. 6.3 Image tagging In the tagging section, you can find a report of species that have been tagged within the Conservation AI platform and the number of tags for each species (Figure 6.2). Here you can check if your species of interest already contains tags from other users. Additionally, after sharing your species list with the Conservation AI team, you will find your species included in this list and see updates of the number of tags for each species as you advance in the tagging process. Figure 6.2: Report of species tagged within Conservation AI and number of tags per species. To begin image tagging, you need to select your project from the project list (Figure 6.3). Figure 6.3: List of projects that can be accessed to perform image tagging. Accessing your project will direct you to your tagging interface (Figure 6.4), where you will see images to be tagged at the left, an image viewer at the center, and a tag list at the right (this is the same list that you shared with the Conservation AI team). As you detect and tag species in an image, make sure you select the “Detection” tab in the top right. Figure 6.4: Tagging interface for drawing bounding boxes around animals detected in the images and for assigning species labels. You will inspect your images using the image viewer, where you can zoom in and out and enter the full-screen mode using the controls below the image. To tag the image, you must draw a bounding box by clicking and dragging a rectangle on top of the animal detected (Figure 6.5). Once you draw the bounding box, you must assign a species label by selecting it from the “Tag list” and save your tag. The image will be moved to the tagged images list at the bottom of the interface (Figure 6.6). When more than one animal is present, multiple tags can be assigned to the image (Figure 6.7). Remember to save all your tags. Figure 6.5: A bounding box is drawn around an animal detected in the image. Figure 6.6: Images tagged will be collected at the bottom of the tagging interface. Figure 6.7: Tags used for multiple animals in an image. The species might also contain higher taxonomic levels or other categories of interest (e.g., vehicles, people, etc.). It will also contain a “No Good” category that must be used for tagging empty or blurred images (Figure 6.8), or images containing small fragments of animal’s body parts. The “No Good” images will not be used for model training. Figure 6.8: Use of the “No Good” tag for empty images. Instead of selecting species labels from the species list at the right, after sketching the bounding box, you can type the shortcuts represented by the letters, numbers or symbols at the right of each species name in the “Tag list”. You can edit these shortcuts by clicking the ... icon. Once you finish tagging your batch of 500 images, you need to upload another batch to your tagging site. If you shared your images via Google Drive with the Conservation AI team, you can contact them to upload the images for you. The upload of each of these batches by the Conservation AI team can take up a few weeks depending on developers’ availability, so it’s recommended that you upload your images directly into the platform. 6.4 Process images - AI module Once the tagging stage is complete, you can contact the Conservation AI team (at admin@conservationai.co.uk) to have them train models using a transfer learning approach using the training data set that you provided in the image-tagging stage. Metadata, such as the time/date of the image and filename, are automatically read once images are uploaded for classification. Thus, you do not need to enter any metadata other than a sensor id if applicable. Completion of model training will depend on the Conservation AI team’s availability, so it’s important to schedule model training with developers that will re-train models with your data. You can also classify your images using one of the Conservation AI models currently available, which classify species from the United Kingdom, North America and Sub-Saharan Africa. You can upload images to the “Upload Files” section (Figure 6.1), by browsing files on a local computer. You can select a project name and a model that classifies species from your region of interest. Once the classification stage is complete, the results can be accessed in the platform’s analytical dashboard. Model output is shared with the data owner after a request is sent to the Conservation AI team. The output is provided in CSV, Excel or PDF format and will include rows for every image, folder names (image path), species name, confidence values (above a 0.5 confidence threshold), sensor name and project name. 6.5 Conclusions We have seen how to set up a project, upload and use AI to classify camera trap photos using Conservation AI’s platform. We described how to use the Conservation AI’s infrastructure for image tagging and model training for specific data sets. Conservation AI does not have models for classification of species from South America; for assessing model performance for species from geographical regions included in Conservation AI models, please refer to the methods described in Chapters 3 and 5. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
